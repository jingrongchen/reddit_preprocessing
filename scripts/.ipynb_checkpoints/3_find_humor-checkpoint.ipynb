{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79661b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723453e",
   "metadata": {},
   "source": [
    "# filter with key words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674a474",
   "metadata": {},
   "source": [
    "## test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "4f92adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchfor=['<3','xd',';D',':D',\n",
    "           '🙃','😂','🤣', \n",
    "           'funny','hilarious','kidding','laughing so hard','going nuts','so funny','Laughing','laughing hard','jokes',\n",
    "           'haha','hahah','hahaha','hahahaha','ahaha','Hehehehe',\n",
    "           'hehe',\n",
    "           'Hehehehe','kkkk',\n",
    "           'Lmao','lmaooo','lmaoo',\n",
    "           'LOL','Lol','lol',\n",
    "#            '\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "           ]\n",
    "\n",
    "emoticon=['<3','xd',';D']\n",
    "emoji=['🙃','😂','🤣']\n",
    "\n",
    "\n",
    "# regrex_pattern=r'\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "regrex_pattern='a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+|:D+'\n",
    "\n",
    "# match 'haha','hahah','hahaha','hahahaha','ahaha'\n",
    "#         'he'......\n",
    "#        lolololol\n",
    "#        lmao, lmaoooo\n",
    "#        :DDDDD\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8de03cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "txt = \"mwahahahahaha\"\n",
    "result = re.match(regrex_pattern, txt)\n",
    "\n",
    "print(bool(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "db3e935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap_searchfor=searchfor[:]\n",
    "# upper_searchfor=searchfor[:]\n",
    "# for i in range(len(cap)):\n",
    "#     cap[i] = cap[i].capitalize()\n",
    "# for i in range(len(cap)):\n",
    "#     upper_searchfor[i] = upper_searchfor[i].upper()    \n",
    "\n",
    "# print(cap_searchfor)\n",
    "# print(upper_searchfor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "fb4caf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_suffix = '20200101_20201231'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52cee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "302af3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>summarized</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ghm0f6y</td>\n",
       "      <td>t3_knrh1n</td>\n",
       "      <td>0</td>\n",
       "      <td>2020 was the only way that you would be doing ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ghm5z0i</td>\n",
       "      <td>t3_kns4dy</td>\n",
       "      <td>1</td>\n",
       "      <td>I personally am excited for the new year becau...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ghm6vjh</td>\n",
       "      <td>t3_kns4dy</td>\n",
       "      <td>0</td>\n",
       "      <td>I just wanna travel internationally lol</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ghm6z42</td>\n",
       "      <td>t3_knsp11</td>\n",
       "      <td>1</td>\n",
       "      <td>Still waiting here.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ghm7hon</td>\n",
       "      <td>t3_knsp11</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy new year!</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194406</th>\n",
       "      <td>fcpa4r8</td>\n",
       "      <td>t3_eicgrr</td>\n",
       "      <td>1</td>\n",
       "      <td>Fuck those people that don’t give any attentio...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194407</th>\n",
       "      <td>fcpbln7</td>\n",
       "      <td>t3_eidode</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh boy that sounds sad.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194408</th>\n",
       "      <td>fcpbudl</td>\n",
       "      <td>t3_eicskc</td>\n",
       "      <td>1</td>\n",
       "      <td>Sounds like you won’t learn your lesson until ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194409</th>\n",
       "      <td>fcpwter</td>\n",
       "      <td>t3_eid0ht</td>\n",
       "      <td>1</td>\n",
       "      <td>Children really are beautiful.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194410</th>\n",
       "      <td>fcsox0q</td>\n",
       "      <td>t3_eidiyb</td>\n",
       "      <td>0</td>\n",
       "      <td>I love potatoes</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194411 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  parent_id  summarized  \\\n",
       "0       ghm0f6y  t3_knrh1n           0   \n",
       "1       ghm5z0i  t3_kns4dy           1   \n",
       "2       ghm6vjh  t3_kns4dy           0   \n",
       "3       ghm6z42  t3_knsp11           1   \n",
       "4       ghm7hon  t3_knsp11           0   \n",
       "...         ...        ...         ...   \n",
       "194406  fcpa4r8  t3_eicgrr           1   \n",
       "194407  fcpbln7  t3_eidode           1   \n",
       "194408  fcpbudl  t3_eicskc           1   \n",
       "194409  fcpwter  t3_eid0ht           1   \n",
       "194410  fcsox0q  t3_eidiyb           0   \n",
       "\n",
       "                                                     text  length  \n",
       "0       2020 was the only way that you would be doing ...      19  \n",
       "1       I personally am excited for the new year becau...      21  \n",
       "2                 I just wanna travel internationally lol       6  \n",
       "3                                     Still waiting here.       4  \n",
       "4                                         Happy new year!       4  \n",
       "...                                                   ...     ...  \n",
       "194406  Fuck those people that don’t give any attentio...      12  \n",
       "194407                            Oh boy that sounds sad.       6  \n",
       "194408  Sounds like you won’t learn your lesson until ...      13  \n",
       "194409                     Children really are beautiful.       5  \n",
       "194410                                    I love potatoes       3  \n",
       "\n",
       "[194411 rows x 5 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df = pd.read_csv('../data/test/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "644dd943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210501_20211231 before filtering has submission and comment: 26139 169377\n",
      "2649\n",
      "length 2649\n",
      "20210501_20211231 after filtering has submission and comment: 1298 2402\n",
      "20210101_20210430 before filtering has submission and comment: 10506 65240\n",
      "1047\n",
      "length 1047\n",
      "20210101_20210430 after filtering has submission and comment: 495 1030\n",
      "20200101_20201231 before filtering has submission and comment: 75166 512226\n",
      "8308\n",
      "length 8308\n",
      "20200101_20201231 after filtering has submission and comment: 3442 7371\n",
      "20190101_20191231 before filtering has submission and comment: 59235 447373\n",
      "7844\n",
      "length 7844\n",
      "20190101_20191231 after filtering has submission and comment: 3416 7128\n",
      "20180101_20181231 before filtering has submission and comment: 60398 629117\n",
      "12104\n",
      "length 12104\n",
      "20180101_20181231 after filtering has submission and comment: 4239 10681\n",
      "20170101_20171231 before filtering has submission and comment: 44191 597154\n",
      "11882\n",
      "length 11882\n",
      "20170101_20171231 after filtering has submission and comment: 3120 10433\n",
      "20160101_20161231 before filtering has submission and comment: 28239 3414\n",
      "32\n",
      "length 32\n",
      "20160101_20161231 after filtering has submission and comment: 2 15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "    '20210501_20211231', '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    org_comment_df = pd.read_csv('../data/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    submission_df = pd.read_csv('../data/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    org_comment_df['text']=org_comment_df['text'].apply(lambda x:str.lower(x))\n",
    "    comment_df = org_comment_df[(org_comment_df['text'].str.contains('|'.join(searchfor)))|((org_comment_df['text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "    print(date_suffix,'before filtering has submission and comment:',submission_df.shape[0],org_comment_df.shape[0])\n",
    "    \n",
    "    t3_df=comment_df[comment_df['parent_id'].str.startswith('t3')]\n",
    "    t3_ids=t3_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    submission_filtered_ids=[x for x in t3_ids]\n",
    "    \n",
    "    t1_df=comment_df[comment_df['parent_id'].str.startswith('t1')]\n",
    "    print(t1_df.shape[0])\n",
    "    t1_ids=t1_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    comment_filtered_ids=[x for x in t1_ids]\n",
    "    print('length',len(t1_ids))\n",
    "    \n",
    "    t3_final_df = submission_df[submission_df['id'].isin(submission_filtered_ids)]    \n",
    "    t1_final_df = org_comment_df[org_comment_df['id'].isin(comment_filtered_ids)]\n",
    "    \n",
    "#     t1_final_df.insert(t1_final_df.shape[1], 're_coment', None)\n",
    "#     print(org_comment_df['parent_id'].str.split('_')[1][1])\n",
    "    \n",
    "    \n",
    "#     for idx, row in t1_final_df.iterrows():\n",
    "#         print(t1_final_df.at[idx,'parent_id'].split('_')[1])\n",
    "#         print(comment_df[comment_df['id']==t1_final_df.at[idx,'parent_id'].split('_')[1]])\n",
    "#         print((org_comment_df['parent_id'].str.split('_'))\n",
    "#         print(org_comment_df[org_comment_df['parent_id'].str.split('_')[1]==t1_final_df.at[idx,'id']])\n",
    "#         cid='t1_'+t1_final_df.at[idx,'id']\n",
    "        \n",
    "#         text_content=org_comment_df[org_comment_df['parent_id']==cid]\n",
    "        \n",
    "#         print(text_content.shape)\n",
    "#         t1_final_df.at[idx,'re_coment'] = str(text_content)\n",
    "    \n",
    "#     print(t1_final_df)\n",
    "#     t1_final_df=t1_final_df[t1_final_df['score']>=10]\n",
    "    \n",
    "    t3_final_df.to_csv('../data/keyword/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    t1_final_df.to_csv('../data/keyword/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    \n",
    "#     print(t1_final_df)\n",
    "    \n",
    "    print(date_suffix,'after filtering has submission and comment:',t3_final_df.shape[0],t1_final_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2708c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fdfa84b",
   "metadata": {},
   "source": [
    "# 2.GPT-2 method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b3aaa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2186/2186 [04:00<00:00,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160101_20161231 :done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "with torch.no_grad():\n",
    "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        model.eval()\n",
    "        if cuda:\n",
    "            model.to('cuda')\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    " \n",
    "def score(sentence):\n",
    "    tokenize_input = tokenizer.encode(sentence)\n",
    "    \n",
    "     \n",
    "    tensor_input = torch.tensor([tokenize_input])\n",
    "\n",
    "    U=\n",
    "    S=\n",
    "    \n",
    "                                                  \n",
    "    return np.exp(loss.detach().numpy())\n",
    " \n",
    "    \n",
    "date_suffices = [\n",
    "    '20160101_20161231'\n",
    "#     ,'20210501_20211231', '20210101_20210430', '20200101_20201231',\n",
    "#                   '20190101_20191231', '20180101_20181231', '20170101_20171231'\n",
    "                 ]\n",
    "for date_suffix in date_suffices:\n",
    "    test_df= pd.read_csv('../data/matched_q/casual_conv_{}.csv'.format(date_suffix))\n",
    "    test_df['combined'] = test_df['src_text']+' '+test_df['com_text']\n",
    "    test_df['gpt2_score']=test_df['combined'].progress_apply(lambda x:score(x))\n",
    "    \n",
    "    print(date_suffix,':done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "73e99906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2186/2186 [03:40<00:00,  9.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from torch.nn import functional as F\n",
    "from scipy.special import softmax\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "with torch.no_grad():\n",
    "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        model.eval()\n",
    "#         if cuda:\n",
    "#             model.to('cuda')\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "#     '20210501_20211231', '20210101_20210430', '20200101_20201231',\n",
    "#                   '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "punc = string.punctuation\n",
    "\n",
    "def com_svalue(setup,punchline):\n",
    "    if setup[-1] in punc:\n",
    "        sentence=setup+' '+punchline\n",
    "    else:\n",
    "        sentence=setup+'. '+punchline\n",
    "\n",
    "\n",
    "    #         print(sentence)\n",
    "    tokenize_all = tokenizer.encode(sentence)\n",
    "    #         print(len(tokenize_all))\n",
    "    #         print('tokenize_all:',tokenize_all)\n",
    "\n",
    "    setup_len=len(tokenizer.encode(setup))\n",
    "    #         pun_ids=tokenizer.encode(punchline)\n",
    "    #         print('setup',len(tokenizer.encode(setup)))\n",
    "    #         print('pun',len(pun_ids))\n",
    "\n",
    "    tensor_input = torch.tensor([tokenize_all])\n",
    "\n",
    "    outputs = model(tensor_input)\n",
    "    logit=outputs.logits[:,setup_len::,:]\n",
    "    tensor1 = logit.detach().numpy()\n",
    "    pre=softmax(tensor1)\n",
    "\n",
    "    #         print(pre.shape)\n",
    "\n",
    "    pun_len=len(tokenize_all)-setup_len\n",
    "    #         print('pun_len',pun_len)\n",
    "    s_value=0\n",
    "    for i in range(0,pun_len):\n",
    "        j=tokenize_all[setup_len+i]\n",
    "        if pre[0][i][j] > 0:\n",
    "            s_value=s_value+math.log(pre[0][i][j])\n",
    "    s_value=-s_value/len(tokenize_all)\n",
    "    return s_value\n",
    "\n",
    "\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    data_df= pd.read_csv('../data/matched_q/casual_conv_{}.csv'.format(date_suffix))\n",
    "    data_df['gpt2_score']=0\n",
    "    data_df['gpt2_score']=data_df.progress_apply(lambda x:com_svalue(x['src_text'],x['com_text']),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278a86db",
   "metadata": {},
   "source": [
    "# 3.just kidding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6c9471e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "    '20210501_20211231', '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "searchfor=['i am kidding','just kidding','i am telling jokes','joking with you','i am joking']\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    data_df = pd.read_csv('../data/matched_q/casual_conv_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    data_df = data_df[data_df['com_text'].str.contains('|'.join(searchfor))]\n",
    "    \n",
    "    data_df.to_csv('../data/kidding/kidding_{}.csv'.format(date_suffix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25139faa",
   "metadata": {},
   "source": [
    "# concate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b6fc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210501_20211231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 466/466 [00:01<00:00, 425.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210501_20211231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 904/904 [00:08<00:00, 102.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (2293, 12)\n",
      "20210101_20210430 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 202/202 [00:00<00:00, 476.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210101_20210430 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 406/406 [00:02<00:00, 202.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (934, 12)\n",
      "20200101_20201231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1326/1326 [00:03<00:00, 411.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200101_20201231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2966/2966 [01:17<00:00, 38.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (6477, 12)\n",
      "20190101_20191231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1331/1331 [00:03<00:00, 370.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190101_20191231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2719/2719 [01:02<00:00, 43.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (5940, 12)\n",
      "20180101_20181231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1808/1808 [00:04<00:00, 397.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180101_20181231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4444/4444 [02:21<00:00, 31.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (8902, 12)\n",
      "20170101_20171231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1619/1619 [00:04<00:00, 366.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170101_20171231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4431/4431 [02:16<00:00, 32.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (8499, 12)\n",
      "20160101_20161231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160101_20161231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 22/22 [00:00<00:00, 607.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (22, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "searchfor=['<3','xd',';D',':D',\n",
    "           '🙃','😂','🤣', \n",
    "           'funny','hilarious','kidding','laughing so hard','going nuts','so funny','Laughing','laughing hard','jokes',\n",
    "           'haha','hahah','hahaha','hahahaha','ahaha','Hehehehe',\n",
    "           'hehe',\n",
    "           'Hehehehe','kkkk',\n",
    "           'Lmao','lmaooo','lmaoo',\n",
    "           'LOL','Lol','lol',\n",
    "#            '\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "           ]\n",
    "\n",
    "emoticon=['<3','xd',';D']\n",
    "emoji=['🙃','😂','🤣']\n",
    "\n",
    "\n",
    "# regrex_pattern=r'\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "regrex_pattern='a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+|:D+'\n",
    "\n",
    "# match 'haha','hahah','hahaha','hahahaha','ahaha'\n",
    "#         'he'......\n",
    "#        lolololol\n",
    "#        lmao, lmaoooo\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "    '20210501_20211231', '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    org_comment_df = pd.read_csv('../data/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    submission_df = pd.read_csv('../data/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    first_comment_df=org_comment_df[(org_comment_df['text'].str.contains('|'.join(searchfor)))|((org_comment_df['text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "    \n",
    "    #t1_ids是回复他的回复里有好笑的回答的指向\n",
    "    t1_ids=first_comment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    filtered_ids=[x for x in t1_ids]\n",
    "        \n",
    "    t1_final_df = org_comment_df[org_comment_df['id'].isin(filtered_ids)]\n",
    "    # t1_parent_ids是指向这个回复的parent\n",
    "    t1_parent_ids=t1_final_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    \n",
    "    \n",
    "    final_cols = ['src_id', 'src_type', 'src_summarized', 'src_from', 'src_text', 'src_root', 'src_length',\n",
    "                  'com_id', 'com_summarized', 'com_text', 'com_length','com_score']\n",
    "    final_dict = {col: [] for col in final_cols}\n",
    "    \n",
    "\n",
    "    \n",
    "    submission_filtered_df = submission_df[submission_df['id'].isin(t1_parent_ids)]    \n",
    "    print(date_suffix,'Start assemble submission with comment')\n",
    "    for i in tqdm(range(submission_filtered_df.shape[0])):       \n",
    "        sub_id = submission_filtered_df.iloc[i]['id']\n",
    "        sub_summarized = submission_filtered_df.iloc[i]['summarized']\n",
    "        sub_from = submission_filtered_df.iloc[i]['from']\n",
    "        sub_text = submission_filtered_df.iloc[i]['text']\n",
    "        sub_root = submission_filtered_df.iloc[i]['root']\n",
    "        sub_length = submission_filtered_df.iloc[i]['length']\n",
    "        comment_filtered_df_sub = t1_final_df[t1_final_df['parent_id'] == 't3_' + sub_id]\n",
    "        for j in range(comment_filtered_df_sub.shape[0]):\n",
    "            final_dict['src_id'].append(sub_id)\n",
    "            final_dict['src_type'].append('sub')\n",
    "            final_dict['src_summarized'].append(sub_summarized)\n",
    "            final_dict['src_from'].append(sub_from)\n",
    "            final_dict['src_text'].append(sub_text)\n",
    "            final_dict['src_root'].append(sub_root)\n",
    "            final_dict['src_length'].append(sub_length)\n",
    "            final_dict['com_id'].append(comment_filtered_df_sub.iloc[j]['id'])\n",
    "            final_dict['com_summarized'].append(comment_filtered_df_sub.iloc[j]['summarized'])\n",
    "            final_dict['com_text'].append(comment_filtered_df_sub.iloc[j]['text'])\n",
    "            final_dict['com_length'].append(comment_filtered_df_sub.iloc[j]['length'])\n",
    "            final_dict['com_score'].append(comment_filtered_df_sub.iloc[j]['score'])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     coment_coment_df=t1_final_df[t1_final_df['parent_id'].str.startswith('t1')]\n",
    "#     coment_coment_ids=coment_coment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist() \n",
    "    \n",
    "    \n",
    "    \n",
    "    comment_have_kid_df = org_comment_df[org_comment_df['id'].isin(t1_parent_ids)]\n",
    "    \n",
    "    print(date_suffix,'Start assemble comment with comment')\n",
    "    for k in tqdm(range(comment_have_kid_df.shape[0])):       \n",
    "        src_id = comment_have_kid_df.iloc[k]['id']\n",
    "        src_summarized = comment_have_kid_df.iloc[k]['summarized']\n",
    "        src_text = comment_have_kid_df.iloc[k]['text']\n",
    "        src_length = comment_have_kid_df.iloc[k]['length']\n",
    "        \n",
    "        comment_comment_df = org_comment_df[org_comment_df['parent_id'] == 't1_' + src_id]\n",
    "        for m in range(comment_comment_df.shape[0]):\n",
    "            final_dict['src_id'].append(src_id)\n",
    "            final_dict['src_type'].append('com')\n",
    "            final_dict['src_summarized'].append(src_summarized)\n",
    "            final_dict['src_from'].append('comment')\n",
    "            final_dict['src_text'].append(src_text)\n",
    "            final_dict['src_root'].append('none')\n",
    "            final_dict['src_length'].append(src_length)\n",
    "            \n",
    "            final_dict['com_id'].append(comment_comment_df.iloc[m]['id'])\n",
    "            final_dict['com_summarized'].append(comment_comment_df.iloc[m]['summarized'])\n",
    "            final_dict['com_text'].append(comment_comment_df.iloc[m]['text'])\n",
    "            final_dict['com_length'].append(comment_comment_df.iloc[m]['length'])\n",
    "            final_dict['com_score'].append(comment_comment_df.iloc[m]['score'])\n",
    "\n",
    "    final_df = pd.DataFrame(final_dict)\n",
    "    print(\"final df shape:\",final_df.shape)\n",
    "    final_df=final_df.drop_duplicates()\n",
    "    final_df.drop(['src_root', 'src_length','com_summarized','src_summarized','src_from'], axis=1)\n",
    "    final_df.to_csv('../data/final_q/casual_conv_{}.csv'.format(date_suffix), index = False)\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18c51f",
   "metadata": {},
   "source": [
    "## Percentage of comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93d81b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210501_20211231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2217/2217 [00:22<00:00, 99.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (2587, 13)\n",
      "20210101_20210430 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 975/975 [00:04<00:00, 195.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (1092, 13)\n",
      "20200101_20201231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6691/6691 [02:59<00:00, 37.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (7621, 13)\n",
      "20190101_20191231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6344/6344 [02:27<00:00, 43.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (6913, 13)\n",
      "20180101_20181231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 9467/9467 [05:06<00:00, 30.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (10152, 13)\n",
      "20170101_20171231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 9108/9108 [04:43<00:00, 32.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (9890, 13)\n",
      "20160101_20161231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 39/39 [00:00<00:00, 612.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (39, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "searchfor=['<3','xd',';D',':D',\n",
    "           '🙃','😂','🤣', \n",
    "           'funny','hilarious','kidding','laughing so hard','going nuts','so funny','Laughing','laughing hard','jokes',\n",
    "           'haha','hahah','hahaha','hahahaha','ahaha','Hehehehe',\n",
    "           'hehe',\n",
    "           'Hehehehe','kkkk',\n",
    "           'Lmao','lmaooo','lmaoo',\n",
    "           'LOL','Lol','lol',\n",
    "#            '\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "           ]\n",
    "\n",
    "emoticon=['<3','xd',';D']\n",
    "emoji=['🙃','😂','🤣']\n",
    "\n",
    "# regrex_pattern=r'\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "regrex_pattern='a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+|:D+'\n",
    "\n",
    "# match 'haha','hahah','hahaha','hahahaha','ahaha'\n",
    "#         'he'......\n",
    "#        lolololol\n",
    "#        lmao, lmaoooo\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "    '20210501_20211231', '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    org_comment_df = pd.read_csv('../data/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    submission_df = pd.read_csv('../data/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    first_comment_df=org_comment_df[(org_comment_df['text'].str.contains('|'.join(searchfor)))|((org_comment_df['text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "    \n",
    "    #t1_ids是回复他的回复里有好笑的回答的指向\n",
    "    t1_ids=first_comment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    filtered_ids=[x for x in t1_ids]\n",
    "    \n",
    "    final_cols = ['src_id', 'src_parent','src_type', 'src_summarized', 'src_from', 'src_text', 'src_root', 'src_length',\n",
    "                  'com_id', 'com_summarized', 'com_text', 'com_length','com_score']\n",
    "    final_dict = {col: [] for col in final_cols}\n",
    "    \n",
    "\n",
    "    comment_have_kid_df = org_comment_df[org_comment_df['id'].isin(t1_ids)]\n",
    "    \n",
    "    print(date_suffix,'Start assemble comment(have fun indicater) with all of its comment')\n",
    "    for k in tqdm(range(comment_have_kid_df.shape[0])):       \n",
    "        src_id = comment_have_kid_df.iloc[k]['id']\n",
    "        src_parent=comment_have_kid_df.iloc[k]['parent_id']\n",
    "        src_summarized = comment_have_kid_df.iloc[k]['summarized']\n",
    "        src_text = comment_have_kid_df.iloc[k]['text']\n",
    "        src_length = comment_have_kid_df.iloc[k]['length']\n",
    "        \n",
    "        comment_comment_df = org_comment_df[org_comment_df['parent_id'] == 't1_' + src_id]\n",
    "        for m in range(comment_comment_df.shape[0]):\n",
    "            final_dict['src_id'].append(src_id)\n",
    "            final_dict['src_parent'].append(src_parent)\n",
    "            final_dict['src_type'].append('com')\n",
    "            final_dict['src_summarized'].append(src_summarized)\n",
    "            final_dict['src_from'].append('comment')\n",
    "            final_dict['src_text'].append(src_text)\n",
    "            final_dict['src_root'].append('none')\n",
    "            final_dict['src_length'].append(src_length)\n",
    "            \n",
    "            final_dict['com_id'].append(comment_comment_df.iloc[m]['id'])\n",
    "            final_dict['com_summarized'].append(comment_comment_df.iloc[m]['summarized'])\n",
    "            final_dict['com_text'].append(comment_comment_df.iloc[m]['text'])\n",
    "            final_dict['com_length'].append(comment_comment_df.iloc[m]['length'])\n",
    "            final_dict['com_score'].append(comment_comment_df.iloc[m]['score'])\n",
    "\n",
    "    final_df = pd.DataFrame(final_dict)\n",
    "    print(\"final df shape:\",final_df.shape)\n",
    "    final_df=final_df.drop_duplicates()\n",
    "    final_df=final_df.drop(['src_type', 'src_summarized', 'src_from','src_root', 'src_length','com_summarized','com_length'], axis=1)\n",
    "    final_df.to_csv('../data/final_q2/casual_conv_{}.csv'.format(date_suffix), index = False)\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca1eb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 103/103 [00:00<00:00, 157.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import string\n",
    "tqdm.pandas()\n",
    "\n",
    "searchfor=['<3','xd',';D',':D',\n",
    "           '🙃','😂','🤣', \n",
    "           'funny','hilarious','kidding','laughing so hard','going nuts','so funny','Laughing','laughing hard','jokes',\n",
    "           'haha','hahah','hahaha','hahahaha','ahaha','Hehehehe',\n",
    "           'hehe',\n",
    "           'Hehehehe','kkkk',\n",
    "           'Lmao','lmaooo','lmaoo',\n",
    "           'LOL','Lol','lol',\n",
    "#            '\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "           ]\n",
    "\n",
    "emoticon=['<3','xd',';D']\n",
    "emoji=['🙃','😂','🤣']\n",
    "\n",
    "# regrex_pattern=r'\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "regrex_pattern='a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+|:D+'\n",
    "\n",
    "# match 'haha','hahah','hahaha','hahahaha','ahaha'\n",
    "#         'he'......\n",
    "#        lolololol\n",
    "#        lmao, lmaoooo\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "    '20210501_20211231'\n",
    "#     , '20210101_20210430', '20200101_20201231',\n",
    "#                   '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "#                  '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "def filter_per(x):\n",
    "    num_com=len(x)\n",
    "    if num_com>=2:     \n",
    "        df=x[(x['com_text'].str.contains('|'.join(searchfor)))|((x['com_text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "        num_fun=len(df)\n",
    "        per=num_fun/num_com\n",
    "        if  per>0.4:\n",
    "            return df    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def concate_fun(x): \n",
    "    src_id=x.split('_')[1]\n",
    "#     print(src_id)\n",
    "    if x.startswith('t3'):\n",
    "#         print(x)\n",
    "        df=org_submission_df[org_submission_df['id']==src_id].head(1)\n",
    "        if len(df)>0:\n",
    "            return df['text'].item()\n",
    "       \n",
    "    else:\n",
    "        df=org_comment_df[org_comment_df['id']==src_id].head(1)\n",
    "        if len(df)>0:\n",
    "            return df['text'].item() \n",
    "\n",
    "\n",
    "punc = string.punctuation        \n",
    "for date_suffix in date_suffices:\n",
    "    org_comment_df = pd.read_csv('../data/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    org_submission_df = pd.read_csv('../data/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    #bigger than 50 percentage of coment are hahaha and bigger than tow comments\n",
    "    \n",
    "    filtered_df= pd.read_csv('../data/final_q2/casual_conv_{}.csv'.format(date_suffix))\n",
    "    final_df=filtered_df.groupby(\"src_id\").apply(filter_per)\n",
    "    \n",
    "    if len(final_df)>0:\n",
    "        final_df.insert(0,'reply_text','')\n",
    "        final_df['reply_text']=final_df.progress_apply(lambda x:concate_fun(x['src_parent']),axis=1)\n",
    "    #     org_submission_df[org_submission_df['id']==x['src_parent']]['text']\n",
    "        final_df=final_df.dropna(subset=['reply_text'])\n",
    "        #['reply_text', 'src_id', 'src_parent', 'src_text', 'com_id', 'com_text','com_score']\n",
    "        final_df['combined'] = final_df.apply(lambda x:(x['reply_text']+' '+x['src_text']) if x['reply_text'][-1] in punc else (x['reply_text']+'. '+x['src_text']),axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        final_df=final_df.drop(['reply_text', 'src_parent', 'src_text', 'com_id', 'com_text'], axis=1)\n",
    "        final_df.to_csv('../data/perc_50/casual_conv_{}.csv'.format(date_suffix), index = False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8bf6792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>com_score</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gyc80wo</th>\n",
       "      <th>659</th>\n",
       "      <td>996.0</td>\n",
       "      <td>I just pulled out a splinter that I've unknowi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gycbx1l</th>\n",
       "      <th>662</th>\n",
       "      <td>10.0</td>\n",
       "      <td>I just pulled out a splinter that I've unknowi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gychvhd</th>\n",
       "      <th>1854</th>\n",
       "      <td>157.0</td>\n",
       "      <td>No reason to stop being black knuckle now. I s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gyo0gkm</th>\n",
       "      <th>1839</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Salad bar! Get some ranch, baby carrots, celer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gyqz136</th>\n",
       "      <th>1836</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Yo no supe la palabra para achievement, gracia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hk7i7rh</th>\n",
       "      <th>46</th>\n",
       "      <td>3.0</td>\n",
       "      <td>What are your dreams like? I dream about human...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hk9jotn</th>\n",
       "      <th>39</th>\n",
       "      <td>100.0</td>\n",
       "      <td>I’m just really happy and kinda proud and just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hl2r1xn</th>\n",
       "      <th>22</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Would anyone like to talk and maybe be friends...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hlhno5r</th>\n",
       "      <th>705</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Is it the budjet or the style of food? Just th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hlt5npi</th>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>i love how guys talk to eachother. This is ver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              com_score                                           combined\n",
       "src_id                                                                    \n",
       "gyc80wo 659       996.0  I just pulled out a splinter that I've unknowi...\n",
       "gycbx1l 662        10.0  I just pulled out a splinter that I've unknowi...\n",
       "gychvhd 1854      157.0  No reason to stop being black knuckle now. I s...\n",
       "gyo0gkm 1839        2.0  Salad bar! Get some ranch, baby carrots, celer...\n",
       "gyqz136 1836        3.0  Yo no supe la palabra para achievement, gracia...\n",
       "...                 ...                                                ...\n",
       "hk7i7rh 46          3.0  What are your dreams like? I dream about human...\n",
       "hk9jotn 39        100.0  I’m just really happy and kinda proud and just...\n",
       "hl2r1xn 22          3.0  Would anyone like to talk and maybe be friends...\n",
       "hlhno5r 705         0.0  Is it the budjet or the style of food? Just th...\n",
       "hlt5npi 5           2.0  i love how guys talk to eachother. This is ver...\n",
       "\n",
       "[80 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92c6ccfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_25 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_27 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_29 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_30 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_31 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_33 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_34 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_35 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_36 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (Custom>TFBertMainLayer)  multiple             109482240   ['input_19[0][0]',               \n",
      "                                                                  'input_20[0][0]',               \n",
      "                                                                  'input_21[0][0]',               \n",
      "                                                                  'input_22[0][0]',               \n",
      "                                                                  'input_23[0][0]',               \n",
      "                                                                  'input_24[0][0]',               \n",
      "                                                                  'input_25[0][0]',               \n",
      "                                                                  'input_26[0][0]',               \n",
      "                                                                  'input_27[0][0]',               \n",
      "                                                                  'input_28[0][0]',               \n",
      "                                                                  'input_29[0][0]',               \n",
      "                                                                  'input_30[0][0]',               \n",
      "                                                                  'input_31[0][0]',               \n",
      "                                                                  'input_32[0][0]',               \n",
      "                                                                  'input_33[0][0]',               \n",
      "                                                                  'input_34[0][0]',               \n",
      "                                                                  'input_35[0][0]',               \n",
      "                                                                  'input_36[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d_6 (Gl  (None, 768)         0           ['bert[0][0]']                   \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_7 (Gl  (None, 768)         0           ['bert[1][0]']                   \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_8 (Gl  (None, 768)         0           ['bert[2][0]']                   \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_9 (Gl  (None, 768)         0           ['bert[3][0]']                   \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_10 (G  (None, 768)         0           ['bert[4][0]']                   \n",
      " lobalAveragePooling1D)                                                                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_11 (G  (None, 768)         0           ['bert[5][0]']                   \n",
      " lobalAveragePooling1D)                                                                           \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 32)           24608       ['global_average_pooling1d_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 32)           24608       ['global_average_pooling1d_7[0][0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 32)           24608       ['global_average_pooling1d_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 32)           24608       ['global_average_pooling1d_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 32)           24608       ['global_average_pooling1d_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 256)          196864      ['global_average_pooling1d_11[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)           (None, 32)           0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_45 (Dropout)           (None, 32)           0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_46 (Dropout)           (None, 32)           0           ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_47 (Dropout)           (None, 32)           0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_48 (Dropout)           (None, 32)           0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)           (None, 256)          0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 8)            264         ['dropout_44[0][0]']             \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8)            264         ['dropout_45[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 8)            264         ['dropout_46[0][0]']             \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 8)            264         ['dropout_47[0][0]']             \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 8)            264         ['dropout_48[0][0]']             \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 64)           16448       ['dropout_49[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 104)          0           ['dense_16[0][0]',               \n",
      "                                                                  'dense_18[0][0]',               \n",
      "                                                                  'dense_20[0][0]',               \n",
      "                                                                  'dense_22[0][0]',               \n",
      "                                                                  'dense_24[0][0]',               \n",
      "                                                                  'dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 512)          53760       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 512)          0           ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 256)          131328      ['dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 1)            257         ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,005,257\n",
      "Trainable params: 110,005,257\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/johnchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "20210501_20211231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2217/2217 [00:22<00:00, 100.69it/s]\n",
      "100%|████████████████████████████████████████| 103/103 [00:00<00:00, 188.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [01:03<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing colbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "80it [00:00, 486.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 20)\n",
      "20210501_20211231 done orignial size:  169377  final size: 80\n",
      "20210101_20210430 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 975/975 [00:05<00:00, 190.12it/s]\n",
      "100%|██████████████████████████████████████████| 29/29 [00:00<00:00, 462.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:20<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing colbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 453.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 20)\n",
      "20210101_20210430 done orignial size:  70843  final size: 25\n",
      "20200101_20201231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6691/6691 [03:15<00:00, 34.22it/s]\n",
      "100%|█████████████████████████████████████████| 313/313 [00:04<00:00, 66.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 227/227 [02:46<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing colbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:00, 485.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227, 20)\n",
      "20200101_20201231 done orignial size:  512226  final size: 227\n",
      "20190101_20191231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6344/6344 [02:30<00:00, 42.26it/s]\n",
      "100%|█████████████████████████████████████████| 221/221 [00:02<00:00, 78.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 173/173 [02:19<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing colbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [00:00, 459.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173, 20)\n",
      "20190101_20191231 done orignial size:  447373  final size: 173\n",
      "20180101_20181231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 9467/9467 [05:32<00:00, 28.51it/s]\n",
      "100%|█████████████████████████████████████████| 284/284 [00:04<00:00, 59.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [02:43<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing colbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "229it [00:00, 530.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(229, 20)\n",
      "20180101_20181231 done orignial size:  629117  final size: 229\n",
      "20170101_20171231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 9108/9108 [04:46<00:00, 31.78it/s]\n",
      "100%|█████████████████████████████████████████| 323/323 [00:04<00:00, 72.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 262/262 [02:59<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing colbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "262it [00:00, 564.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262, 20)\n",
      "20170101_20171231 done orignial size:  597154  final size: 262\n",
      "20160101_20161231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 39/39 [00:00<00:00, 460.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import networkx as nx\n",
    "# from pyvis.network import Network\n",
    "import subprocess\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "colber_model = tf.keras.models.load_model(\"../Colbert/colbert-trained/\")\n",
    "colber_model.summary()\n",
    "\n",
    "# import keras\n",
    "\n",
    "# colber_model = keras.models.load_model(\"../Colbert/colbert-trained/\")\n",
    "# colber_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "colber_tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)\n",
    "\n",
    "\n",
    "def score(sentence):\n",
    "    tokenize_input = colber_tokenizer.encode(sentence)\n",
    "    tensor_input = torch.tensor([tokenize_input])\n",
    "    loss=colber_model(tensor_input, labels=tensor_input)[0]\n",
    "    return np.exp(loss.detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "    inputs = colber_tokenizer.encode_plus(str1, str2,\n",
    "        add_special_tokens=True,\n",
    "        max_length=length,\n",
    "        truncation_strategy=truncation_strategy)\n",
    "\n",
    "    input_ids =  inputs[\"input_ids\"]\n",
    "    input_masks = [1] * len(input_ids)\n",
    "    input_segments = inputs[\"token_type_ids\"]\n",
    "    padding_length = length - len(input_ids)\n",
    "    padding_id = colber_tokenizer.pad_token_id\n",
    "    input_ids = input_ids + ([padding_id] * padding_length)\n",
    "    input_masks = input_masks + ([0] * padding_length)\n",
    "    input_segments = input_segments + ([0] * padding_length)\n",
    "\n",
    "    return [input_ids, input_masks, input_segments]\n",
    "\n",
    "\n",
    "def compute_input_arrays(df, columns, colber_tokenizer):\n",
    "    model_input = []\n",
    "    for xx in range((MAX_SENTENCES*3)+3):\n",
    "        model_input.append([])\n",
    "    \n",
    "    for _, row in tqdm(df[columns].iterrows()):\n",
    "        i = 0\n",
    "        \n",
    "        # sent\n",
    "        sentences = sent_tokenize(row.combined)\n",
    "        for xx in range(MAX_SENTENCES):\n",
    "            s = sentences[xx] if xx<len(sentences) else ''\n",
    "            ids_q, masks_q, segments_q = return_id(s, None, 'longest_first', MAX_SENTENCE_LENGTH)\n",
    "            model_input[i].append(ids_q)\n",
    "            i+=1\n",
    "            model_input[i].append(masks_q)\n",
    "            i+=1\n",
    "            model_input[i].append(segments_q)\n",
    "            i+=1\n",
    "        \n",
    "        # full row\n",
    "        ids_q, masks_q, segments_q = return_id(row.combined, None, 'longest_first', MAX_LENGTH)\n",
    "        model_input[i].append(ids_q)\n",
    "        i+=1\n",
    "        model_input[i].append(masks_q)\n",
    "        i+=1\n",
    "        model_input[i].append(segments_q)\n",
    "        \n",
    "    for xx in range((MAX_SENTENCES*3)+3):\n",
    "        model_input[xx] = np.asarray(model_input[xx], dtype=np.int32)\n",
    "        \n",
    "    print(model_input[0].shape)\n",
    "    return model_input\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from torch.nn import functional as F\n",
    "from scipy.special import softmax\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "#cuda=True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "with torch.no_grad():\n",
    "        model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "        model.eval()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "\n",
    "\n",
    "\n",
    "def com_svalue(setup,punchline):\n",
    "    if setup[-1] in punc:\n",
    "        sentence=setup+' '+punchline\n",
    "    else:\n",
    "        sentence=setup+'. '+punchline\n",
    "\n",
    "\n",
    "    setup_ids = tokenizer.encode(setup, return_tensors = 'tf')\n",
    "    punchline_ids = tokenizer.encode(punchline, return_tensors = 'tf')\n",
    "\n",
    "    all_ids = tokenizer.encode(sentence, return_tensors = 'tf')\n",
    "\n",
    "    outputs = model(all_ids[:,:-1])\n",
    "    logits = outputs.logits[0,-(punchline_ids.shape[1]):,:]\n",
    "    scores = tf.nn.softmax(logits, 1)\n",
    "    log_scores = logits - tf.expand_dims(tf.reduce_logsumexp(logits, 1), 1)\n",
    "\n",
    "    uncertainty = -tf.reduce_mean(tf.reduce_sum(scores * log_scores, 1))\n",
    "    uncertainties.append(uncertainty.numpy())\n",
    "\n",
    "    labels = tf.one_hot(all_ids[0,-(punchline_ids.shape[1]):], logits.shape[-1])\n",
    "    if labels.shape[0] == log_scores.shape[0]:\n",
    "        loss = -tf.reduce_mean(tf.reduce_sum(labels * log_scores, 1))\n",
    "        return loss.numpy()\n",
    "    else:\n",
    "        return -1.0  # Ignore examples with surprisal value == -1.0\n",
    "\n",
    "\n",
    "\n",
    "def filter_per(x):\n",
    "    num_com=len(x)\n",
    "    if num_com>=2:     \n",
    "        df=x[(x['com_text'].str.contains('|'.join(searchfor)))|((x['com_text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "        num_fun=len(df)\n",
    "        per=num_fun/num_com\n",
    "        if  per>0.4:\n",
    "            return df    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def concate_fun(x): \n",
    "    src_id=x.split('_')[1]\n",
    "    if x.startswith('t3'):\n",
    "        df=org_submission_df[org_submission_df['id']==src_id].head(1)\n",
    "        if len(df)>0:\n",
    "            return df['text'].item()\n",
    "       \n",
    "    else:\n",
    "        df=org_comment_df[org_comment_df['id']==src_id].head(1)\n",
    "        if len(df)>0:\n",
    "            return df['text'].item() \n",
    "\n",
    "MAX_SENTENCE_LENGTH = 20\n",
    "MAX_SENTENCES = 5\n",
    "MAX_LENGTH = 100\n",
    "punc = string.punctuation\n",
    "searchfor=['<3','xd',';D',':D',\n",
    "           '🙃','😂','🤣', \n",
    "           'funny','hilarious','kidding','laughing so hard','going nuts','so funny','Laughing','laughing hard','jokes',\n",
    "           'haha','hahah','hahaha','hahahaha','ahaha','Hehehehe',\n",
    "           'hehe',\n",
    "           'Hehehehe','kkkk',\n",
    "           'Lmao','lmaooo','lmaoo',\n",
    "           'LOL','Lol','lol',\n",
    "#            '\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "           ]\n",
    "\n",
    "emoticon=['<3','xd',';D']\n",
    "emoji=['🙃','😂','🤣']\n",
    "\n",
    "# regrex_pattern=r'\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "regrex_pattern='a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+|:D+'\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "   '20210501_20211231'\n",
    "    , '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "for date_suffix in date_suffices:\n",
    "\n",
    "    org_comment_df = pd.read_csv('../data/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    org_submission_df = pd.read_csv('../data/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    #bigger than 50 percentage of coment are hahaha and bigger than tow comments\n",
    "#     filtered_df= pd.read_csv('../data/final_q2/casual_conv_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    ##########################################################################################################generating filtered_df\n",
    "    first_comment_df=org_comment_df[(org_comment_df['text'].str.contains('|'.join(searchfor)))|((org_comment_df['text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "    t1_ids=first_comment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    #submission=>fun_comment=>kidcomment\n",
    "    #t1_ids point to fun_comment\n",
    "    filtered_ids=[x for x in t1_ids]\n",
    "    final_cols = ['src_id', 'src_parent','src_type', 'src_summarized', 'src_from', 'src_text', 'src_root', 'src_length',\n",
    "                  'com_id', 'com_summarized', 'com_text', 'com_length','com_score']\n",
    "    final_dict = {col: [] for col in final_cols}\n",
    "    \n",
    "    #submission=>fun_comment=>kidcomment\n",
    "    #fun_comment df\n",
    "    comment_have_kid_df = org_comment_df[org_comment_df['id'].isin(t1_ids)]\n",
    "    print(date_suffix,'Start assemble comment(have fun indicater) with all of its comment')\n",
    "    for k in tqdm(range(comment_have_kid_df.shape[0])):       \n",
    "        src_id = comment_have_kid_df.iloc[k]['id']\n",
    "        src_parent=comment_have_kid_df.iloc[k]['parent_id']\n",
    "        src_summarized = comment_have_kid_df.iloc[k]['summarized']\n",
    "        src_text = comment_have_kid_df.iloc[k]['text']\n",
    "        src_length = comment_have_kid_df.iloc[k]['length']\n",
    "        fun_comment_score=comment_have_kid_df.iloc[k]['score']\n",
    "    #submission=>fun_comment=>kidcomment\n",
    "    #kidcomment_df\n",
    "        comment_comment_df = org_comment_df[org_comment_df['parent_id'] == 't1_' + src_id]\n",
    "        for m in range(comment_comment_df.shape[0]):\n",
    "            final_dict['src_id'].append(src_id)\n",
    "            final_dict['src_parent'].append(src_parent)\n",
    "            final_dict['src_type'].append('com')\n",
    "            final_dict['src_summarized'].append(src_summarized)\n",
    "            final_dict['src_from'].append('comment')\n",
    "            final_dict['src_text'].append(src_text)\n",
    "            final_dict['src_root'].append('none')\n",
    "            final_dict['src_length'].append(src_length)\n",
    "            \n",
    "            final_dict['com_id'].append(comment_comment_df.iloc[m]['id'])\n",
    "            final_dict['com_summarized'].append(comment_comment_df.iloc[m]['summarized'])\n",
    "            final_dict['com_text'].append(comment_comment_df.iloc[m]['text'])\n",
    "            final_dict['com_length'].append(comment_comment_df.iloc[m]['length'])\n",
    "            \n",
    "            \n",
    "            final_dict['com_score'].append(fun_comment_score)\n",
    "\n",
    "    filtered_df = pd.DataFrame(final_dict)\n",
    "    filtered_df=filtered_df.drop_duplicates()\n",
    "    filtered_df=filtered_df.drop(['src_type', 'src_summarized', 'src_from','src_root', 'src_length','com_summarized','com_length'], axis=1)\n",
    "    ########################################################################################################################################\n",
    "    \n",
    "    final_df=filtered_df.groupby(\"src_id\").apply(filter_per)\n",
    "    if len(final_df)>0:\n",
    "        final_df.insert(0,'reply_text','')\n",
    "        final_df['reply_text']=final_df.progress_apply(lambda x:concate_fun(x['src_parent']),axis=1)\n",
    "        final_df=final_df.dropna(subset=['reply_text'])\n",
    "        #['reply_text', 'src_id', 'src_parent', 'src_text', 'com_id', 'com_text','com_score']\n",
    "        final_df['combined'] = final_df.apply(lambda x:(x['reply_text']+' '+x['src_text']) if x['reply_text'][-1] in punc else (x['reply_text']+'. '+x['src_text']),axis=1)\n",
    "\n",
    "        input_categories=['combined']\n",
    "        print('doing gpt2')\n",
    "        final_df['gpt2_score']=final_df.progress_apply(lambda x:com_svalue(x['src_text'],x['com_text']),axis=1)\n",
    "\n",
    "        print('doing colbert')\n",
    "        test_inputs = compute_input_arrays(final_df, input_categories, colber_tokenizer)\n",
    "        final_df['colbert_score']=colber_model.predict(test_inputs)\n",
    "\n",
    "        final_df=final_df.drop(['reply_text', 'src_parent', 'com_id', 'com_text'], axis=1)\n",
    "        final_df.to_csv('../data/test1/casual_conv_final_{}.csv'.format(date_suffix), index = False)\n",
    "        \n",
    "        print(date_suffix,'done','orignial size: ',len(org_comment_df),' final size:',len(final_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbdd2e8",
   "metadata": {},
   "source": [
    "# Filter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6404db",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6704600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_suffices = [\n",
    "   '20210501_20211231'\n",
    "    , '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "#                  '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "df_all = pd.DataFrame(columns = [\"src_id\", \"com_score\", \"combined\", \"gpt2_score\",'colbert_score'])\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "        df = pd.read_csv('../data/result/casual_conv_final_{}.csv'.format(date_suffix))\n",
    "        df=df[(df['com_score']>=2) & (df['gpt2_score']>8) &(df['colbert_score']>0.9)]\n",
    "#         df=df[(df['com_score']>=2) & (df['gpt2_score']>5) &(df['colbert_score']>0.)]\n",
    "        df_all=df_all.append(df)\n",
    "\n",
    "\n",
    "df_all.to_csv('../data/result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6264b336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3469387755102041 percentage of fun\n"
     ]
    }
   ],
   "source": [
    "print(17/49,'percentage of fun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58de5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####not kids indicate\n",
    "\n",
    "date_suffices = [\n",
    "   '20210501_20211231'\n",
    "    , '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "df_all = pd.DataFrame(columns = [\"src_id\", \"com_score\", \"combined\", \"gpt2_score\",'colbert_score'])\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "        df = pd.read_csv('../data/no_kid/casual_conv_final_{}.csv'.format(date_suffix))\n",
    "        df=df[(df['gpt2_score']>8) &(df['colbert_score']>0.9)]\n",
    "#         df=df[(df['com_score']>=2) & (df['gpt2_score']>5) &(df['colbert_score']>0.)]\n",
    "        df_all=df_all.append(df)\n",
    "\n",
    "\n",
    "df_all.to_csv('../data/no-kid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0bbea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
