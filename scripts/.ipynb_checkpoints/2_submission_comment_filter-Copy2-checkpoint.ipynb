{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a3716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "import neuralcoref\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0aa9b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/johnchen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a4fcc",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a00609",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde66b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fa69f655250>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9724b1",
   "metadata": {},
   "source": [
    "# The SMMRY Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d692b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_phrases = ['thus', 'for example', 'for instance', 'namely', 'to illustrate',\n",
    "                      'in other words', 'in particular', 'specifically', 'such as',\n",
    "                      'on the contrary', 'contrarily', 'notwithstanding', 'but', 'however',\n",
    "                      'nevertheless', 'in spite of', 'in contrast', 'yet', 'on one hand',\n",
    "                      'on the other hand', 'rather', 'or', 'nor', 'conversely', 'at the same time',\n",
    "                      'while this may be true', 'and', 'in addition to', 'furthermore',\n",
    "                      'moreover', 'besides', 'than', 'too', 'also', 'both-and', 'another',\n",
    "                      'equally important', 'second', 'etc.', 'again', 'further', 'last',\n",
    "                      'finally', 'not only-but also', 'as well as', 'in the second place',\n",
    "                      'next', 'likewise', 'similarly', 'in fact', 'as a result', 'consequently',\n",
    "                      'in the same way', 'for example', 'for instance', 'however', 'thus',\n",
    "                      'therefore', 'otherwise', 'after that', 'afterward', 'then', 'next',\n",
    "                      'last', 'at last', 'at length', 'at first', 'formerly', 'another', 'finally',\n",
    "                      'meanwhile', 'at the same time', 'afterwards', 'subsequently',\n",
    "                      'in the meantime', 'eventually', 'concurrently', 'simultaneously', 'although',\n",
    "                      'at least', 'still', 'even though', 'granted that', 'while it may be true',\n",
    "                      'in spite of', 'of course', 'similarly', 'likewise', 'in like fashion',\n",
    "                      'in like manner', 'analogous to', 'above all', 'indeed', 'of course',\n",
    "                      'certainly', 'surely', 'in fact', 'really', 'in truth', 'again', 'besides',\n",
    "                      'also', 'furthermore', 'in addition', 'specifically', 'especially',\n",
    "                      'in particular', 'to explain', 'to list', 'to enumerate', 'in detail',\n",
    "                      'namely', 'including', 'for example', 'for instance', 'to illustrate',\n",
    "                      'thus', 'in other words', 'as an illustration', 'in particular', 'so that',\n",
    "                      'with the result that', 'consequently', 'hence', 'accordingly', 'for this reason',\n",
    "                      'therefore', 'because', 'due to', 'as a result', 'in other words', 'then',\n",
    "                      'therefore', 'finally', 'consequently', 'thus', 'in conclusion', 'as a result',\n",
    "                      'accordingly', 'for this purpose', 'to this end', 'with this in mind',\n",
    "                      'with this purpose in mind', 'therefore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dff0f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_start(first_sent, dialog_turn):\n",
    "    if dialog_turn == 1:\n",
    "        for phrase in transition_phrases:\n",
    "            if first_sent.lower().startswith(phrase):\n",
    "                return True\n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d5c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smmry(text, doc, sent_count, dialog_turn):\n",
    "\n",
    "    # some preprocessing to omit text within brackets and replace u with you. \n",
    "    \n",
    "    # text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    # text = text.replace(' u ', ' you ')\n",
    "\n",
    "    formatted_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    formatted_text = re.sub(r'\\s+', ' ', formatted_text)\n",
    "\n",
    "    # doc = nlp(text)\n",
    "\n",
    "    fdist = {}\n",
    "    word_arr = nltk.word_tokenize(formatted_text.lower())\n",
    "\n",
    "    # preparing a frequency dictionary without considering stop words\n",
    "    \n",
    "    for word in word_arr:\n",
    "        if not word in stop_words:\n",
    "            word = wnl.lemmatize(word)\n",
    "            if word not in fdist.keys():\n",
    "                    fdist[word] = 1\n",
    "            else:\n",
    "                    fdist[word] += 1\n",
    "\n",
    "    sent_arr = nltk.sent_tokenize(text)\n",
    "    sent_score_arr = []\n",
    "    summary_arr = []\n",
    "\n",
    "    sent_arr_coref_resolved = nltk.sent_tokenize(doc._.coref_resolved)\n",
    "\n",
    "    # compute scores for each sentence\n",
    "\n",
    "    for sent in sent_arr:\n",
    "        score = 0\n",
    "        token_arr = nltk.word_tokenize(sent.lower())\n",
    "        for word in token_arr:\n",
    "            word = wnl.lemmatize(word)\n",
    "            if word in fdist.keys():\n",
    "                score += fdist[word]\n",
    "\n",
    "        sent_score_arr.append(score/len(token_arr))\n",
    "\n",
    "    sent_score_arr = np.array(sent_score_arr)\n",
    "\n",
    "    all_ind_arr = sent_score_arr.argsort()[-len(sent_score_arr):][::-1]\n",
    "\n",
    "    ind_arr_unsorted = sent_score_arr.argsort()[-sent_count:][::-1]\n",
    "\n",
    "    ind_arr = np.sort(ind_arr_unsorted) \n",
    "\n",
    "    summary = ''\n",
    "    changed_first = False\n",
    "\n",
    "    if len(ind_arr) > 0:\n",
    "\n",
    "        try:\n",
    "\n",
    "            ind = ind_arr[0]\n",
    "            first_sent = sent_arr[ind]\n",
    "\n",
    "            while (first_sent != sent_arr_coref_resolved[ind] or transition_start(first_sent, dialog_turn)):\n",
    "                changed_first = True\n",
    "                for index in all_ind_arr:\n",
    "                    if index < ind:\n",
    "                        ind = index\n",
    "                        break\n",
    "                first_sent = sent_arr[ind]\n",
    "                if ind == 0:\n",
    "                    break\n",
    "            summary = summary + first_sent + ' '     \n",
    "            \n",
    "            if (changed_first):\n",
    "                first_ind = ind\n",
    "                sent_score_modified = sent_score_arr[first_ind+1:]\n",
    "                ind_arr_unsorted = sent_score_modified.argsort()[-(sent_count-1):][::-1]\n",
    "                ind_arr_next = np.sort(ind_arr_unsorted) \n",
    "                \n",
    "                for i in range(0, len(ind_arr_next)):\n",
    "                    ind = (first_ind+1) + ind_arr_next[i]\n",
    "                    if i == len(ind_arr_next) - 1:\n",
    "                        summary = summary + sent_arr[ind]\n",
    "                    else:\n",
    "                        summary = summary + sent_arr[ind] + ' '\n",
    "            \n",
    "            else:\n",
    "                for i in range(1, len(ind_arr)):\n",
    "                    ind = ind_arr[i]\n",
    "                    if i == len(ind_arr) - 1:\n",
    "                        summary = summary + sent_arr[ind]\n",
    "                    else:\n",
    "                        summary = summary + sent_arr[ind] + ' '\n",
    "\n",
    "            return summary\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(\"EXCEPTION occured\")\n",
    "            return text\n",
    "\n",
    "    else:\n",
    "        print(text)\n",
    "        print(sent_arr)\n",
    "        print(\"EXCEPTION occured: length of sentence array is not > 0\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a57f2f",
   "metadata": {},
   "source": [
    "# Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af79b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux_verbs = ['be', 'can', 'could', 'dare', 'do', 'have', 'may', 'might', 'must',\n",
    "#              'need', 'ought', 'shall', 'should', 'will', 'would']\n",
    "# wh_words = ['what', 'when', 'where', 'which', 'who', 'whom', 'whose', 'why', 'how']\n",
    "# q_words = aux_verbs + wh_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c1259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw(text):\n",
    "    # Check if text is a str\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    # Replace HTML escape chars\n",
    "    text = text.replace('&gt;', '>')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('#x200B;', ' ')\n",
    "    text = text.replace('nbsp;', ' ')\n",
    "\n",
    "    # Remove brackets\n",
    "    b_pattern = re.compile(r'(\\([^\\(\\)]*\\))|(\\[[^\\[\\]]*\\])')\n",
    "    while b_pattern.search(text):\n",
    "        text = re.sub(r'(\\([^\\(\\)]*\\))|(\\[[^\\[\\]]*\\])', '', text)\n",
    "\n",
    "    # Remove redundant spaces (including breaklines)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Check if text is empty\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Check if text is [deleted] or [removed]\n",
    "    if text_lower == '[deleted]' or text_lower == '[removed]':\n",
    "        return None\n",
    "\n",
    "    # Check if text contains URL\n",
    "    url_pattern = re.compile(r'[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "    if url_pattern.search(text_lower):\n",
    "        return None\n",
    "\n",
    "    # Check if text contains 'r/<subreddit>' or 'u/<username>'\n",
    "    r_pattern = re.compile(r'(^| )\\/?r\\/[^ ]*')\n",
    "    if r_pattern.search(text_lower):\n",
    "        return None\n",
    "    u_pattern = re.compile(r'(^| )\\/?u\\/[^ ]*')\n",
    "    if u_pattern.search(text_lower):\n",
    "        return None\n",
    "\n",
    "    # Check if text contains 'reddit'\n",
    "    if 'reddit' in text_lower:\n",
    "        return None\n",
    "\n",
    "    # Check the percentage of alphabetical letters\n",
    "    num_alphas = 0\n",
    "    for ch in text:\n",
    "        if ch.isalpha():\n",
    "            num_alphas += 1\n",
    "    if num_alphas / len(text) < 0.7:\n",
    "        return None\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Check the number of tokens\n",
    "    if len(doc) < 2:\n",
    "        return None\n",
    "\n",
    "    return {'text': text, 'doc': doc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d84f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_summary(text):\n",
    "    # Check if text is a str\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    # Remove redundant spaces (including breaklines)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Check if text is empty\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # Check the percentage of alphabetical letters\n",
    "    num_alphas = 0\n",
    "    for ch in text:\n",
    "        if ch.isalpha():\n",
    "            num_alphas += 1\n",
    "    if num_alphas / len(text) < 0.7:\n",
    "        return None\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Check the number of tokens\n",
    "    if len(doc) < 2:\n",
    "        return None\n",
    "\n",
    "    return {'text': text, 'doc': doc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6bf909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_root(text, sent):\n",
    "#     # Check if the last character is a question mark\n",
    "#     if sent[-1].text == '?':\n",
    "#         return None\n",
    "\n",
    "    if sent.root.pos_ == 'VERB':\n",
    "#         # Check the first token\n",
    "#         if sent[0].lemma_.lower() in q_words:\n",
    "#             return None\n",
    "        return sent.root.lemma_\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebf556c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(preprocessed_text, dialog_turn):\n",
    "    if preprocessed_text is None:\n",
    "        return None\n",
    "\n",
    "    text = preprocessed_text['text']\n",
    "    doc = preprocessed_text['doc']\n",
    "\n",
    "    summarized = 0\n",
    "    sents = [sent for sent in doc.sents]\n",
    "    if len(sents) > 1:\n",
    "        summarized = 1\n",
    "        summary = smmry(text, doc, 1, dialog_turn)\n",
    "        preprocessed_summary = preprocess_summary(summary)\n",
    "        if preprocessed_summary is None:\n",
    "            return None\n",
    "        summarized_text = preprocessed_summary['text']\n",
    "        summarized_doc = preprocessed_summary['doc']\n",
    "        summarized_sents = [sent for sent in summarized_doc.sents]\n",
    "        if len(summarized_sents) != 1:\n",
    "            return None\n",
    "    elif len(sents) == 1:\n",
    "        summarized_text = text\n",
    "        summarized_doc = doc\n",
    "        summarized_sents = sents\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if dialog_turn > 1:\n",
    "        return {'text': summarized_text, 'summarized': summarized, 'length': len(summarized_sents[0])}\n",
    "\n",
    "    root = extract_root(summarized_text, summarized_sents[0])\n",
    "    if root is not None:\n",
    "        return {'text': summarized_text, 'summarized': summarized, 'root': root, 'length': len(summarized_sents[0])}\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c0ef6",
   "metadata": {},
   "source": [
    "# Filter Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b560f26",
   "metadata": {},
   "source": [
    "## Batch Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f82ee93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26106, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 26106/26106 [04:48<00:00, 90.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19990, 7)\n",
      "(24595, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 24595/24595 [04:36<00:00, 88.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18486, 7)\n",
      "(23326, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 23326/23326 [04:24<00:00, 88.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17392, 7)\n",
      "(28113, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 28113/28113 [05:19<00:00, 87.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21241, 7)\n",
      "(32344, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 32344/32344 [06:12<00:00, 86.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25242, 7)\n"
     ]
    }
   ],
   "source": [
    "date_suffices = ['20170101_20171231', '20180101_20181231', '20190101_20191231',\n",
    "                 '20200101_20201231', '20210101_20211231']\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    submission_df = pd.read_csv('../pushshift/submission/askReddit_submission_{}.csv'.format(date_suffix))\n",
    "    print(submission_df.shape)\n",
    "    submission_filtered_cols = ['id', 'summarized', 'from', 'text', 'root','score', 'length']\n",
    "    submission_filtered_dict = {col: [] for col in submission_filtered_cols}\n",
    "    for i in tqdm(range(submission_df.shape[0])):\n",
    "        submission_id = submission_df.iloc[i]['id']\n",
    "        submission_score = submission_df.iloc[i]['score']\n",
    "        \n",
    "        title = submission_df.iloc[i]['title']\n",
    "        preprocessed_title = preprocess_raw(title)\n",
    "        summarized_title = summarize(preprocessed_title, dialog_turn = 1)\n",
    "        if summarized_title is not None:\n",
    "            submission_filtered_dict['id'].append(submission_id)\n",
    "            submission_filtered_dict['summarized'].append(summarized_title['summarized'])\n",
    "            submission_filtered_dict['from'].append('title')\n",
    "            submission_filtered_dict['text'].append(summarized_title['text'])\n",
    "            submission_filtered_dict['root'].append(summarized_title['root'])\n",
    "            submission_filtered_dict['score'].append(submission_score)\n",
    "            submission_filtered_dict['length'].append(summarized_title['length'])\n",
    "        else:\n",
    "            selftext = submission_df.iloc[i]['selftext']\n",
    "            preprocessed_selftext = preprocess_raw(selftext)\n",
    "            summarized_selftext = summarize(preprocessed_selftext, dialog_turn = 1)\n",
    "            if summarized_selftext is not None:\n",
    "                submission_filtered_dict['id'].append(submission_id)\n",
    "                submission_filtered_dict['summarized'].append(summarized_selftext['summarized'])\n",
    "                submission_filtered_dict['from'].append('selftext')\n",
    "                submission_filtered_dict['text'].append(summarized_selftext['text'])\n",
    "                submission_filtered_dict['root'].append(summarized_selftext['root'])\n",
    "                submission_filtered_dict['score'].append(submission_score)\n",
    "                submission_filtered_dict['length'].append(summarized_selftext['length'])\n",
    "    submission_filtered_df = pd.DataFrame(submission_filtered_dict)\n",
    "    print(submission_filtered_df.shape)\n",
    "    submission_filtered_df.to_csv('../pushshift/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144ba0f",
   "metadata": {},
   "source": [
    "# Filter Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be3cb1",
   "metadata": {},
   "source": [
    "## Batch Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe0ae02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                  | 7808/104951 [03:17<32:43, 49.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████▍                           | 24704/104951 [11:07<55:36, 24.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████▍                         | 30590/104951 [14:55<56:56, 21.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████▋                       | 36831/104951 [18:22<33:15, 34.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████▎                    | 44734/104951 [22:32<32:38, 30.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████▌                   | 48448/104951 [24:32<22:42, 41.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████████████████▌                   | 48453/104951 [24:33<26:08, 36.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████▏                | 55976/104951 [28:06<15:39, 52.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████▊                | 57626/104951 [28:48<23:38, 33.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████████████████████▏             | 64852/104951 [31:48<22:33, 29.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|█████████████████████████▋          | 74716/104951 [36:20<17:34, 28.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|██████████████████████████▏         | 76239/104951 [37:11<17:26, 27.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████▌         | 77544/104951 [38:00<17:03, 26.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████▊         | 78050/104951 [38:10<08:33, 52.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████▉     | 90027/104951 [43:58<04:35, 54.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|████████████████████████████████▊   | 95744/104951 [46:17<04:11, 36.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|████████████████████████████████▉   | 95957/104951 [46:23<04:37, 32.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████▋  | 98279/104951 [47:55<03:07, 35.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 104951/104951 [51:29<00:00, 33.97it/s]\n",
      "  1%|▏                                      | 207/35065 [00:03<11:41, 49.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▏                               | 5760/35065 [03:42<11:53, 41.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▎                               | 5796/35065 [03:43<17:35, 27.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████▎                             | 7631/35065 [04:38<16:15, 28.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▋                           | 9854/35065 [05:57<16:23, 25.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████                          | 10485/35065 [06:23<16:38, 24.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████▍                        | 11828/35065 [07:15<12:43, 30.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████▌                        | 11961/35065 [07:20<11:15, 34.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████▌                 | 18598/35065 [11:42<09:09, 29.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████▉               | 20807/35065 [12:38<05:27, 43.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|█████████████████████████▌           | 24266/35065 [14:31<05:38, 31.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|████████████████████████████▋        | 27226/35065 [15:38<03:03, 42.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████████████▉       | 28342/35065 [16:12<03:19, 33.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████▋ | 33781/35065 [19:03<00:37, 34.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████ | 34143/35065 [19:15<00:31, 29.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 35065/35065 [19:46<00:00, 29.55it/s]\n",
      "100%|█████████████████████████████████████████| 722/722 [00:18<00:00, 39.40it/s]\n",
      " 95%|█████████████████████████████████████▏ | 3224/3384 [01:56<00:05, 30.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3384/3384 [01:59<00:00, 28.30it/s]\n",
      " 19%|███████▎                              | 7958/41456 [03:41<11:30, 48.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████▉                             | 9757/41456 [04:35<12:25, 42.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████▊                        | 14355/41456 [06:11<32:21, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████▌                      | 16259/41456 [06:54<07:07, 58.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████▍                | 22909/41456 [10:07<05:11, 59.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████▏               | 23738/41456 [10:27<04:23, 67.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████████████████████████████     | 35955/41456 [15:06<02:18, 39.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f0/3f71ctjx64zgpnygz80qt4bw0000gn/T/ipykernel_12008/3842615583.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomment_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomment_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpreprocessed_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0msummarized_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialog_turn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msummarized_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/f0/3f71ctjx64zgpnygz80qt4bw0000gn/T/ipykernel_12008/3725006772.py\u001b[0m in \u001b[0;36mpreprocess_raw\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Check the number of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mneuralcoref.pyx\u001b[0m in \u001b[0;36mneuralcoref.neuralcoref.NeuralCoref.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mneuralcoref.pyx\u001b[0m in \u001b[0;36mneuralcoref.neuralcoref.NeuralCoref.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/thinc/check.py\u001b[0m in \u001b[0;36mchecked_function\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mExpectedTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Callable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0marg_check_adder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/thinc/neural/_classes/relu.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input__BI)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput__BO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAffine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moutput__BO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput__BO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput__BO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/thinc/check.py\u001b[0m in \u001b[0;36mchecked_function\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mExpectedTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Callable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0marg_check_adder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/thinc/neural/_classes/affine.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input__BI)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput__BI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "date_suffices = ['20170101_20171231', '20180101_20181231', '20190101_20191231',\n",
    "                 '20200101_20201231', '20210101_20211231']\n",
    "for date_suffix in date_suffices:\n",
    "    comment_df = pd.read_csv('../pushshift/comment/askReddit_comment_{}.csv'.format(date_suffix))\n",
    "#     print(comment_df.shape)\n",
    "    submission_filtered_df = pd.read_csv('../pushshift/submission/askReddit_submission_{}.csv'.format(date_suffix))\n",
    "    submission_filtered_ids = submission_filtered_df['id'].tolist()\n",
    "    submission_filtered_ids = ['t3_' + x for x in submission_filtered_ids]\n",
    "    \n",
    "    coment_coment_df=comment_df[comment_df['parent_id'].str.startswith('t1',na=False)]\n",
    "    coment_coment_ids=coment_coment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    coment_coment_filtered_ids=[x for x in coment_coment_ids]\n",
    "    coment_coment_filtered_ids = list(dict.fromkeys(coment_coment_filtered_ids))\n",
    "\n",
    "    total_ids=submission_filtered_ids+coment_coment_filtered_ids\n",
    "    total_ids = list(dict.fromkeys(total_ids))   \n",
    "    comment_df=comment_df[comment_df['id'].isin(total_ids)]\n",
    "    \n",
    "    \n",
    "    comment_filtered_cols = ['id','link_id','parent_id', 'summarized', 'text', 'score','length']\n",
    "    comment_filtered_dict = {col: [] for col in comment_filtered_cols}\n",
    "    for i in tqdm(range(comment_df.shape[0])):\n",
    "        comment_id = comment_df.iloc[i]['id']\n",
    "        parent_id = comment_df.iloc[i]['parent_id']\n",
    "        score=comment_df.iloc[i]['score']\n",
    "        body = comment_df.iloc[i]['body']\n",
    "        preprocessed_body = preprocess_raw(body)\n",
    "        summarized_body = summarize(preprocessed_body, dialog_turn = 2)\n",
    "        if summarized_body is not None:\n",
    "            comment_filtered_dict['id'].append(comment_id)\n",
    "            comment_filtered_dict['parent_id'].append(parent_id)\n",
    "            comment_filtered_dict['summarized'].append(summarized_body['summarized'])\n",
    "            comment_filtered_dict['text'].append(summarized_body['text'])\n",
    "            comment_filtered_dict['score'].append(score)\n",
    "            comment_filtered_dict['length'].append(summarized_body['length'])\n",
    "    comment_filtered_df = pd.DataFrame(comment_filtered_dict)\n",
    "#     print(comment_filtered_df)\n",
    "    comment_filtered_df.to_csv('../pushshift/filtered_q/comment/askReddit_comment_{}.csv'.format(date_suffix), index = False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b997972",
   "metadata": {},
   "source": [
    "# Finalize the Submissions and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8913cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_filtered_df = pd.read_csv('../data/reddit/filtered/casual_conv_submissions_{}.csv'.format(date_suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7575456",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_filtered_df = pd.read_csv('../data/reddit/filtered/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "comment_filtered_parent_ids = comment_filtered_df['parent_id'].tolist()\n",
    "comment_filtered_parent_ids = [x[3:] for x in comment_filtered_parent_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d4dc612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7941, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_filtered_df = submission_filtered_df[submission_filtered_df['id'].isin(comment_filtered_parent_ids)]\n",
    "submission_filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ac2440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = ['sub_id', 'sub_summarized', 'sub_from', 'sub_text', 'sub_root', 'sub_length',\n",
    "              'com_id', 'com_summarized', 'com_text', 'com_length']\n",
    "final_dict = {col: [] for col in final_cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a99cbb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7941/7941 [00:46<00:00, 170.12it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(submission_filtered_df.shape[0])):\n",
    "    sub_id = submission_filtered_df.iloc[i]['id']\n",
    "    sub_summarized = submission_filtered_df.iloc[i]['summarized']\n",
    "    sub_from = submission_filtered_df.iloc[i]['from']\n",
    "    sub_text = submission_filtered_df.iloc[i]['text']\n",
    "    sub_root = submission_filtered_df.iloc[i]['root']\n",
    "    sub_length = submission_filtered_df.iloc[i]['length']\n",
    "\n",
    "    comment_filtered_df_sub = comment_filtered_df[comment_filtered_df['parent_id'] == 't3_' + sub_id]\n",
    "    for j in range(comment_filtered_df_sub.shape[0]):\n",
    "        final_dict['sub_id'].append(sub_id)\n",
    "        final_dict['sub_summarized'].append(sub_summarized)\n",
    "        final_dict['sub_from'].append(sub_from)\n",
    "        final_dict['sub_text'].append(sub_text)\n",
    "        final_dict['sub_root'].append(sub_root)\n",
    "        final_dict['sub_length'].append(sub_length)\n",
    "        final_dict['com_id'].append(comment_filtered_df_sub.iloc[j]['id'])\n",
    "        final_dict['com_summarized'].append(comment_filtered_df_sub.iloc[j]['summarized'])\n",
    "        final_dict['com_text'].append(comment_filtered_df_sub.iloc[j]['text'])\n",
    "        final_dict['com_length'].append(comment_filtered_df_sub.iloc[j]['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1daf8321",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88444562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_id</th>\n",
       "      <th>sub_summarized</th>\n",
       "      <th>sub_from</th>\n",
       "      <th>sub_text</th>\n",
       "      <th>sub_root</th>\n",
       "      <th>sub_length</th>\n",
       "      <th>com_id</th>\n",
       "      <th>com_summarized</th>\n",
       "      <th>com_text</th>\n",
       "      <th>com_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6trwr</td>\n",
       "      <td>1</td>\n",
       "      <td>Take it as a compliment.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6tw3v</td>\n",
       "      <td>1</td>\n",
       "      <td>However working at a bar where sex-charged men...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6uegk</td>\n",
       "      <td>1</td>\n",
       "      <td>Why do people get bent out of shape about hair...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6xlf7</td>\n",
       "      <td>1</td>\n",
       "      <td>Will appearing more attractive to customers in...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6zt2c</td>\n",
       "      <td>1</td>\n",
       "      <td>are you suggesting that its a problem for peop...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53689</th>\n",
       "      <td>n8sdhl</td>\n",
       "      <td>0</td>\n",
       "      <td>selftext</td>\n",
       "      <td>I never really got to do things like this when...</td>\n",
       "      <td>get</td>\n",
       "      <td>36</td>\n",
       "      <td>gxkseja</td>\n",
       "      <td>1</td>\n",
       "      <td>Nice man</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53690</th>\n",
       "      <td>n8sdhl</td>\n",
       "      <td>0</td>\n",
       "      <td>selftext</td>\n",
       "      <td>I never really got to do things like this when...</td>\n",
       "      <td>get</td>\n",
       "      <td>36</td>\n",
       "      <td>gxocvzh</td>\n",
       "      <td>1</td>\n",
       "      <td>I've never lived anywhere with fireflies, and ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53691</th>\n",
       "      <td>n8sbcb</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>After the worst 4 years of my life during coll...</td>\n",
       "      <td>move</td>\n",
       "      <td>22</td>\n",
       "      <td>gxk4z84</td>\n",
       "      <td>0</td>\n",
       "      <td>Not a bad place to live if you can solve for COL</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53692</th>\n",
       "      <td>n8sbcb</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>After the worst 4 years of my life during coll...</td>\n",
       "      <td>move</td>\n",
       "      <td>22</td>\n",
       "      <td>gxk5bpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Good luck on your new adventure!</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53693</th>\n",
       "      <td>n8sbcb</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>After the worst 4 years of my life during coll...</td>\n",
       "      <td>move</td>\n",
       "      <td>22</td>\n",
       "      <td>gxk9wzm</td>\n",
       "      <td>1</td>\n",
       "      <td>I also only took what could fit in my car.</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53694 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sub_id  sub_summarized  sub_from  \\\n",
       "0      r2v7x0               0     title   \n",
       "1      r2v7x0               0     title   \n",
       "2      r2v7x0               0     title   \n",
       "3      r2v7x0               0     title   \n",
       "4      r2v7x0               0     title   \n",
       "...       ...             ...       ...   \n",
       "53689  n8sdhl               0  selftext   \n",
       "53690  n8sdhl               0  selftext   \n",
       "53691  n8sbcb               0     title   \n",
       "53692  n8sbcb               0     title   \n",
       "53693  n8sbcb               0     title   \n",
       "\n",
       "                                                sub_text sub_root  sub_length  \\\n",
       "0      My boss told me that during the time my hair w...     make          21   \n",
       "1      My boss told me that during the time my hair w...     make          21   \n",
       "2      My boss told me that during the time my hair w...     make          21   \n",
       "3      My boss told me that during the time my hair w...     make          21   \n",
       "4      My boss told me that during the time my hair w...     make          21   \n",
       "...                                                  ...      ...         ...   \n",
       "53689  I never really got to do things like this when...      get          36   \n",
       "53690  I never really got to do things like this when...      get          36   \n",
       "53691  After the worst 4 years of my life during coll...     move          22   \n",
       "53692  After the worst 4 years of my life during coll...     move          22   \n",
       "53693  After the worst 4 years of my life during coll...     move          22   \n",
       "\n",
       "        com_id  com_summarized  \\\n",
       "0      hm6trwr               1   \n",
       "1      hm6tw3v               1   \n",
       "2      hm6uegk               1   \n",
       "3      hm6xlf7               1   \n",
       "4      hm6zt2c               1   \n",
       "...        ...             ...   \n",
       "53689  gxkseja               1   \n",
       "53690  gxocvzh               1   \n",
       "53691  gxk4z84               0   \n",
       "53692  gxk5bpg               1   \n",
       "53693  gxk9wzm               1   \n",
       "\n",
       "                                                com_text  com_length  \n",
       "0                               Take it as a compliment.           6  \n",
       "1      However working at a bar where sex-charged men...          14  \n",
       "2      Why do people get bent out of shape about hair...          15  \n",
       "3      Will appearing more attractive to customers in...          16  \n",
       "4      are you suggesting that its a problem for peop...          28  \n",
       "...                                                  ...         ...  \n",
       "53689                                           Nice man           2  \n",
       "53690  I've never lived anywhere with fireflies, and ...          18  \n",
       "53691   Not a bad place to live if you can solve for COL          12  \n",
       "53692                   Good luck on your new adventure!           7  \n",
       "53693         I also only took what could fit in my car.          11  \n",
       "\n",
       "[53694 rows x 10 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27025c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../data/reddit/matched/casual_conv_{}.csv'.format(date_suffix), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30679e83",
   "metadata": {},
   "source": [
    "## Batch Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b971178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 7)\n",
      "20160101_20161231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 389.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160101_20161231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2164/2164 [00:03<00:00, 692.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (2186, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "date_suffices = ['20170101_20171231', '20180101_20181231', '20190101_20191231',\n",
    "                 '20200101_20201231', '20210101_20211231']\n",
    "\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    submission_filtered_df = pd.read_csv('../pushshift/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    comment_filtered_df = pd.read_csv('../pushshift/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    final_cols = ['src_id', 'src_type', 'src_summarized', 'src_from', 'src_text', 'src_root', 'src_length',\n",
    "                  'com_id', 'com_summarized', 'com_text', 'com_length','com_score']\n",
    "    final_dict = {col: [] for col in final_cols}\n",
    "    \n",
    "    comment_filtered_parent_df=comment_filtered_df[comment_filtered_df['parent_id'].str.startswith('t3')]\n",
    "    comment_filtered_parent_ids = comment_filtered_parent_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    \n",
    "    submission_filtered_df = submission_filtered_df[submission_filtered_df['id'].isin(comment_filtered_parent_ids)]\n",
    "    print(submission_filtered_df.shape)\n",
    "\n",
    "    \n",
    "    print(date_suffix,'Start assemble submission with comment')\n",
    "    for i in tqdm(range(submission_filtered_df.shape[0])):       \n",
    "        sub_id = submission_filtered_df.iloc[i]['id']\n",
    "        sub_summarized = submission_filtered_df.iloc[i]['summarized']\n",
    "        sub_from = submission_filtered_df.iloc[i]['from']\n",
    "        sub_text = submission_filtered_df.iloc[i]['text']\n",
    "        sub_root = submission_filtered_df.iloc[i]['root']\n",
    "        sub_length = submission_filtered_df.iloc[i]['length']\n",
    "        comment_filtered_df_sub = comment_filtered_df[comment_filtered_df['parent_id'] == 't3_' + sub_id]\n",
    "        for j in range(comment_filtered_df_sub.shape[0]):\n",
    "            final_dict['src_id'].append(sub_id)\n",
    "            final_dict['src_type'].append('sub')\n",
    "            final_dict['src_summarized'].append(sub_summarized)\n",
    "            final_dict['src_from'].append(sub_from)\n",
    "            final_dict['src_text'].append(sub_text)\n",
    "            final_dict['src_root'].append(sub_root)\n",
    "            final_dict['src_length'].append(sub_length)\n",
    "            final_dict['com_id'].append(comment_filtered_df_sub.iloc[j]['id'])\n",
    "            final_dict['com_summarized'].append(comment_filtered_df_sub.iloc[j]['summarized'])\n",
    "            final_dict['com_text'].append(comment_filtered_df_sub.iloc[j]['text'])\n",
    "            final_dict['com_length'].append(comment_filtered_df_sub.iloc[j]['length'])\n",
    "            final_dict['com_score'].append(comment_filtered_df_sub.iloc[j]['score'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    coment_coment_df=comment_filtered_df[comment_filtered_df['parent_id'].str.startswith('t1')]\n",
    "    coment_coment_ids=coment_coment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()    \n",
    "    comment_have_kid_df = comment_filtered_df[comment_filtered_df['id'].isin(coment_coment_ids)]\n",
    "    \n",
    "    print(date_suffix,'Start assemble comment with comment')\n",
    "    for k in tqdm(range(comment_have_kid_df.shape[0])):       \n",
    "        src_id = comment_have_kid_df.iloc[k]['id']\n",
    "        src_summarized = comment_have_kid_df.iloc[k]['summarized']\n",
    "        src_text = comment_have_kid_df.iloc[k]['text']\n",
    "        src_length = comment_have_kid_df.iloc[k]['length']\n",
    "        \n",
    "        comment_comment_df = comment_filtered_df[comment_filtered_df['parent_id'] == 't1_' + src_id]\n",
    "        for m in range(comment_comment_df.shape[0]):\n",
    "            final_dict['src_id'].append(src_id)\n",
    "            final_dict['src_type'].append('com')\n",
    "            final_dict['src_summarized'].append(src_summarized)\n",
    "            final_dict['src_from'].append('comment')\n",
    "            final_dict['src_text'].append(src_text)\n",
    "            final_dict['src_root'].append(src_root)\n",
    "            final_dict['src_length'].append(src_length)\n",
    "            \n",
    "            final_dict['com_id'].append(comment_comment_df.iloc[m]['id'])\n",
    "            final_dict['com_summarized'].append(comment_comment_df.iloc[m]['summarized'])\n",
    "            final_dict['com_text'].append(comment_comment_df.iloc[m]['text'])\n",
    "            final_dict['com_length'].append(comment_comment_df.iloc[m]['length'])\n",
    "            final_dict['com_score'].append(comment_comment_df.iloc[m]['score'])\n",
    "\n",
    "\n",
    "    \n",
    "    final_df = pd.DataFrame(final_dict)\n",
    "    print(\"final df shape:\",final_df.shape)\n",
    "    final_df.to_csv('../pushshift/matched_q/casual_conv_{}.csv'.format(date_suffix), index = False)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5178cba1",
   "metadata": {},
   "source": [
    "# Sample Some Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04400925",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = final_df.shape[0]\n",
    "indices = np.sort(np.random.choice(N, 1000, replace = False))\n",
    "final_df_sample = final_df.iloc[indices]\n",
    "final_df_sample.to_csv('pushshift/final/casual_conv_20200101_20201231_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c42cb",
   "metadata": {},
   "source": [
    "# Some Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2464a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad703c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"A silly question\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df9d284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "AUX\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I am happy about it.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69a34e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I went to the market today\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93048868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Just worked more than I ever have in my life\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4053c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year\n",
      "PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Happy New Year!\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edfab88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "VERB\n",
      "have\n",
      "Has have AUX\n",
      "she -PRON- PRON\n",
      "done do VERB\n",
      "your -PRON- DET\n",
      "homework homework NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Has she done your homework\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)\n",
    "    print(sent[0].lemma_)\n",
    "    for token in sent:\n",
    "        print(token, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5f70370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you finish your homework\n",
      "VERB\n",
      "do\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Did you finish your homework\")\n",
    "sents = [sent for sent in doc.sents]\n",
    "sent = sents[0]\n",
    "print(sent)\n",
    "print(sent.root.pos_)\n",
    "print(sent[0].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e4caf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 gave VERB give\n",
      "-PRON- PRON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I just gave you the paper\")\n",
    "sents = [sent for sent in doc.sents]\n",
    "sent = sents[0]\n",
    "print(len(sent), sent.root, sent.root.pos_, sent.root.lemma_)\n",
    "print(sent[0].lemma_, sent[0].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04dafca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "r_pattern = re.compile(r'(^| )\\/?r\\/[^ ]*')\n",
    "print(r_pattern.search('ashjs/r/haha__ ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52e46a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how about  and and ['"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'(\\([^\\(\\)]*\\))|(\\[[^\\[\\]]*\\])', '', 'how about [ashs] and and [[ss]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef137b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
