{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a3716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "import neuralcoref\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0aa9b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/johnchen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a4fcc",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a00609",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde66b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fedc1c43250>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9724b1",
   "metadata": {},
   "source": [
    "# The SMMRY Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d692b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_phrases = ['thus', 'for example', 'for instance', 'namely', 'to illustrate',\n",
    "                      'in other words', 'in particular', 'specifically', 'such as',\n",
    "                      'on the contrary', 'contrarily', 'notwithstanding', 'but', 'however',\n",
    "                      'nevertheless', 'in spite of', 'in contrast', 'yet', 'on one hand',\n",
    "                      'on the other hand', 'rather', 'or', 'nor', 'conversely', 'at the same time',\n",
    "                      'while this may be true', 'and', 'in addition to', 'furthermore',\n",
    "                      'moreover', 'besides', 'than', 'too', 'also', 'both-and', 'another',\n",
    "                      'equally important', 'second', 'etc.', 'again', 'further', 'last',\n",
    "                      'finally', 'not only-but also', 'as well as', 'in the second place',\n",
    "                      'next', 'likewise', 'similarly', 'in fact', 'as a result', 'consequently',\n",
    "                      'in the same way', 'for example', 'for instance', 'however', 'thus',\n",
    "                      'therefore', 'otherwise', 'after that', 'afterward', 'then', 'next',\n",
    "                      'last', 'at last', 'at length', 'at first', 'formerly', 'another', 'finally',\n",
    "                      'meanwhile', 'at the same time', 'afterwards', 'subsequently',\n",
    "                      'in the meantime', 'eventually', 'concurrently', 'simultaneously', 'although',\n",
    "                      'at least', 'still', 'even though', 'granted that', 'while it may be true',\n",
    "                      'in spite of', 'of course', 'similarly', 'likewise', 'in like fashion',\n",
    "                      'in like manner', 'analogous to', 'above all', 'indeed', 'of course',\n",
    "                      'certainly', 'surely', 'in fact', 'really', 'in truth', 'again', 'besides',\n",
    "                      'also', 'furthermore', 'in addition', 'specifically', 'especially',\n",
    "                      'in particular', 'to explain', 'to list', 'to enumerate', 'in detail',\n",
    "                      'namely', 'including', 'for example', 'for instance', 'to illustrate',\n",
    "                      'thus', 'in other words', 'as an illustration', 'in particular', 'so that',\n",
    "                      'with the result that', 'consequently', 'hence', 'accordingly', 'for this reason',\n",
    "                      'therefore', 'because', 'due to', 'as a result', 'in other words', 'then',\n",
    "                      'therefore', 'finally', 'consequently', 'thus', 'in conclusion', 'as a result',\n",
    "                      'accordingly', 'for this purpose', 'to this end', 'with this in mind',\n",
    "                      'with this purpose in mind', 'therefore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dff0f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_start(first_sent, dialog_turn):\n",
    "    if dialog_turn == 1:\n",
    "        for phrase in transition_phrases:\n",
    "            if first_sent.lower().startswith(phrase):\n",
    "                return True\n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d5c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smmry(text, doc, sent_count, dialog_turn):\n",
    "\n",
    "    # some preprocessing to omit text within brackets and replace u with you. \n",
    "    \n",
    "    # text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    # text = text.replace(' u ', ' you ')\n",
    "\n",
    "    formatted_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    formatted_text = re.sub(r'\\s+', ' ', formatted_text)\n",
    "\n",
    "    # doc = nlp(text)\n",
    "\n",
    "    fdist = {}\n",
    "    word_arr = nltk.word_tokenize(formatted_text.lower())\n",
    "\n",
    "    # preparing a frequency dictionary without considering stop words\n",
    "    \n",
    "    for word in word_arr:\n",
    "        if not word in stop_words:\n",
    "            word = wnl.lemmatize(word)\n",
    "            if word not in fdist.keys():\n",
    "                    fdist[word] = 1\n",
    "            else:\n",
    "                    fdist[word] += 1\n",
    "\n",
    "    sent_arr = nltk.sent_tokenize(text)\n",
    "    sent_score_arr = []\n",
    "    summary_arr = []\n",
    "\n",
    "    sent_arr_coref_resolved = nltk.sent_tokenize(doc._.coref_resolved)\n",
    "\n",
    "    # compute scores for each sentence\n",
    "\n",
    "    for sent in sent_arr:\n",
    "        score = 0\n",
    "        token_arr = nltk.word_tokenize(sent.lower())\n",
    "        for word in token_arr:\n",
    "            word = wnl.lemmatize(word)\n",
    "            if word in fdist.keys():\n",
    "                score += fdist[word]\n",
    "\n",
    "        sent_score_arr.append(score/len(token_arr))\n",
    "\n",
    "    sent_score_arr = np.array(sent_score_arr)\n",
    "\n",
    "    all_ind_arr = sent_score_arr.argsort()[-len(sent_score_arr):][::-1]\n",
    "\n",
    "    ind_arr_unsorted = sent_score_arr.argsort()[-sent_count:][::-1]\n",
    "\n",
    "    ind_arr = np.sort(ind_arr_unsorted) \n",
    "\n",
    "    summary = ''\n",
    "    changed_first = False\n",
    "\n",
    "    if len(ind_arr) > 0:\n",
    "\n",
    "        try:\n",
    "\n",
    "            ind = ind_arr[0]\n",
    "            first_sent = sent_arr[ind]\n",
    "\n",
    "            while (first_sent != sent_arr_coref_resolved[ind] or transition_start(first_sent, dialog_turn)):\n",
    "                changed_first = True\n",
    "                for index in all_ind_arr:\n",
    "                    if index < ind:\n",
    "                        ind = index\n",
    "                        break\n",
    "                first_sent = sent_arr[ind]\n",
    "                if ind == 0:\n",
    "                    break\n",
    "            summary = summary + first_sent + ' '     \n",
    "            \n",
    "            if (changed_first):\n",
    "                first_ind = ind\n",
    "                sent_score_modified = sent_score_arr[first_ind+1:]\n",
    "                ind_arr_unsorted = sent_score_modified.argsort()[-(sent_count-1):][::-1]\n",
    "                ind_arr_next = np.sort(ind_arr_unsorted) \n",
    "                \n",
    "                for i in range(0, len(ind_arr_next)):\n",
    "                    ind = (first_ind+1) + ind_arr_next[i]\n",
    "                    if i == len(ind_arr_next) - 1:\n",
    "                        summary = summary + sent_arr[ind]\n",
    "                    else:\n",
    "                        summary = summary + sent_arr[ind] + ' '\n",
    "            \n",
    "            else:\n",
    "                for i in range(1, len(ind_arr)):\n",
    "                    ind = ind_arr[i]\n",
    "                    if i == len(ind_arr) - 1:\n",
    "                        summary = summary + sent_arr[ind]\n",
    "                    else:\n",
    "                        summary = summary + sent_arr[ind] + ' '\n",
    "\n",
    "            return summary\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(\"EXCEPTION occured\")\n",
    "            return text\n",
    "\n",
    "    else:\n",
    "        print(text)\n",
    "        print(sent_arr)\n",
    "        print(\"EXCEPTION occured: length of sentence array is not > 0\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a57f2f",
   "metadata": {},
   "source": [
    "# Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af79b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux_verbs = ['be', 'can', 'could', 'dare', 'do', 'have', 'may', 'might', 'must',\n",
    "#              'need', 'ought', 'shall', 'should', 'will', 'would']\n",
    "# wh_words = ['what', 'when', 'where', 'which', 'who', 'whom', 'whose', 'why', 'how']\n",
    "# q_words = aux_verbs + wh_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c1259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw(text):\n",
    "    # Check if text is a str\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    # Replace HTML escape chars\n",
    "    text = text.replace('&gt;', '>')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('#x200B;', ' ')\n",
    "    text = text.replace('nbsp;', ' ')\n",
    "\n",
    "    # Remove brackets\n",
    "    b_pattern = re.compile(r'(\\([^\\(\\)]*\\))|(\\[[^\\[\\]]*\\])')\n",
    "    while b_pattern.search(text):\n",
    "        text = re.sub(r'(\\([^\\(\\)]*\\))|(\\[[^\\[\\]]*\\])', '', text)\n",
    "\n",
    "    # Remove redundant spaces (including breaklines)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Check if text is empty\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Check if text is [deleted] or [removed]\n",
    "    if text_lower == '[deleted]' or text_lower == '[removed]':\n",
    "        return None\n",
    "\n",
    "    # Check if text contains URL\n",
    "    url_pattern = re.compile(r'[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "    if url_pattern.search(text_lower):\n",
    "        return None\n",
    "\n",
    "    # Check if text contains 'r/<subreddit>' or 'u/<username>'\n",
    "    r_pattern = re.compile(r'(^| )\\/?r\\/[^ ]*')\n",
    "    if r_pattern.search(text_lower):\n",
    "        return None\n",
    "    u_pattern = re.compile(r'(^| )\\/?u\\/[^ ]*')\n",
    "    if u_pattern.search(text_lower):\n",
    "        return None\n",
    "\n",
    "    # Check if text contains 'reddit'\n",
    "    if 'reddit' in text_lower:\n",
    "        return None\n",
    "\n",
    "    # Check the percentage of alphabetical letters\n",
    "    num_alphas = 0\n",
    "    for ch in text:\n",
    "        if ch.isalpha():\n",
    "            num_alphas += 1\n",
    "    if num_alphas / len(text) < 0.7:\n",
    "        return None\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Check the number of tokens\n",
    "    if len(doc) < 2:\n",
    "        return None\n",
    "\n",
    "    return {'text': text, 'doc': doc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d84f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_summary(text):\n",
    "    # Check if text is a str\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    # Remove redundant spaces (including breaklines)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Check if text is empty\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # Check the percentage of alphabetical letters\n",
    "    num_alphas = 0\n",
    "    for ch in text:\n",
    "        if ch.isalpha():\n",
    "            num_alphas += 1\n",
    "    if num_alphas / len(text) < 0.7:\n",
    "        return None\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Check the number of tokens\n",
    "    if len(doc) < 2:\n",
    "        return None\n",
    "\n",
    "    return {'text': text, 'doc': doc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6bf909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_root(text, sent):\n",
    "#     # Check if the last character is a question mark\n",
    "#     if sent[-1].text == '?':\n",
    "#         return None\n",
    "\n",
    "    if sent.root.pos_ == 'VERB':\n",
    "#         # Check the first token\n",
    "#         if sent[0].lemma_.lower() in q_words:\n",
    "#             return None\n",
    "        return sent.root.lemma_\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebf556c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(preprocessed_text, dialog_turn):\n",
    "    if preprocessed_text is None:\n",
    "        return None\n",
    "\n",
    "    text = preprocessed_text['text']\n",
    "    doc = preprocessed_text['doc']\n",
    "\n",
    "    summarized = 0\n",
    "    sents = [sent for sent in doc.sents]\n",
    "    if len(sents) > 1:\n",
    "        summarized = 1\n",
    "        summary = smmry(text, doc, 1, dialog_turn)\n",
    "        preprocessed_summary = preprocess_summary(summary)\n",
    "        if preprocessed_summary is None:\n",
    "            return None\n",
    "        summarized_text = preprocessed_summary['text']\n",
    "        summarized_doc = preprocessed_summary['doc']\n",
    "        summarized_sents = [sent for sent in summarized_doc.sents]\n",
    "        if len(summarized_sents) != 1:\n",
    "            return None\n",
    "    elif len(sents) == 1:\n",
    "        summarized_text = text\n",
    "        summarized_doc = doc\n",
    "        summarized_sents = sents\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if dialog_turn > 1:\n",
    "        return {'text': summarized_text, 'summarized': summarized, 'length': len(summarized_sents[0])}\n",
    "\n",
    "    root = extract_root(summarized_text, summarized_sents[0])\n",
    "    if root is not None:\n",
    "        return {'text': summarized_text, 'summarized': summarized, 'root': root, 'length': len(summarized_sents[0])}\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c0ef6",
   "metadata": {},
   "source": [
    "# Filter Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b560f26",
   "metadata": {},
   "source": [
    "## Batch Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f82ee93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26106, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 26106/26106 [04:48<00:00, 90.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19990, 7)\n",
      "(24595, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 24595/24595 [04:36<00:00, 88.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18486, 7)\n",
      "(23326, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 23326/23326 [04:24<00:00, 88.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17392, 7)\n",
      "(28113, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 28113/28113 [05:19<00:00, 87.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21241, 7)\n",
      "(32344, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 32344/32344 [06:12<00:00, 86.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25242, 7)\n"
     ]
    }
   ],
   "source": [
    "date_suffices = ['20170101_20171231', '20180101_20181231', '20190101_20191231',\n",
    "                 '20200101_20201231', '20210101_20211231']\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    submission_df = pd.read_csv('../pushshift/submission/askReddit_submission_{}.csv'.format(date_suffix))\n",
    "    print(submission_df.shape)\n",
    "    submission_filtered_cols = ['id', 'summarized', 'from', 'text', 'root','score', 'length']\n",
    "    submission_filtered_dict = {col: [] for col in submission_filtered_cols}\n",
    "    for i in tqdm(range(submission_df.shape[0])):\n",
    "        submission_id = submission_df.iloc[i]['id']\n",
    "        submission_score = submission_df.iloc[i]['score']\n",
    "        \n",
    "        title = submission_df.iloc[i]['title']\n",
    "        preprocessed_title = preprocess_raw(title)\n",
    "        summarized_title = summarize(preprocessed_title, dialog_turn = 1)\n",
    "        if summarized_title is not None:\n",
    "            submission_filtered_dict['id'].append(submission_id)\n",
    "            submission_filtered_dict['summarized'].append(summarized_title['summarized'])\n",
    "            submission_filtered_dict['from'].append('title')\n",
    "            submission_filtered_dict['text'].append(summarized_title['text'])\n",
    "            submission_filtered_dict['root'].append(summarized_title['root'])\n",
    "            submission_filtered_dict['score'].append(submission_score)\n",
    "            submission_filtered_dict['length'].append(summarized_title['length'])\n",
    "        else:\n",
    "            selftext = submission_df.iloc[i]['selftext']\n",
    "            preprocessed_selftext = preprocess_raw(selftext)\n",
    "            summarized_selftext = summarize(preprocessed_selftext, dialog_turn = 1)\n",
    "            if summarized_selftext is not None:\n",
    "                submission_filtered_dict['id'].append(submission_id)\n",
    "                submission_filtered_dict['summarized'].append(summarized_selftext['summarized'])\n",
    "                submission_filtered_dict['from'].append('selftext')\n",
    "                submission_filtered_dict['text'].append(summarized_selftext['text'])\n",
    "                submission_filtered_dict['root'].append(summarized_selftext['root'])\n",
    "                submission_filtered_dict['score'].append(submission_score)\n",
    "                submission_filtered_dict['length'].append(summarized_selftext['length'])\n",
    "    submission_filtered_df = pd.DataFrame(submission_filtered_dict)\n",
    "    print(submission_filtered_df.shape)\n",
    "    submission_filtered_df.to_csv('../pushshift/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144ba0f",
   "metadata": {},
   "source": [
    "# Filter Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be3cb1",
   "metadata": {},
   "source": [
    "## Batch Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe0ae02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                  | 7806/104951 [04:28<41:24, 39.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████                          | 24704/104951 [14:25<1:03:00, 21.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████▉                        | 30592/104951 [19:41<1:10:50, 17.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████▋                       | 36831/104951 [24:21<40:13, 28.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████▎                    | 44734/104951 [28:55<32:29, 30.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████▌                   | 48446/104951 [30:53<24:19, 38.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████▌                   | 48455/104951 [30:53<27:25, 34.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████▏                | 55971/104951 [34:24<17:21, 47.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████▊                | 57627/104951 [35:05<20:20, 38.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████████████████████▏             | 64851/104951 [38:06<22:41, 29.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|█████████████████████████▋          | 74716/104951 [42:36<17:22, 28.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|██████████████████████████▏         | 76239/104951 [43:26<17:27, 27.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████▌         | 77545/104951 [44:13<18:00, 25.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████▊         | 78056/104951 [44:23<07:50, 57.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████▉     | 90023/104951 [49:59<04:31, 54.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|████████████████████████████████▊   | 95744/104951 [52:17<04:14, 36.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|████████████████████████████████▉   | 95957/104951 [52:22<04:32, 33.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████▋  | 98279/104951 [53:53<03:03, 36.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 104951/104951 [57:21<00:00, 30.50it/s]\n"
     ]
    }
   ],
   "source": [
    "date_suffices = ['20170101_20171231']\n",
    "for date_suffix in date_suffices:\n",
    "    comment_df = pd.read_csv('../pushshift/comment/askReddit_comment_{}.csv'.format(date_suffix))\n",
    "#     print(comment_df.shape)\n",
    "    submission_filtered_df = pd.read_csv('../pushshift/submission/askReddit_submission_{}.csv'.format(date_suffix))\n",
    "    submission_filtered_ids = submission_filtered_df['id'].tolist()\n",
    "    submission_filtered_ids = ['t3_' + x for x in submission_filtered_ids]\n",
    "    \n",
    "    coment_coment_df=comment_df[comment_df['parent_id'].str.startswith('t1',na=False)]\n",
    "    coment_coment_ids=coment_coment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    coment_coment_filtered_ids=[x for x in coment_coment_ids]\n",
    "    coment_coment_filtered_ids = list(dict.fromkeys(coment_coment_filtered_ids))\n",
    "\n",
    "    total_ids=submission_filtered_ids+coment_coment_filtered_ids\n",
    "    total_ids = list(dict.fromkeys(total_ids))   \n",
    "    comment_df=comment_df[comment_df['id'].isin(total_ids)]\n",
    "    \n",
    "    \n",
    "    comment_filtered_cols = ['id','link_id','parent_id', 'summarized', 'text', 'score','length']\n",
    "    comment_filtered_dict = {col: [] for col in comment_filtered_cols}\n",
    "    for i in tqdm(range(comment_df.shape[0])):\n",
    "        comment_id = comment_df.iloc[i]['id']\n",
    "        link_id = comment_df.iloc[i]['link_id']\n",
    "        parent_id = comment_df.iloc[i]['parent_id']\n",
    "        score=comment_df.iloc[i]['score']\n",
    "        body = comment_df.iloc[i]['body']\n",
    "        preprocessed_body = preprocess_raw(body)\n",
    "        summarized_body = summarize(preprocessed_body, dialog_turn = 2)\n",
    "        if summarized_body is not None:\n",
    "            comment_filtered_dict['id'].append(comment_id)\n",
    "            comment_filtered_dict['link_id'].append(link_id)\n",
    "            comment_filtered_dict['parent_id'].append(parent_id)\n",
    "            comment_filtered_dict['summarized'].append(summarized_body['summarized'])\n",
    "            comment_filtered_dict['text'].append(summarized_body['text'])\n",
    "            comment_filtered_dict['score'].append(score)\n",
    "            comment_filtered_dict['length'].append(summarized_body['length'])\n",
    "    comment_filtered_df = pd.DataFrame(comment_filtered_dict)\n",
    "#     print(comment_filtered_df)\n",
    "    comment_filtered_df.to_csv('../pushshift/filtered_q/comment/askReddit_comment_{}.csv'.format(date_suffix), index = False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b997972",
   "metadata": {},
   "source": [
    "# Finalize the Submissions and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8913cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_filtered_df = pd.read_csv('../data/reddit/filtered/casual_conv_submissions_{}.csv'.format(date_suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7575456",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_filtered_df = pd.read_csv('../data/reddit/filtered/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "comment_filtered_parent_ids = comment_filtered_df['parent_id'].tolist()\n",
    "comment_filtered_parent_ids = [x[3:] for x in comment_filtered_parent_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d4dc612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7941, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_filtered_df = submission_filtered_df[submission_filtered_df['id'].isin(comment_filtered_parent_ids)]\n",
    "submission_filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ac2440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = ['sub_id', 'sub_summarized', 'sub_from', 'sub_text', 'sub_root', 'sub_length',\n",
    "              'com_id', 'com_summarized', 'com_text', 'com_length']\n",
    "final_dict = {col: [] for col in final_cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a99cbb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7941/7941 [00:46<00:00, 170.12it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(submission_filtered_df.shape[0])):\n",
    "    sub_id = submission_filtered_df.iloc[i]['id']\n",
    "    sub_summarized = submission_filtered_df.iloc[i]['summarized']\n",
    "    sub_from = submission_filtered_df.iloc[i]['from']\n",
    "    sub_text = submission_filtered_df.iloc[i]['text']\n",
    "    sub_root = submission_filtered_df.iloc[i]['root']\n",
    "    sub_length = submission_filtered_df.iloc[i]['length']\n",
    "\n",
    "    comment_filtered_df_sub = comment_filtered_df[comment_filtered_df['parent_id'] == 't3_' + sub_id]\n",
    "    for j in range(comment_filtered_df_sub.shape[0]):\n",
    "        final_dict['sub_id'].append(sub_id)\n",
    "        final_dict['sub_summarized'].append(sub_summarized)\n",
    "        final_dict['sub_from'].append(sub_from)\n",
    "        final_dict['sub_text'].append(sub_text)\n",
    "        final_dict['sub_root'].append(sub_root)\n",
    "        final_dict['sub_length'].append(sub_length)\n",
    "        final_dict['com_id'].append(comment_filtered_df_sub.iloc[j]['id'])\n",
    "        final_dict['com_summarized'].append(comment_filtered_df_sub.iloc[j]['summarized'])\n",
    "        final_dict['com_text'].append(comment_filtered_df_sub.iloc[j]['text'])\n",
    "        final_dict['com_length'].append(comment_filtered_df_sub.iloc[j]['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1daf8321",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88444562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_id</th>\n",
       "      <th>sub_summarized</th>\n",
       "      <th>sub_from</th>\n",
       "      <th>sub_text</th>\n",
       "      <th>sub_root</th>\n",
       "      <th>sub_length</th>\n",
       "      <th>com_id</th>\n",
       "      <th>com_summarized</th>\n",
       "      <th>com_text</th>\n",
       "      <th>com_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6trwr</td>\n",
       "      <td>1</td>\n",
       "      <td>Take it as a compliment.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6tw3v</td>\n",
       "      <td>1</td>\n",
       "      <td>However working at a bar where sex-charged men...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6uegk</td>\n",
       "      <td>1</td>\n",
       "      <td>Why do people get bent out of shape about hair...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6xlf7</td>\n",
       "      <td>1</td>\n",
       "      <td>Will appearing more attractive to customers in...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6zt2c</td>\n",
       "      <td>1</td>\n",
       "      <td>are you suggesting that its a problem for peop...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53689</th>\n",
       "      <td>n8sdhl</td>\n",
       "      <td>0</td>\n",
       "      <td>selftext</td>\n",
       "      <td>I never really got to do things like this when...</td>\n",
       "      <td>get</td>\n",
       "      <td>36</td>\n",
       "      <td>gxkseja</td>\n",
       "      <td>1</td>\n",
       "      <td>Nice man</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53690</th>\n",
       "      <td>n8sdhl</td>\n",
       "      <td>0</td>\n",
       "      <td>selftext</td>\n",
       "      <td>I never really got to do things like this when...</td>\n",
       "      <td>get</td>\n",
       "      <td>36</td>\n",
       "      <td>gxocvzh</td>\n",
       "      <td>1</td>\n",
       "      <td>I've never lived anywhere with fireflies, and ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53691</th>\n",
       "      <td>n8sbcb</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>After the worst 4 years of my life during coll...</td>\n",
       "      <td>move</td>\n",
       "      <td>22</td>\n",
       "      <td>gxk4z84</td>\n",
       "      <td>0</td>\n",
       "      <td>Not a bad place to live if you can solve for COL</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53692</th>\n",
       "      <td>n8sbcb</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>After the worst 4 years of my life during coll...</td>\n",
       "      <td>move</td>\n",
       "      <td>22</td>\n",
       "      <td>gxk5bpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Good luck on your new adventure!</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53693</th>\n",
       "      <td>n8sbcb</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>After the worst 4 years of my life during coll...</td>\n",
       "      <td>move</td>\n",
       "      <td>22</td>\n",
       "      <td>gxk9wzm</td>\n",
       "      <td>1</td>\n",
       "      <td>I also only took what could fit in my car.</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53694 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sub_id  sub_summarized  sub_from  \\\n",
       "0      r2v7x0               0     title   \n",
       "1      r2v7x0               0     title   \n",
       "2      r2v7x0               0     title   \n",
       "3      r2v7x0               0     title   \n",
       "4      r2v7x0               0     title   \n",
       "...       ...             ...       ...   \n",
       "53689  n8sdhl               0  selftext   \n",
       "53690  n8sdhl               0  selftext   \n",
       "53691  n8sbcb               0     title   \n",
       "53692  n8sbcb               0     title   \n",
       "53693  n8sbcb               0     title   \n",
       "\n",
       "                                                sub_text sub_root  sub_length  \\\n",
       "0      My boss told me that during the time my hair w...     make          21   \n",
       "1      My boss told me that during the time my hair w...     make          21   \n",
       "2      My boss told me that during the time my hair w...     make          21   \n",
       "3      My boss told me that during the time my hair w...     make          21   \n",
       "4      My boss told me that during the time my hair w...     make          21   \n",
       "...                                                  ...      ...         ...   \n",
       "53689  I never really got to do things like this when...      get          36   \n",
       "53690  I never really got to do things like this when...      get          36   \n",
       "53691  After the worst 4 years of my life during coll...     move          22   \n",
       "53692  After the worst 4 years of my life during coll...     move          22   \n",
       "53693  After the worst 4 years of my life during coll...     move          22   \n",
       "\n",
       "        com_id  com_summarized  \\\n",
       "0      hm6trwr               1   \n",
       "1      hm6tw3v               1   \n",
       "2      hm6uegk               1   \n",
       "3      hm6xlf7               1   \n",
       "4      hm6zt2c               1   \n",
       "...        ...             ...   \n",
       "53689  gxkseja               1   \n",
       "53690  gxocvzh               1   \n",
       "53691  gxk4z84               0   \n",
       "53692  gxk5bpg               1   \n",
       "53693  gxk9wzm               1   \n",
       "\n",
       "                                                com_text  com_length  \n",
       "0                               Take it as a compliment.           6  \n",
       "1      However working at a bar where sex-charged men...          14  \n",
       "2      Why do people get bent out of shape about hair...          15  \n",
       "3      Will appearing more attractive to customers in...          16  \n",
       "4      are you suggesting that its a problem for peop...          28  \n",
       "...                                                  ...         ...  \n",
       "53689                                           Nice man           2  \n",
       "53690  I've never lived anywhere with fireflies, and ...          18  \n",
       "53691   Not a bad place to live if you can solve for COL          12  \n",
       "53692                   Good luck on your new adventure!           7  \n",
       "53693         I also only took what could fit in my car.          11  \n",
       "\n",
       "[53694 rows x 10 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27025c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../data/reddit/matched/casual_conv_{}.csv'.format(date_suffix), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30679e83",
   "metadata": {},
   "source": [
    "## Batch Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b971178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 7)\n",
      "20160101_20161231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 389.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160101_20161231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2164/2164 [00:03<00:00, 692.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (2186, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "date_suffices = ['20170101_20171231', '20180101_20181231', '20190101_20191231',\n",
    "                 '20200101_20201231', '20210101_20211231']\n",
    "\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    submission_filtered_df = pd.read_csv('../pushshift/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    comment_filtered_df = pd.read_csv('../pushshift/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    final_cols = ['src_id', 'src_type', 'src_summarized', 'src_from', 'src_text', 'src_root', 'src_length',\n",
    "                  'com_id', 'com_summarized', 'com_text', 'com_length','com_score']\n",
    "    final_dict = {col: [] for col in final_cols}\n",
    "    \n",
    "    comment_filtered_parent_df=comment_filtered_df[comment_filtered_df['parent_id'].str.startswith('t3')]\n",
    "    comment_filtered_parent_ids = comment_filtered_parent_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    \n",
    "    submission_filtered_df = submission_filtered_df[submission_filtered_df['id'].isin(comment_filtered_parent_ids)]\n",
    "    print(submission_filtered_df.shape)\n",
    "\n",
    "    \n",
    "    print(date_suffix,'Start assemble submission with comment')\n",
    "    for i in tqdm(range(submission_filtered_df.shape[0])):       \n",
    "        sub_id = submission_filtered_df.iloc[i]['id']\n",
    "        sub_summarized = submission_filtered_df.iloc[i]['summarized']\n",
    "        sub_from = submission_filtered_df.iloc[i]['from']\n",
    "        sub_text = submission_filtered_df.iloc[i]['text']\n",
    "        sub_root = submission_filtered_df.iloc[i]['root']\n",
    "        sub_length = submission_filtered_df.iloc[i]['length']\n",
    "        comment_filtered_df_sub = comment_filtered_df[comment_filtered_df['parent_id'] == 't3_' + sub_id]\n",
    "        for j in range(comment_filtered_df_sub.shape[0]):\n",
    "            final_dict['src_id'].append(sub_id)\n",
    "            final_dict['src_type'].append('sub')\n",
    "            final_dict['src_summarized'].append(sub_summarized)\n",
    "            final_dict['src_from'].append(sub_from)\n",
    "            final_dict['src_text'].append(sub_text)\n",
    "            final_dict['src_root'].append(sub_root)\n",
    "            final_dict['src_length'].append(sub_length)\n",
    "            final_dict['com_id'].append(comment_filtered_df_sub.iloc[j]['id'])\n",
    "            final_dict['com_summarized'].append(comment_filtered_df_sub.iloc[j]['summarized'])\n",
    "            final_dict['com_text'].append(comment_filtered_df_sub.iloc[j]['text'])\n",
    "            final_dict['com_length'].append(comment_filtered_df_sub.iloc[j]['length'])\n",
    "            final_dict['com_score'].append(comment_filtered_df_sub.iloc[j]['score'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    coment_coment_df=comment_filtered_df[comment_filtered_df['parent_id'].str.startswith('t1')]\n",
    "    coment_coment_ids=coment_coment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()    \n",
    "    comment_have_kid_df = comment_filtered_df[comment_filtered_df['id'].isin(coment_coment_ids)]\n",
    "    \n",
    "    print(date_suffix,'Start assemble comment with comment')\n",
    "    for k in tqdm(range(comment_have_kid_df.shape[0])):       \n",
    "        src_id = comment_have_kid_df.iloc[k]['id']\n",
    "        src_summarized = comment_have_kid_df.iloc[k]['summarized']\n",
    "        src_text = comment_have_kid_df.iloc[k]['text']\n",
    "        src_length = comment_have_kid_df.iloc[k]['length']\n",
    "        \n",
    "        comment_comment_df = comment_filtered_df[comment_filtered_df['parent_id'] == 't1_' + src_id]\n",
    "        for m in range(comment_comment_df.shape[0]):\n",
    "            final_dict['src_id'].append(src_id)\n",
    "            final_dict['src_type'].append('com')\n",
    "            final_dict['src_summarized'].append(src_summarized)\n",
    "            final_dict['src_from'].append('comment')\n",
    "            final_dict['src_text'].append(src_text)\n",
    "            final_dict['src_root'].append(src_root)\n",
    "            final_dict['src_length'].append(src_length)\n",
    "            \n",
    "            final_dict['com_id'].append(comment_comment_df.iloc[m]['id'])\n",
    "            final_dict['com_summarized'].append(comment_comment_df.iloc[m]['summarized'])\n",
    "            final_dict['com_text'].append(comment_comment_df.iloc[m]['text'])\n",
    "            final_dict['com_length'].append(comment_comment_df.iloc[m]['length'])\n",
    "            final_dict['com_score'].append(comment_comment_df.iloc[m]['score'])\n",
    "\n",
    "\n",
    "    \n",
    "    final_df = pd.DataFrame(final_dict)\n",
    "    print(\"final df shape:\",final_df.shape)\n",
    "    final_df.to_csv('../pushshift/matched_q/casual_conv_{}.csv'.format(date_suffix), index = False)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5178cba1",
   "metadata": {},
   "source": [
    "# Sample Some Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04400925",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = final_df.shape[0]\n",
    "indices = np.sort(np.random.choice(N, 1000, replace = False))\n",
    "final_df_sample = final_df.iloc[indices]\n",
    "final_df_sample.to_csv('pushshift/final/casual_conv_20200101_20201231_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c42cb",
   "metadata": {},
   "source": [
    "# Some Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2464a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad703c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"A silly question\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df9d284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "AUX\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I am happy about it.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69a34e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I went to the market today\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93048868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Just worked more than I ever have in my life\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4053c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year\n",
      "PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Happy New Year!\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edfab88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "VERB\n",
      "have\n",
      "Has have AUX\n",
      "she -PRON- PRON\n",
      "done do VERB\n",
      "your -PRON- DET\n",
      "homework homework NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Has she done your homework\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)\n",
    "    print(sent[0].lemma_)\n",
    "    for token in sent:\n",
    "        print(token, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5f70370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you finish your homework\n",
      "VERB\n",
      "do\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Did you finish your homework\")\n",
    "sents = [sent for sent in doc.sents]\n",
    "sent = sents[0]\n",
    "print(sent)\n",
    "print(sent.root.pos_)\n",
    "print(sent[0].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e4caf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 gave VERB give\n",
      "-PRON- PRON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I just gave you the paper\")\n",
    "sents = [sent for sent in doc.sents]\n",
    "sent = sents[0]\n",
    "print(len(sent), sent.root, sent.root.pos_, sent.root.lemma_)\n",
    "print(sent[0].lemma_, sent[0].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04dafca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "r_pattern = re.compile(r'(^| )\\/?r\\/[^ ]*')\n",
    "print(r_pattern.search('ashjs/r/haha__ ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52e46a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how about  and and ['"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'(\\([^\\(\\)]*\\))|(\\[[^\\[\\]]*\\])', '', 'how about [ashs] and and [[ss]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef137b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 5)\n"
     ]
    }
   ],
   "source": [
    "data_suffices = [\n",
    "'20170101_20171231', '20180101_20181231', \n",
    "                 '20200101_20201231', '20210101_20211231']\n",
    "df_all = pd.DataFrame(columns = [\"src_id\", \"com_score\", \"combined\", \"gpt2_score\",'colbert_score'])\n",
    "\n",
    "for date_suffix in data_suffices:\n",
    "    df=pd.read_csv('../pushshift/final/askreddit_final_{}.csv'.format(date_suffix))\n",
    "#     df=df[ (df['com_score']>=2) & (df['gpt2_score']>5) &(df['colbert_score']>0.99) ]\n",
    "#         df=df[(df['com_score']>=2) & (df['gpt2_score']>5) &(sdf['colbert_core']>0.99)]\n",
    "    df_all=df_all.append(df)\n",
    "   \n",
    "\n",
    "# df_all.to_csv('../pushshift/all_result.csv')\n",
    "print(df_all.shape)\n",
    "# check=df_all.sample(n=50)\n",
    "# print(check.combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f15f1b8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../pushshift/final/askreddit_final_20190101_20191231.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f0/3f71ctjx64zgpnygz80qt4bw0000gn/T/ipykernel_12554/2464898186.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate_suffix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdate_suffices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../pushshift/final/askreddit_final_{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_suffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'com_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#         df=df[(df['com_score']>=2) & (df['gpt2_score']>5) &(df['colbert_score']>0.)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m         )\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m             )\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../pushshift/final/askreddit_final_20190101_20191231.csv'"
     ]
    }
   ],
   "source": [
    "data_suffices = [\n",
    "'20170101_20171231', '20180101_20181231', \n",
    "                 '20200101_20201231', '20210101_20211231']\n",
    "\n",
    "df_all = pd.DataFrame(columns = [\"src_id\", \"com_score\", \"combined\", \"gpt2_score\",'colbert_score'])\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "        df = pd.read_csv('../pushshift/final/askreddit_final_{}.csv'.format(date_suffix))\n",
    "        df=df[(df['com_score']>=2)]\n",
    "#         df=df[(df['com_score']>=2) & (df['gpt2_score']>5) &(df['colbert_score']>0.)]\n",
    "        df_all=df_all.append(df)\n",
    "\n",
    "print(df_all.shape)\n",
    "# df_all.to_csv('../pushshift/score_result.csv')\n",
    "\n",
    "# check=df_all.sample(n=50)\n",
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63cc652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
