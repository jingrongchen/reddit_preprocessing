{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79661b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723453e",
   "metadata": {},
   "source": [
    "# filter with key words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674a474",
   "metadata": {},
   "source": [
    "## test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "4f92adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchfor=['<3','xd',';D',':D',\n",
    "           'ðŸ™ƒ','ðŸ˜‚','ðŸ¤£', \n",
    "           'funny','hilarious','kidding','laughing so hard','going nuts','so funny','Laughing','laughing hard','jokes',\n",
    "           'haha','hahah','hahaha','hahahaha','ahaha','Hehehehe',\n",
    "           'hehe',\n",
    "           'Hehehehe','kkkk',\n",
    "           'Lmao','lmaooo','lmaoo',\n",
    "           'LOL','Lol','lol',\n",
    "#            '\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "           ]\n",
    "\n",
    "emoticon=['<3','xd',';D']\n",
    "emoji=['ðŸ™ƒ','ðŸ˜‚','ðŸ¤£']\n",
    "\n",
    "\n",
    "# regrex_pattern=r'\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "regrex_pattern='a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+|:D+'\n",
    "\n",
    "# match 'haha','hahah','hahaha','hahahaha','ahaha'\n",
    "#         'he'......\n",
    "#        lolololol\n",
    "#        lmao, lmaoooo\n",
    "#        :DDDDD\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8de03cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "txt = \"mwahahahahaha\"\n",
    "result = re.match(regrex_pattern, txt)\n",
    "\n",
    "print(bool(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "db3e935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap_searchfor=searchfor[:]\n",
    "# upper_searchfor=searchfor[:]\n",
    "# for i in range(len(cap)):\n",
    "#     cap[i] = cap[i].capitalize()\n",
    "# for i in range(len(cap)):\n",
    "#     upper_searchfor[i] = upper_searchfor[i].upper()    \n",
    "\n",
    "# print(cap_searchfor)\n",
    "# print(upper_searchfor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "fb4caf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_suffix = '20200101_20201231'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52cee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "302af3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>summarized</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ghm0f6y</td>\n",
       "      <td>t3_knrh1n</td>\n",
       "      <td>0</td>\n",
       "      <td>2020 was the only way that you would be doing ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ghm5z0i</td>\n",
       "      <td>t3_kns4dy</td>\n",
       "      <td>1</td>\n",
       "      <td>I personally am excited for the new year becau...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ghm6vjh</td>\n",
       "      <td>t3_kns4dy</td>\n",
       "      <td>0</td>\n",
       "      <td>I just wanna travel internationally lol</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ghm6z42</td>\n",
       "      <td>t3_knsp11</td>\n",
       "      <td>1</td>\n",
       "      <td>Still waiting here.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ghm7hon</td>\n",
       "      <td>t3_knsp11</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy new year!</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194406</th>\n",
       "      <td>fcpa4r8</td>\n",
       "      <td>t3_eicgrr</td>\n",
       "      <td>1</td>\n",
       "      <td>Fuck those people that donâ€™t give any attentio...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194407</th>\n",
       "      <td>fcpbln7</td>\n",
       "      <td>t3_eidode</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh boy that sounds sad.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194408</th>\n",
       "      <td>fcpbudl</td>\n",
       "      <td>t3_eicskc</td>\n",
       "      <td>1</td>\n",
       "      <td>Sounds like you wonâ€™t learn your lesson until ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194409</th>\n",
       "      <td>fcpwter</td>\n",
       "      <td>t3_eid0ht</td>\n",
       "      <td>1</td>\n",
       "      <td>Children really are beautiful.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194410</th>\n",
       "      <td>fcsox0q</td>\n",
       "      <td>t3_eidiyb</td>\n",
       "      <td>0</td>\n",
       "      <td>I love potatoes</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194411 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  parent_id  summarized  \\\n",
       "0       ghm0f6y  t3_knrh1n           0   \n",
       "1       ghm5z0i  t3_kns4dy           1   \n",
       "2       ghm6vjh  t3_kns4dy           0   \n",
       "3       ghm6z42  t3_knsp11           1   \n",
       "4       ghm7hon  t3_knsp11           0   \n",
       "...         ...        ...         ...   \n",
       "194406  fcpa4r8  t3_eicgrr           1   \n",
       "194407  fcpbln7  t3_eidode           1   \n",
       "194408  fcpbudl  t3_eicskc           1   \n",
       "194409  fcpwter  t3_eid0ht           1   \n",
       "194410  fcsox0q  t3_eidiyb           0   \n",
       "\n",
       "                                                     text  length  \n",
       "0       2020 was the only way that you would be doing ...      19  \n",
       "1       I personally am excited for the new year becau...      21  \n",
       "2                 I just wanna travel internationally lol       6  \n",
       "3                                     Still waiting here.       4  \n",
       "4                                         Happy new year!       4  \n",
       "...                                                   ...     ...  \n",
       "194406  Fuck those people that donâ€™t give any attentio...      12  \n",
       "194407                            Oh boy that sounds sad.       6  \n",
       "194408  Sounds like you wonâ€™t learn your lesson until ...      13  \n",
       "194409                     Children really are beautiful.       5  \n",
       "194410                                    I love potatoes       3  \n",
       "\n",
       "[194411 rows x 5 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df = pd.read_csv('../data/test/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "644dd943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210501_20211231 before filtering has submission and comment: 26139 169377\n",
      "2649\n",
      "length 2649\n",
      "20210501_20211231 after filtering has submission and comment: 1298 2402\n",
      "20210101_20210430 before filtering has submission and comment: 10506 65240\n",
      "1047\n",
      "length 1047\n",
      "20210101_20210430 after filtering has submission and comment: 495 1030\n",
      "20200101_20201231 before filtering has submission and comment: 75166 512226\n",
      "8308\n",
      "length 8308\n",
      "20200101_20201231 after filtering has submission and comment: 3442 7371\n",
      "20190101_20191231 before filtering has submission and comment: 59235 447373\n",
      "7844\n",
      "length 7844\n",
      "20190101_20191231 after filtering has submission and comment: 3416 7128\n",
      "20180101_20181231 before filtering has submission and comment: 60398 629117\n",
      "12104\n",
      "length 12104\n",
      "20180101_20181231 after filtering has submission and comment: 4239 10681\n",
      "20170101_20171231 before filtering has submission and comment: 44191 597154\n",
      "11882\n",
      "length 11882\n",
      "20170101_20171231 after filtering has submission and comment: 3120 10433\n",
      "20160101_20161231 before filtering has submission and comment: 28239 3414\n",
      "32\n",
      "length 32\n",
      "20160101_20161231 after filtering has submission and comment: 2 15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "    '20210501_20211231', '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    org_comment_df = pd.read_csv('../data/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    submission_df = pd.read_csv('../data/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    org_comment_df['text']=org_comment_df['text'].apply(lambda x:str.lower(x))\n",
    "    comment_df = org_comment_df[(org_comment_df['text'].str.contains('|'.join(searchfor)))|((org_comment_df['text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "    print(date_suffix,'before filtering has submission and comment:',submission_df.shape[0],org_comment_df.shape[0])\n",
    "    \n",
    "    t3_df=comment_df[comment_df['parent_id'].str.startswith('t3')]\n",
    "    t3_ids=t3_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    submission_filtered_ids=[x for x in t3_ids]\n",
    "    \n",
    "    t1_df=comment_df[comment_df['parent_id'].str.startswith('t1')]\n",
    "    print(t1_df.shape[0])\n",
    "    t1_ids=t1_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    comment_filtered_ids=[x for x in t1_ids]\n",
    "    print('length',len(t1_ids))\n",
    "    \n",
    "    t3_final_df = submission_df[submission_df['id'].isin(submission_filtered_ids)]    \n",
    "    t1_final_df = org_comment_df[org_comment_df['id'].isin(comment_filtered_ids)]\n",
    "    \n",
    "#     t1_final_df.insert(t1_final_df.shape[1], 're_coment', None)\n",
    "#     print(org_comment_df['parent_id'].str.split('_')[1][1])\n",
    "    \n",
    "    \n",
    "#     for idx, row in t1_final_df.iterrows():\n",
    "#         print(t1_final_df.at[idx,'parent_id'].split('_')[1])\n",
    "#         print(comment_df[comment_df['id']==t1_final_df.at[idx,'parent_id'].split('_')[1]])\n",
    "#         print((org_comment_df['parent_id'].str.split('_'))\n",
    "#         print(org_comment_df[org_comment_df['parent_id'].str.split('_')[1]==t1_final_df.at[idx,'id']])\n",
    "#         cid='t1_'+t1_final_df.at[idx,'id']\n",
    "        \n",
    "#         text_content=org_comment_df[org_comment_df['parent_id']==cid]\n",
    "        \n",
    "#         print(text_content.shape)\n",
    "#         t1_final_df.at[idx,'re_coment'] = str(text_content)\n",
    "    \n",
    "#     print(t1_final_df)\n",
    "#     t1_final_df=t1_final_df[t1_final_df['score']>=10]\n",
    "    \n",
    "    t3_final_df.to_csv('../data/keyword/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    t1_final_df.to_csv('../data/keyword/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    \n",
    "#     print(t1_final_df)\n",
    "    \n",
    "    print(date_suffix,'after filtering has submission and comment:',t3_final_df.shape[0],t1_final_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2708c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fdfa84b",
   "metadata": {},
   "source": [
    "# 2.GPT-2 method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b3aaa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2186/2186 [04:00<00:00,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160101_20161231 :done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "with torch.no_grad():\n",
    "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        model.eval()\n",
    "        if cuda:\n",
    "            model.to('cuda')\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    " \n",
    "def score(sentence):\n",
    "    tokenize_input = tokenizer.encode(sentence)\n",
    "    \n",
    "     \n",
    "    tensor_input = torch.tensor([tokenize_input])\n",
    "\n",
    "    U=\n",
    "    S=\n",
    "    \n",
    "                                                  \n",
    "    return np.exp(loss.detach().numpy())\n",
    " \n",
    "    \n",
    "date_suffices = [\n",
    "    '20160101_20161231'\n",
    "#     ,'20210501_20211231', '20210101_20210430', '20200101_20201231',\n",
    "#                   '20190101_20191231', '20180101_20181231', '20170101_20171231'\n",
    "                 ]\n",
    "for date_suffix in date_suffices:\n",
    "    test_df= pd.read_csv('../data/matched_q/casual_conv_{}.csv'.format(date_suffix))\n",
    "    test_df['combined'] = test_df['src_text']+' '+test_df['com_text']\n",
    "    test_df['gpt2_score']=test_df['combined'].progress_apply(lambda x:score(x))\n",
    "    \n",
    "    print(date_suffix,':done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "73e99906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2186/2186 [03:40<00:00,  9.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from torch.nn import functional as F\n",
    "from scipy.special import softmax\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "with torch.no_grad():\n",
    "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        model.eval()\n",
    "#         if cuda:\n",
    "#             model.to('cuda')\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "#     '20210501_20211231', '20210101_20210430', '20200101_20201231',\n",
    "#                   '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "punc = string.punctuation\n",
    "\n",
    "def com_svalue(setup,punchline):\n",
    "    if setup[-1] in punc:\n",
    "        sentence=setup+' '+punchline\n",
    "    else:\n",
    "        sentence=setup+'. '+punchline\n",
    "\n",
    "\n",
    "    #         print(sentence)\n",
    "    tokenize_all = tokenizer.encode(sentence)\n",
    "    #         print(len(tokenize_all))\n",
    "    #         print('tokenize_all:',tokenize_all)\n",
    "\n",
    "    setup_len=len(tokenizer.encode(setup))\n",
    "    #         pun_ids=tokenizer.encode(punchline)\n",
    "    #         print('setup',len(tokenizer.encode(setup)))\n",
    "    #         print('pun',len(pun_ids))\n",
    "\n",
    "    tensor_input = torch.tensor([tokenize_all])\n",
    "\n",
    "    outputs = model(tensor_input)\n",
    "    logit=outputs.logits[:,setup_len::,:]\n",
    "    tensor1 = logit.detach().numpy()\n",
    "    pre=softmax(tensor1)\n",
    "\n",
    "    #         print(pre.shape)\n",
    "\n",
    "    pun_len=len(tokenize_all)-setup_len\n",
    "    #         print('pun_len',pun_len)\n",
    "    s_value=0\n",
    "    for i in range(0,pun_len):\n",
    "        j=tokenize_all[setup_len+i]\n",
    "        if pre[0][i][j] > 0:\n",
    "            s_value=s_value+math.log(pre[0][i][j])\n",
    "    s_value=-s_value/len(tokenize_all)\n",
    "    return s_value\n",
    "\n",
    "\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    data_df= pd.read_csv('../data/matched_q/casual_conv_{}.csv'.format(date_suffix))\n",
    "    data_df['gpt2_score']=0\n",
    "    data_df['gpt2_score']=data_df.progress_apply(lambda x:com_svalue(x['src_text'],x['com_text']),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278a86db",
   "metadata": {},
   "source": [
    "# 3.just kidding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6c9471e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "    '20210501_20211231', '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "searchfor=['i am kidding','just kidding','i am telling jokes','joking with you','i am joking']\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    data_df = pd.read_csv('../data/matched_q/casual_conv_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    data_df = data_df[data_df['com_text'].str.contains('|'.join(searchfor))]\n",
    "    \n",
    "    data_df.to_csv('../data/kidding/kidding_{}.csv'.format(date_suffix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25139faa",
   "metadata": {},
   "source": [
    "# concate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b6fc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210501_20211231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466/466 [00:01<00:00, 425.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210501_20211231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 904/904 [00:08<00:00, 102.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (2293, 12)\n",
      "20210101_20210430 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 202/202 [00:00<00:00, 476.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210101_20210430 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 406/406 [00:02<00:00, 202.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (934, 12)\n",
      "20200101_20201231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1326/1326 [00:03<00:00, 411.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200101_20201231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2966/2966 [01:17<00:00, 38.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (6477, 12)\n",
      "20190101_20191231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1331/1331 [00:03<00:00, 370.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190101_20191231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2719/2719 [01:02<00:00, 43.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (5940, 12)\n",
      "20180101_20181231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1808/1808 [00:04<00:00, 397.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180101_20181231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4444/4444 [02:21<00:00, 31.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (8902, 12)\n",
      "20170101_20171231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1619/1619 [00:04<00:00, 366.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170101_20171231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4431/4431 [02:16<00:00, 32.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (8499, 12)\n",
      "20160101_20161231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160101_20161231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 607.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (22, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "searchfor=['<3','xd',';D',':D',\n",
    "           'ðŸ™ƒ','ðŸ˜‚','ðŸ¤£', \n",
    "           'funny','hilarious','kidding','laughing so hard','going nuts','so funny','Laughing','laughing hard','jokes',\n",
    "           'haha','hahah','hahaha','hahahaha','ahaha','Hehehehe',\n",
    "           'hehe',\n",
    "           'Hehehehe','kkkk',\n",
    "           'Lmao','lmaooo','lmaoo',\n",
    "           'LOL','Lol','lol',\n",
    "#            '\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "           ]\n",
    "\n",
    "emoticon=['<3','xd',';D']\n",
    "emoji=['ðŸ™ƒ','ðŸ˜‚','ðŸ¤£']\n",
    "\n",
    "\n",
    "# regrex_pattern=r'\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "regrex_pattern='a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+|:D+'\n",
    "\n",
    "# match 'haha','hahah','hahaha','hahahaha','ahaha'\n",
    "#         'he'......\n",
    "#        lolololol\n",
    "#        lmao, lmaoooo\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "    '20210501_20211231', '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    org_comment_df = pd.read_csv('../data/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    submission_df = pd.read_csv('../data/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    first_comment_df=org_comment_df[(org_comment_df['text'].str.contains('|'.join(searchfor)))|((org_comment_df['text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "    \n",
    "    #t1_idsæ˜¯å›žå¤ä»–çš„å›žå¤é‡Œæœ‰å¥½ç¬‘çš„å›žç­”çš„æŒ‡å‘\n",
    "    t1_ids=first_comment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    filtered_ids=[x for x in t1_ids]\n",
    "        \n",
    "    t1_final_df = org_comment_df[org_comment_df['id'].isin(filtered_ids)]\n",
    "    # t1_parent_idsæ˜¯æŒ‡å‘è¿™ä¸ªå›žå¤çš„parent\n",
    "    t1_parent_ids=t1_final_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    \n",
    "    \n",
    "    final_cols = ['src_id', 'src_type', 'src_summarized', 'src_from', 'src_text', 'src_root', 'src_length',\n",
    "                  'com_id', 'com_summarized', 'com_text', 'com_length','com_score']\n",
    "    final_dict = {col: [] for col in final_cols}\n",
    "    \n",
    "\n",
    "    \n",
    "    submission_filtered_df = submission_df[submission_df['id'].isin(t1_parent_ids)]    \n",
    "    print(date_suffix,'Start assemble submission with comment')\n",
    "    for i in tqdm(range(submission_filtered_df.shape[0])):       \n",
    "        sub_id = submission_filtered_df.iloc[i]['id']\n",
    "        sub_summarized = submission_filtered_df.iloc[i]['summarized']\n",
    "        sub_from = submission_filtered_df.iloc[i]['from']\n",
    "        sub_text = submission_filtered_df.iloc[i]['text']\n",
    "        sub_root = submission_filtered_df.iloc[i]['root']\n",
    "        sub_length = submission_filtered_df.iloc[i]['length']\n",
    "        comment_filtered_df_sub = t1_final_df[t1_final_df['parent_id'] == 't3_' + sub_id]\n",
    "        for j in range(comment_filtered_df_sub.shape[0]):\n",
    "            final_dict['src_id'].append(sub_id)\n",
    "            final_dict['src_type'].append('sub')\n",
    "            final_dict['src_summarized'].append(sub_summarized)\n",
    "            final_dict['src_from'].append(sub_from)\n",
    "            final_dict['src_text'].append(sub_text)\n",
    "            final_dict['src_root'].append(sub_root)\n",
    "            final_dict['src_length'].append(sub_length)\n",
    "            final_dict['com_id'].append(comment_filtered_df_sub.iloc[j]['id'])\n",
    "            final_dict['com_summarized'].append(comment_filtered_df_sub.iloc[j]['summarized'])\n",
    "            final_dict['com_text'].append(comment_filtered_df_sub.iloc[j]['text'])\n",
    "            final_dict['com_length'].append(comment_filtered_df_sub.iloc[j]['length'])\n",
    "            final_dict['com_score'].append(comment_filtered_df_sub.iloc[j]['score'])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     coment_coment_df=t1_final_df[t1_final_df['parent_id'].str.startswith('t1')]\n",
    "#     coment_coment_ids=coment_coment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist() \n",
    "    \n",
    "    \n",
    "    \n",
    "    comment_have_kid_df = org_comment_df[org_comment_df['id'].isin(t1_parent_ids)]\n",
    "    \n",
    "    print(date_suffix,'Start assemble comment with comment')\n",
    "    for k in tqdm(range(comment_have_kid_df.shape[0])):       \n",
    "        src_id = comment_have_kid_df.iloc[k]['id']\n",
    "        src_summarized = comment_have_kid_df.iloc[k]['summarized']\n",
    "        src_text = comment_have_kid_df.iloc[k]['text']\n",
    "        src_length = comment_have_kid_df.iloc[k]['length']\n",
    "        \n",
    "        comment_comment_df = org_comment_df[org_comment_df['parent_id'] == 't1_' + src_id]\n",
    "        for m in range(comment_comment_df.shape[0]):\n",
    "            final_dict['src_id'].append(src_id)\n",
    "            final_dict['src_type'].append('com')\n",
    "            final_dict['src_summarized'].append(src_summarized)\n",
    "            final_dict['src_from'].append('comment')\n",
    "            final_dict['src_text'].append(src_text)\n",
    "            final_dict['src_root'].append('none')\n",
    "            final_dict['src_length'].append(src_length)\n",
    "            \n",
    "            final_dict['com_id'].append(comment_comment_df.iloc[m]['id'])\n",
    "            final_dict['com_summarized'].append(comment_comment_df.iloc[m]['summarized'])\n",
    "            final_dict['com_text'].append(comment_comment_df.iloc[m]['text'])\n",
    "            final_dict['com_length'].append(comment_comment_df.iloc[m]['length'])\n",
    "            final_dict['com_score'].append(comment_comment_df.iloc[m]['score'])\n",
    "\n",
    "    final_df = pd.DataFrame(final_dict)\n",
    "    print(\"final df shape:\",final_df.shape)\n",
    "    final_df=final_df.drop_duplicates()\n",
    "    final_df.drop(['src_root', 'src_length','com_summarized','src_summarized','src_from'], axis=1)\n",
    "    final_df.to_csv('../data/final_q/casual_conv_{}.csv'.format(date_suffix), index = False)\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18c51f",
   "metadata": {},
   "source": [
    "## Percentage of comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93d81b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210501_20211231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2217/2217 [00:22<00:00, 99.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (2587, 13)\n",
      "20210101_20210430 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 975/975 [00:04<00:00, 195.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (1092, 13)\n",
      "20200101_20201231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6691/6691 [02:59<00:00, 37.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (7621, 13)\n",
      "20190101_20191231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6344/6344 [02:27<00:00, 43.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (6913, 13)\n",
      "20180101_20181231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9467/9467 [05:06<00:00, 30.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (10152, 13)\n",
      "20170101_20171231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9108/9108 [04:43<00:00, 32.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (9890, 13)\n",
      "20160101_20161231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 612.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (39, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "searchfor=['<3','xd',';D',':D',\n",
    "           'ðŸ™ƒ','ðŸ˜‚','ðŸ¤£', \n",
    "           'funny','hilarious','kidding','laughing so hard','going nuts','so funny','Laughing','laughing hard','jokes',\n",
    "           'haha','hahah','hahaha','hahahaha','ahaha','Hehehehe',\n",
    "           'hehe',\n",
    "           'Hehehehe','kkkk',\n",
    "           'Lmao','lmaooo','lmaoo',\n",
    "           'LOL','Lol','lol',\n",
    "#            '\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "           ]\n",
    "\n",
    "emoticon=['<3','xd',';D']\n",
    "emoji=['ðŸ™ƒ','ðŸ˜‚','ðŸ¤£']\n",
    "\n",
    "# regrex_pattern=r'\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "regrex_pattern='a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+|:D+'\n",
    "\n",
    "# match 'haha','hahah','hahaha','hahahaha','ahaha'\n",
    "#         'he'......\n",
    "#        lolololol\n",
    "#        lmao, lmaoooo\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "    '20210501_20211231', '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    org_comment_df = pd.read_csv('../data/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    submission_df = pd.read_csv('../data/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    first_comment_df=org_comment_df[(org_comment_df['text'].str.contains('|'.join(searchfor)))|((org_comment_df['text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "    \n",
    "    #t1_idsæ˜¯å›žå¤ä»–çš„å›žå¤é‡Œæœ‰å¥½ç¬‘çš„å›žç­”çš„æŒ‡å‘\n",
    "    t1_ids=first_comment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    filtered_ids=[x for x in t1_ids]\n",
    "    \n",
    "    final_cols = ['src_id', 'src_parent','src_type', 'src_summarized', 'src_from', 'src_text', 'src_root', 'src_length',\n",
    "                  'com_id', 'com_summarized', 'com_text', 'com_length','com_score']\n",
    "    final_dict = {col: [] for col in final_cols}\n",
    "    \n",
    "\n",
    "    comment_have_kid_df = org_comment_df[org_comment_df['id'].isin(t1_ids)]\n",
    "    \n",
    "    print(date_suffix,'Start assemble comment(have fun indicater) with all of its comment')\n",
    "    for k in tqdm(range(comment_have_kid_df.shape[0])):       \n",
    "        src_id = comment_have_kid_df.iloc[k]['id']\n",
    "        src_parent=comment_have_kid_df.iloc[k]['parent_id']\n",
    "        src_summarized = comment_have_kid_df.iloc[k]['summarized']\n",
    "        src_text = comment_have_kid_df.iloc[k]['text']\n",
    "        src_length = comment_have_kid_df.iloc[k]['length']\n",
    "        \n",
    "        comment_comment_df = org_comment_df[org_comment_df['parent_id'] == 't1_' + src_id]\n",
    "        for m in range(comment_comment_df.shape[0]):\n",
    "            final_dict['src_id'].append(src_id)\n",
    "            final_dict['src_parent'].append(src_parent)\n",
    "            final_dict['src_type'].append('com')\n",
    "            final_dict['src_summarized'].append(src_summarized)\n",
    "            final_dict['src_from'].append('comment')\n",
    "            final_dict['src_text'].append(src_text)\n",
    "            final_dict['src_root'].append('none')\n",
    "            final_dict['src_length'].append(src_length)\n",
    "            \n",
    "            final_dict['com_id'].append(comment_comment_df.iloc[m]['id'])\n",
    "            final_dict['com_summarized'].append(comment_comment_df.iloc[m]['summarized'])\n",
    "            final_dict['com_text'].append(comment_comment_df.iloc[m]['text'])\n",
    "            final_dict['com_length'].append(comment_comment_df.iloc[m]['length'])\n",
    "            final_dict['com_score'].append(comment_comment_df.iloc[m]['score'])\n",
    "\n",
    "    final_df = pd.DataFrame(final_dict)\n",
    "    print(\"final df shape:\",final_df.shape)\n",
    "    final_df=final_df.drop_duplicates()\n",
    "    final_df=final_df.drop(['src_type', 'src_summarized', 'src_from','src_root', 'src_length','com_summarized','com_length'], axis=1)\n",
    "    final_df.to_csv('../data/final_q2/casual_conv_{}.csv'.format(date_suffix), index = False)\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca1eb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 157.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import string\n",
    "tqdm.pandas()\n",
    "\n",
    "searchfor=['<3','xd',';D',':D',\n",
    "           'ðŸ™ƒ','ðŸ˜‚','ðŸ¤£', \n",
    "           'funny','hilarious','kidding','laughing so hard','going nuts','so funny','Laughing','laughing hard','jokes',\n",
    "           'haha','hahah','hahaha','hahahaha','ahaha','Hehehehe',\n",
    "           'hehe',\n",
    "           'Hehehehe','kkkk',\n",
    "           'Lmao','lmaooo','lmaoo',\n",
    "           'LOL','Lol','lol',\n",
    "#            '\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "           ]\n",
    "\n",
    "emoticon=['<3','xd',';D']\n",
    "emoji=['ðŸ™ƒ','ðŸ˜‚','ðŸ¤£']\n",
    "\n",
    "# regrex_pattern=r'\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "regrex_pattern='a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+|:D+'\n",
    "\n",
    "# match 'haha','hahah','hahaha','hahahaha','ahaha'\n",
    "#         'he'......\n",
    "#        lolololol\n",
    "#        lmao, lmaoooo\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "    '20210501_20211231'\n",
    "#     , '20210101_20210430', '20200101_20201231',\n",
    "#                   '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "#                  '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "def filter_per(x):\n",
    "    num_com=len(x)\n",
    "    if num_com>=2:     \n",
    "        df=x[(x['com_text'].str.contains('|'.join(searchfor)))|((x['com_text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "        num_fun=len(df)\n",
    "        per=num_fun/num_com\n",
    "        if  per>0.4:\n",
    "            return df    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def concate_fun(x): \n",
    "    src_id=x.split('_')[1]\n",
    "#     print(src_id)\n",
    "    if x.startswith('t3'):\n",
    "#         print(x)\n",
    "        df=org_submission_df[org_submission_df['id']==src_id].head(1)\n",
    "        if len(df)>0:\n",
    "            return df['text'].item()\n",
    "       \n",
    "    else:\n",
    "        df=org_comment_df[org_comment_df['id']==src_id].head(1)\n",
    "        if len(df)>0:\n",
    "            return df['text'].item() \n",
    "\n",
    "\n",
    "punc = string.punctuation        \n",
    "for date_suffix in date_suffices:\n",
    "    org_comment_df = pd.read_csv('../data/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    org_submission_df = pd.read_csv('../data/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    #bigger than 50 percentage of coment are hahaha and bigger than tow comments\n",
    "    \n",
    "    filtered_df= pd.read_csv('../data/final_q2/casual_conv_{}.csv'.format(date_suffix))\n",
    "    final_df=filtered_df.groupby(\"src_id\").apply(filter_per)\n",
    "    \n",
    "    if len(final_df)>0:\n",
    "        final_df.insert(0,'reply_text','')\n",
    "        final_df['reply_text']=final_df.progress_apply(lambda x:concate_fun(x['src_parent']),axis=1)\n",
    "    #     org_submission_df[org_submission_df['id']==x['src_parent']]['text']\n",
    "        final_df=final_df.dropna(subset=['reply_text'])\n",
    "        #['reply_text', 'src_id', 'src_parent', 'src_text', 'com_id', 'com_text','com_score']\n",
    "        final_df['combined'] = final_df.apply(lambda x:(x['reply_text']+' '+x['src_text']) if x['reply_text'][-1] in punc else (x['reply_text']+'. '+x['src_text']),axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        final_df=final_df.drop(['reply_text', 'src_parent', 'src_text', 'com_id', 'com_text'], axis=1)\n",
    "        final_df.to_csv('../data/perc_50/casual_conv_{}.csv'.format(date_suffix), index = False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8bf6792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>com_score</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gyc80wo</th>\n",
       "      <th>659</th>\n",
       "      <td>996.0</td>\n",
       "      <td>I just pulled out a splinter that I've unknowi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gycbx1l</th>\n",
       "      <th>662</th>\n",
       "      <td>10.0</td>\n",
       "      <td>I just pulled out a splinter that I've unknowi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gychvhd</th>\n",
       "      <th>1854</th>\n",
       "      <td>157.0</td>\n",
       "      <td>No reason to stop being black knuckle now. I s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gyo0gkm</th>\n",
       "      <th>1839</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Salad bar! Get some ranch, baby carrots, celer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gyqz136</th>\n",
       "      <th>1836</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Yo no supe la palabra para achievement, gracia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hk7i7rh</th>\n",
       "      <th>46</th>\n",
       "      <td>3.0</td>\n",
       "      <td>What are your dreams like? I dream about human...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hk9jotn</th>\n",
       "      <th>39</th>\n",
       "      <td>100.0</td>\n",
       "      <td>Iâ€™m just really happy and kinda proud and just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hl2r1xn</th>\n",
       "      <th>22</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Would anyone like to talk and maybe be friends...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hlhno5r</th>\n",
       "      <th>705</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Is it the budjet or the style of food? Just th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hlt5npi</th>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>i love how guys talk to eachother. This is ver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              com_score                                           combined\n",
       "src_id                                                                    \n",
       "gyc80wo 659       996.0  I just pulled out a splinter that I've unknowi...\n",
       "gycbx1l 662        10.0  I just pulled out a splinter that I've unknowi...\n",
       "gychvhd 1854      157.0  No reason to stop being black knuckle now. I s...\n",
       "gyo0gkm 1839        2.0  Salad bar! Get some ranch, baby carrots, celer...\n",
       "gyqz136 1836        3.0  Yo no supe la palabra para achievement, gracia...\n",
       "...                 ...                                                ...\n",
       "hk7i7rh 46          3.0  What are your dreams like? I dream about human...\n",
       "hk9jotn 39        100.0  Iâ€™m just really happy and kinda proud and just...\n",
       "hl2r1xn 22          3.0  Would anyone like to talk and maybe be friends...\n",
       "hlhno5r 705         0.0  Is it the budjet or the style of food? Just th...\n",
       "hlt5npi 5           2.0  i love how guys talk to eachother. This is ver...\n",
       "\n",
       "[80 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92c6ccfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_25 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_27 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_29 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_30 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_31 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_33 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_34 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_35 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_36 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (Custom>TFBertMainLayer)  multiple             109482240   ['input_19[0][0]',               \n",
      "                                                                  'input_20[0][0]',               \n",
      "                                                                  'input_21[0][0]',               \n",
      "                                                                  'input_22[0][0]',               \n",
      "                                                                  'input_23[0][0]',               \n",
      "                                                                  'input_24[0][0]',               \n",
      "                                                                  'input_25[0][0]',               \n",
      "                                                                  'input_26[0][0]',               \n",
      "                                                                  'input_27[0][0]',               \n",
      "                                                                  'input_28[0][0]',               \n",
      "                                                                  'input_29[0][0]',               \n",
      "                                                                  'input_30[0][0]',               \n",
      "                                                                  'input_31[0][0]',               \n",
      "                                                                  'input_32[0][0]',               \n",
      "                                                                  'input_33[0][0]',               \n",
      "                                                                  'input_34[0][0]',               \n",
      "                                                                  'input_35[0][0]',               \n",
      "                                                                  'input_36[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d_6 (Gl  (None, 768)         0           ['bert[0][0]']                   \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_7 (Gl  (None, 768)         0           ['bert[1][0]']                   \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_8 (Gl  (None, 768)         0           ['bert[2][0]']                   \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_9 (Gl  (None, 768)         0           ['bert[3][0]']                   \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_10 (G  (None, 768)         0           ['bert[4][0]']                   \n",
      " lobalAveragePooling1D)                                                                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_11 (G  (None, 768)         0           ['bert[5][0]']                   \n",
      " lobalAveragePooling1D)                                                                           \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 32)           24608       ['global_average_pooling1d_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 32)           24608       ['global_average_pooling1d_7[0][0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 32)           24608       ['global_average_pooling1d_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 32)           24608       ['global_average_pooling1d_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 32)           24608       ['global_average_pooling1d_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 256)          196864      ['global_average_pooling1d_11[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)           (None, 32)           0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_45 (Dropout)           (None, 32)           0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_46 (Dropout)           (None, 32)           0           ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_47 (Dropout)           (None, 32)           0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_48 (Dropout)           (None, 32)           0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)           (None, 256)          0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 8)            264         ['dropout_44[0][0]']             \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8)            264         ['dropout_45[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 8)            264         ['dropout_46[0][0]']             \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 8)            264         ['dropout_47[0][0]']             \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 8)            264         ['dropout_48[0][0]']             \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 64)           16448       ['dropout_49[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 104)          0           ['dense_16[0][0]',               \n",
      "                                                                  'dense_18[0][0]',               \n",
      "                                                                  'dense_20[0][0]',               \n",
      "                                                                  'dense_22[0][0]',               \n",
      "                                                                  'dense_24[0][0]',               \n",
      "                                                                  'dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 512)          53760       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 512)          0           ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 256)          131328      ['dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 1)            257         ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,005,257\n",
      "Trainable params: 110,005,257\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/johnchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210501_20211231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2217/2217 [00:21<00:00, 103.62it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 181.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [01:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing colbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "80it [00:00, 407.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 20)\n",
      "20210501_20211231 done orignial size:  169377  final size: 80\n",
      "20210101_20210430 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 975/975 [00:11<00:00, 83.80it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 204.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:30<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing colbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 374.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 20)\n",
      "20210101_20210430 done orignial size:  70843  final size: 25\n",
      "20200101_20201231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6691/6691 [02:59<00:00, 37.28it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:04<00:00, 65.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 227/227 [03:27<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing colbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:00, 412.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227, 20)\n",
      "20200101_20201231 done orignial size:  512226  final size: 227\n",
      "20190101_20191231 Start assemble comment(have fun indicater) with all of its comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6344/6344 [19:59<00:00,  5.29it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 221/221 [00:02<00:00, 79.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 114/173 [01:44<00:53,  1.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f0/3f71ctjx64zgpnygz80qt4bw0000gn/T/ipykernel_21217/2941726305.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0minput_categories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'combined'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'doing gpt2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mfinal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gpt2_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcom_svalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reply_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'doing colbert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8739\u001b[0m         )\n\u001b[0;32m-> 8740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8742\u001b[0m     def applymap(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m                     \u001b[0;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/f0/3f71ctjx64zgpnygz80qt4bw0000gn/T/ipykernel_21217/2941726305.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0minput_categories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'combined'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'doing gpt2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mfinal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gpt2_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcom_svalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reply_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'doing colbert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/f0/3f71ctjx64zgpnygz80qt4bw0000gn/T/ipykernel_21217/2941726305.py\u001b[0m in \u001b[0;36mcom_svalue\u001b[0;34m(setup, punchline)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mall_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunchline_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mlog_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_logsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, labels, training, **kwargs)\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m         )\n\u001b[1;32m    946\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cache\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_attentions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             )\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, training)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         )\n\u001b[1;32m    264\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_attn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, training)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3712\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3713\u001b[0m         return gen_math_ops.mat_mul(\n\u001b[0;32m-> 3714\u001b[0;31m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   3715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/SP/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6013\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   6014\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6015\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   6016\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6017\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import networkx as nx\n",
    "# from pyvis.network import Network\n",
    "import subprocess\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "colber_model = tf.keras.models.load_model(\"../Colbert/colbert-trained/\")\n",
    "colber_model.summary()\n",
    "\n",
    "# import keras\n",
    "\n",
    "# colber_model = keras.models.load_model(\"../Colbert/colbert-trained/\")\n",
    "# colber_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "colber_tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)\n",
    "\n",
    "\n",
    "def score(sentence):\n",
    "    tokenize_input = colber_tokenizer.encode(sentence)\n",
    "    tensor_input = torch.tensor([tokenize_input])\n",
    "    loss=colber_model(tensor_input, labels=tensor_input)[0]\n",
    "    return np.exp(loss.detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "    inputs = colber_tokenizer.encode_plus(str1, str2,\n",
    "        add_special_tokens=True,\n",
    "        max_length=length,\n",
    "        truncation_strategy=truncation_strategy)\n",
    "\n",
    "    input_ids =  inputs[\"input_ids\"]\n",
    "    input_masks = [1] * len(input_ids)\n",
    "    input_segments = inputs[\"token_type_ids\"]\n",
    "    padding_length = length - len(input_ids)\n",
    "    padding_id = colber_tokenizer.pad_token_id\n",
    "    input_ids = input_ids + ([padding_id] * padding_length)\n",
    "    input_masks = input_masks + ([0] * padding_length)\n",
    "    input_segments = input_segments + ([0] * padding_length)\n",
    "\n",
    "    return [input_ids, input_masks, input_segments]\n",
    "\n",
    "\n",
    "def compute_input_arrays(df, columns, colber_tokenizer):\n",
    "    model_input = []\n",
    "    for xx in range((MAX_SENTENCES*3)+3):\n",
    "        model_input.append([])\n",
    "    \n",
    "    for _, row in tqdm(df[columns].iterrows()):\n",
    "        i = 0\n",
    "        \n",
    "        # sent\n",
    "        sentences = sent_tokenize(row.combined)\n",
    "        for xx in range(MAX_SENTENCES):\n",
    "            s = sentences[xx] if xx<len(sentences) else ''\n",
    "            ids_q, masks_q, segments_q = return_id(s, None, 'longest_first', MAX_SENTENCE_LENGTH)\n",
    "            model_input[i].append(ids_q)\n",
    "            i+=1\n",
    "            model_input[i].append(masks_q)\n",
    "            i+=1\n",
    "            model_input[i].append(segments_q)\n",
    "            i+=1\n",
    "        \n",
    "        # full row\n",
    "        ids_q, masks_q, segments_q = return_id(row.combined, None, 'longest_first', MAX_LENGTH)\n",
    "        model_input[i].append(ids_q)\n",
    "        i+=1\n",
    "        model_input[i].append(masks_q)\n",
    "        i+=1\n",
    "        model_input[i].append(segments_q)\n",
    "        \n",
    "    for xx in range((MAX_SENTENCES*3)+3):\n",
    "        model_input[xx] = np.asarray(model_input[xx], dtype=np.int32)\n",
    "        \n",
    "    print(model_input[0].shape)\n",
    "    return model_input\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from torch.nn import functional as F\n",
    "from scipy.special import softmax\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "#cuda=True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "# with torch.no_grad():\n",
    "#         model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "#         model.eval()\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2-large', pad_token_id = tokenizer.eos_token_id, return_dict = True)\n",
    "\n",
    "\n",
    "\n",
    "def com_svalue(setup,punchline):\n",
    "    if setup[-1] in punc:\n",
    "        sentence=setup+' '+punchline\n",
    "    else:\n",
    "        sentence=setup+'. '+punchline\n",
    "\n",
    "\n",
    "    setup_ids = tokenizer.encode(setup, return_tensors = 'tf')\n",
    "    punchline_ids = tokenizer.encode(punchline, return_tensors = 'tf')\n",
    "\n",
    "    all_ids = tokenizer.encode(sentence, return_tensors = 'tf')\n",
    "\n",
    "    outputs = model(all_ids[:,:-1])\n",
    "    logits = outputs.logits[0,-(punchline_ids.shape[1]):,:]\n",
    "    log_scores = logits - tf.expand_dims(tf.reduce_logsumexp(logits, 1), 1)\n",
    "\n",
    "\n",
    "    labels = tf.one_hot(all_ids[0,-(punchline_ids.shape[1]):], logits.shape[-1])\n",
    "    if labels.shape[0] == log_scores.shape[0]:\n",
    "        loss = -tf.reduce_mean(tf.reduce_sum(labels * log_scores, 1))\n",
    "        return loss.numpy()\n",
    "    else:\n",
    "        return -1.0  # Ignore examples with surprisal value == -1.0\n",
    "\n",
    "\n",
    "\n",
    "def filter_per(x):\n",
    "    num_com=len(x)\n",
    "    if num_com>=2:     \n",
    "        df=x[(x['com_text'].str.contains('|'.join(searchfor)))|((x['com_text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "        num_fun=len(df)\n",
    "        per=num_fun/num_com\n",
    "        if  per>0.4:\n",
    "            return df    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def concate_fun(x): \n",
    "    src_id=x.split('_')[1]\n",
    "    if x.startswith('t3'):\n",
    "        df=org_submission_df[org_submission_df['id']==src_id].head(1)\n",
    "        if len(df)>0:\n",
    "            return df['text'].item()\n",
    "    else:\n",
    "        df=org_comment_df[org_comment_df['id']==src_id].head(1)\n",
    "        if len(df)>0:\n",
    "            return df['text'].item() \n",
    "\n",
    "MAX_SENTENCE_LENGTH = 20\n",
    "MAX_SENTENCES = 5\n",
    "MAX_LENGTH = 100\n",
    "punc = string.punctuation\n",
    "searchfor=['<3','xd',';D',':D',\n",
    "           'ðŸ™ƒ','ðŸ˜‚','ðŸ¤£', \n",
    "           'funny','hilarious','kidding','laughing so hard','going nuts','so funny','Laughing','laughing hard','jokes',\n",
    "           'haha','hahah','hahaha','hahahaha','ahaha','Hehehehe',\n",
    "           'hehe',\n",
    "           'Hehehehe','kkkk',\n",
    "           'Lmao','lmaooo','lmaoo',\n",
    "           'LOL','Lol','lol',\n",
    "#            '\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "           ]\n",
    "\n",
    "emoticon=['<3','xd',';D']\n",
    "emoji=['ðŸ™ƒ','ðŸ˜‚','ðŸ¤£']\n",
    "\n",
    "# regrex_pattern=r'\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "regrex_pattern='a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+|:D+'\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "   '20210501_20211231'\n",
    "    , '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "for date_suffix in date_suffices:\n",
    "\n",
    "    org_comment_df = pd.read_csv('../data/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    org_submission_df = pd.read_csv('../data/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    #bigger than 50 percentage of coment are hahaha and bigger than tow comments\n",
    "#     filtered_df= pd.read_csv('../data/final_q2/casual_conv_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    ##########################################################################################################generating filtered_df\n",
    "    first_comment_df=org_comment_df[(org_comment_df['text'].str.contains('|'.join(searchfor)))|((org_comment_df['text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "    t1_ids=first_comment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    #submission=>fun_comment=>kidcomment\n",
    "    #t1_ids point to fun_comment\n",
    "    filtered_ids=[x for x in t1_ids]\n",
    "    final_cols = ['src_id', 'src_parent','src_type', 'src_summarized', 'src_from', 'src_text', 'src_root', 'src_length',\n",
    "                  'com_id', 'com_summarized', 'com_text', 'com_length','com_score']\n",
    "    final_dict = {col: [] for col in final_cols}\n",
    "    \n",
    "    #submission=>fun_comment=>kidcomment\n",
    "    #fun_comment df\n",
    "    comment_have_kid_df = org_comment_df[org_comment_df['id'].isin(t1_ids)]\n",
    "    print(date_suffix,'Start assemble comment(have fun indicater) with all of its comment')\n",
    "    for k in tqdm(range(comment_have_kid_df.shape[0])):       \n",
    "        src_id = comment_have_kid_df.iloc[k]['id']\n",
    "        src_parent=comment_have_kid_df.iloc[k]['parent_id']\n",
    "        src_summarized = comment_have_kid_df.iloc[k]['summarized']\n",
    "        src_text = comment_have_kid_df.iloc[k]['text']\n",
    "        src_length = comment_have_kid_df.iloc[k]['length']\n",
    "        fun_comment_score=comment_have_kid_df.iloc[k]['score']\n",
    "    #submission=>fun_comment=>kidcomment\n",
    "    #kidcomment_df\n",
    "        comment_comment_df = org_comment_df[org_comment_df['parent_id'] == 't1_' + src_id]\n",
    "        for m in range(comment_comment_df.shape[0]):\n",
    "            final_dict['src_id'].append(src_id)\n",
    "            final_dict['src_parent'].append(src_parent)\n",
    "            final_dict['src_type'].append('com')\n",
    "            final_dict['src_summarized'].append(src_summarized)\n",
    "            final_dict['src_from'].append('comment')\n",
    "            final_dict['src_text'].append(src_text)\n",
    "            final_dict['src_root'].append('none')\n",
    "            final_dict['src_length'].append(src_length)\n",
    "            \n",
    "            final_dict['com_id'].append(comment_comment_df.iloc[m]['id'])\n",
    "            final_dict['com_summarized'].append(comment_comment_df.iloc[m]['summarized'])\n",
    "            final_dict['com_text'].append(comment_comment_df.iloc[m]['text'])\n",
    "            final_dict['com_length'].append(comment_comment_df.iloc[m]['length'])\n",
    "            \n",
    "            \n",
    "            final_dict['com_score'].append(fun_comment_score)\n",
    "\n",
    "    filtered_df = pd.DataFrame(final_dict)\n",
    "    filtered_df=filtered_df.drop_duplicates()\n",
    "    filtered_df=filtered_df.drop(['src_type', 'src_summarized', 'src_from','src_root', 'src_length','com_summarized','com_length'], axis=1)\n",
    "    ########################################################################################################################################\n",
    "    \n",
    "    final_df=filtered_df.groupby(\"src_id\").apply(filter_per)\n",
    "    if len(final_df)>0:\n",
    "        final_df.insert(0,'reply_text','')\n",
    "        final_df['reply_text']=final_df.progress_apply(lambda x:concate_fun(x['src_parent']),axis=1)\n",
    "        final_df=final_df.dropna(subset=['reply_text'])\n",
    "        final_df=final_df.drop_duplicates()\n",
    "        #['reply_text', 'src_id', 'src_parent', 'src_text', 'com_id', 'com_text','com_score']\n",
    "        final_df['combined'] = final_df.apply(lambda x:(x['reply_text']+' '+x['src_text']) if x['reply_text'][-1] in punc else (x['reply_text']+'. '+x['src_text']),axis=1)\n",
    "\n",
    "        input_categories=['combined']\n",
    "        print('doing gpt2')\n",
    "        final_df['gpt2_score']=final_df.progress_apply(lambda x:com_svalue(x['reply_text'],x['src_text']),axis=1)\n",
    "\n",
    "        print('doing colbert')\n",
    "        test_inputs = compute_input_arrays(final_df, input_categories, colber_tokenizer)\n",
    "        final_df['colbert_score']=colber_model.predict(test_inputs)\n",
    "\n",
    "        final_df=final_df.drop(['reply_text', 'src_parent', 'com_id', 'com_text'], axis=1)\n",
    "        final_df.to_csv('../data/test1/casual_conv_final_{}.csv'.format(date_suffix), index = False)\n",
    "        \n",
    "        print(date_suffix,'done','orignial size: ',len(org_comment_df),' final size:',len(final_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349468b",
   "metadata": {},
   "source": [
    "# only for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ff433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import networkx as nx\n",
    "# from pyvis.network import Network\n",
    "import subprocess\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "colber_model = tf.keras.models.load_model(\"../Colbert/colbert-trained/\")\n",
    "colber_model.summary()\n",
    "\n",
    "# import keras\n",
    "\n",
    "# colber_model = keras.models.load_model(\"../Colbert/colbert-trained/\")\n",
    "# colber_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "colber_tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)\n",
    "\n",
    "\n",
    "def score(sentence):\n",
    "    tokenize_input = colber_tokenizer.encode(sentence)\n",
    "    tensor_input = torch.tensor([tokenize_input])\n",
    "    loss=colber_model(tensor_input, labels=tensor_input)[0]\n",
    "    return np.exp(loss.detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "    inputs = colber_tokenizer.encode_plus(str1, str2,\n",
    "        add_special_tokens=True,\n",
    "        max_length=length,\n",
    "        truncation_strategy=truncation_strategy)\n",
    "\n",
    "    input_ids =  inputs[\"input_ids\"]\n",
    "    input_masks = [1] * len(input_ids)\n",
    "    input_segments = inputs[\"token_type_ids\"]\n",
    "    padding_length = length - len(input_ids)\n",
    "    padding_id = colber_tokenizer.pad_token_id\n",
    "    input_ids = input_ids + ([padding_id] * padding_length)\n",
    "    input_masks = input_masks + ([0] * padding_length)\n",
    "    input_segments = input_segments + ([0] * padding_length)\n",
    "\n",
    "    return [input_ids, input_masks, input_segments]\n",
    "\n",
    "\n",
    "def compute_input_arrays(df, columns, colber_tokenizer):\n",
    "    model_input = []\n",
    "    for xx in range((MAX_SENTENCES*3)+3):\n",
    "        model_input.append([])\n",
    "    \n",
    "    for _, row in tqdm(df[columns].iterrows()):\n",
    "        i = 0\n",
    "        \n",
    "        # sent\n",
    "        sentences = sent_tokenize(row.combined)\n",
    "        for xx in range(MAX_SENTENCES):\n",
    "            s = sentences[xx] if xx<len(sentences) else ''\n",
    "            ids_q, masks_q, segments_q = return_id(s, None, 'longest_first', MAX_SENTENCE_LENGTH)\n",
    "            model_input[i].append(ids_q)\n",
    "            i+=1\n",
    "            model_input[i].append(masks_q)\n",
    "            i+=1\n",
    "            model_input[i].append(segments_q)\n",
    "            i+=1\n",
    "        \n",
    "        # full row\n",
    "        ids_q, masks_q, segments_q = return_id(row.combined, None, 'longest_first', MAX_LENGTH)\n",
    "        model_input[i].append(ids_q)\n",
    "        i+=1\n",
    "        model_input[i].append(masks_q)\n",
    "        i+=1\n",
    "        model_input[i].append(segments_q)\n",
    "        \n",
    "    for xx in range((MAX_SENTENCES*3)+3):\n",
    "        model_input[xx] = np.asarray(model_input[xx], dtype=np.int32)\n",
    "        \n",
    "    print(model_input[0].shape)\n",
    "    return model_input\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from torch.nn import functional as F\n",
    "from scipy.special import softmax\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "#cuda=True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "# with torch.no_grad():\n",
    "#         model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "#         model.eval()\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2-large', pad_token_id = tokenizer.eos_token_id, return_dict = True)\n",
    "\n",
    "\n",
    "\n",
    "def com_svalue(setup,punchline):\n",
    "    if setup[-1] in punc:\n",
    "        sentence=setup+' '+punchline\n",
    "    else:\n",
    "        sentence=setup+'. '+punchline\n",
    "\n",
    "\n",
    "    setup_ids = tokenizer.encode(setup, return_tensors = 'tf')\n",
    "    punchline_ids = tokenizer.encode(punchline, return_tensors = 'tf')\n",
    "\n",
    "    all_ids = tokenizer.encode(sentence, return_tensors = 'tf')\n",
    "\n",
    "    outputs = model(all_ids[:,:-1])\n",
    "    logits = outputs.logits[0,-(punchline_ids.shape[1]):,:]\n",
    "    log_scores = logits - tf.expand_dims(tf.reduce_logsumexp(logits, 1), 1)\n",
    "\n",
    "\n",
    "    labels = tf.one_hot(all_ids[0,-(punchline_ids.shape[1]):], logits.shape[-1])\n",
    "    if labels.shape[0] == log_scores.shape[0]:\n",
    "        loss = -tf.reduce_mean(tf.reduce_sum(labels * log_scores, 1))\n",
    "        return loss.numpy()\n",
    "    else:\n",
    "        return -1.0  # Ignore examples with surprisal value == -1.0\n",
    "\n",
    "\n",
    "\n",
    "def filter_per(x):\n",
    "    num_com=len(x)\n",
    "    if num_com>=2:     \n",
    "        df=x[(x['com_text'].str.contains('|'.join(searchfor)))|((x['com_text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "        num_fun=len(df)\n",
    "        per=num_fun/num_com\n",
    "        if  per>0.4:\n",
    "            return df    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def concate_fun(x): \n",
    "    src_id=x.split('_')[1]\n",
    "    if x.startswith('t3'):\n",
    "        df=org_submission_df[org_submission_df['id']==src_id].head(1)\n",
    "        if len(df)>0:\n",
    "            return df['text'].item()\n",
    "       \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "MAX_SENTENCE_LENGTH = 20\n",
    "MAX_SENTENCES = 5\n",
    "MAX_LENGTH = 100\n",
    "punc = string.punctuation\n",
    "searchfor=['<3','xd',';D',':D',\n",
    "           'ðŸ™ƒ','ðŸ˜‚','ðŸ¤£', \n",
    "           'funny','hilarious','kidding','laughing so hard','going nuts','so funny','Laughing','laughing hard','jokes',\n",
    "           'haha','hahah','hahaha','hahahaha','ahaha','Hehehehe',\n",
    "           'hehe',\n",
    "           'Hehehehe','kkkk',\n",
    "           'Lmao','lmaooo','lmaoo',\n",
    "           'LOL','Lol','lol',\n",
    "#            '\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "           ]\n",
    "\n",
    "emoticon=['<3','xd',';D']\n",
    "emoji=['ðŸ™ƒ','ðŸ˜‚','ðŸ¤£']\n",
    "\n",
    "# regrex_pattern=r'\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+)\\b'\n",
    "regrex_pattern='a*ha+h[ha]*|o?l+o+l+[ol]*|e*he+h[he]*|lmao+|:D+'\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "   '20210501_20211231'\n",
    "    , '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "                 '20160101_20161231'\n",
    "                ]\n",
    "for date_suffix in date_suffices:\n",
    "\n",
    "    org_comment_df = pd.read_csv('../data/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    org_submission_df = pd.read_csv('../data/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    #bigger than 50 percentage of coment are hahaha and bigger than tow comments\n",
    "#     filtered_df= pd.read_csv('../data/final_q2/casual_conv_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    ##########################################################################################################generating filtered_df\n",
    "    first_comment_df=org_comment_df[(org_comment_df['text'].str.contains('|'.join(searchfor)))|((org_comment_df['text'].str.contains(regrex_pattern,regex= True, na=False)))]\n",
    "    t1_ids=first_comment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    #submission=>fun_comment=>kidcomment\n",
    "    #t1_ids point to fun_comment\n",
    "    filtered_ids=[x for x in t1_ids]\n",
    "    final_cols = ['src_id', 'src_parent','src_type', 'src_summarized', 'src_from', 'src_text', 'src_root', 'src_length',\n",
    "                  'com_id', 'com_summarized', 'com_text', 'com_length','com_score']\n",
    "    final_dict = {col: [] for col in final_cols}\n",
    "    \n",
    "    #submission=>fun_comment=>kidcomment\n",
    "    #fun_comment df\n",
    "    comment_have_kid_df = org_comment_df[org_comment_df['id'].isin(t1_ids)]\n",
    "    print(date_suffix,'Start assemble comment(have fun indicater) with all of its comment')\n",
    "    for k in tqdm(range(comment_have_kid_df.shape[0])):       \n",
    "        src_id = comment_have_kid_df.iloc[k]['id']\n",
    "        src_parent=comment_have_kid_df.iloc[k]['parent_id']\n",
    "        src_summarized = comment_have_kid_df.iloc[k]['summarized']\n",
    "        src_text = comment_have_kid_df.iloc[k]['text']\n",
    "        src_length = comment_have_kid_df.iloc[k]['length']\n",
    "        fun_comment_score=comment_have_kid_df.iloc[k]['score']\n",
    "    #submission=>fun_comment=>kidcomment\n",
    "    #kidcomment_df\n",
    "        comment_comment_df = org_comment_df[org_comment_df['parent_id'] == 't1_' + src_id]\n",
    "        for m in range(comment_comment_df.shape[0]):\n",
    "            final_dict['src_id'].append(src_id)\n",
    "            final_dict['src_parent'].append(src_parent)\n",
    "            final_dict['src_type'].append('com')\n",
    "            final_dict['src_summarized'].append(src_summarized)\n",
    "            final_dict['src_from'].append('comment')\n",
    "            final_dict['src_text'].append(src_text)\n",
    "            final_dict['src_root'].append('none')\n",
    "            final_dict['src_length'].append(src_length)\n",
    "            \n",
    "            final_dict['com_id'].append(comment_comment_df.iloc[m]['id'])\n",
    "            final_dict['com_summarized'].append(comment_comment_df.iloc[m]['summarized'])\n",
    "            final_dict['com_text'].append(comment_comment_df.iloc[m]['text'])\n",
    "            final_dict['com_length'].append(comment_comment_df.iloc[m]['length'])\n",
    "            \n",
    "            \n",
    "            final_dict['com_score'].append(fun_comment_score)\n",
    "\n",
    "    filtered_df = pd.DataFrame(final_dict)\n",
    "    filtered_df=filtered_df.drop_duplicates()\n",
    "    filtered_df=filtered_df.drop(['src_type', 'src_summarized', 'src_from','src_root', 'src_length','com_summarized','com_length'], axis=1)\n",
    "    ########################################################################################################################################\n",
    "    \n",
    "    final_df=filtered_df.groupby(\"src_id\").apply(filter_per)\n",
    "    if len(final_df)>0:\n",
    "        final_df.insert(0,'reply_text','')\n",
    "        final_df['reply_text']=final_df.progress_apply(lambda x:concate_fun(x['src_parent']),axis=1)\n",
    "        final_df=final_df.dropna(subset=['reply_text'])\n",
    "        final_df=final_df.drop_duplicates()\n",
    "        #['reply_text', 'src_id', 'src_parent', 'src_text', 'com_id', 'com_text','com_score']\n",
    "        final_df['combined'] = final_df.apply(lambda x:(x['reply_text']+' '+x['src_text']) if x['reply_text'][-1] in punc else (x['reply_text']+'. '+x['src_text']),axis=1)\n",
    "\n",
    "        input_categories=['combined']\n",
    "        print('doing gpt2')\n",
    "        final_df['gpt2_score']=final_df.progress_apply(lambda x:com_svalue(x['reply_text'],x['src_text']),axis=1)\n",
    "\n",
    "        print('doing colbert')\n",
    "        test_inputs = compute_input_arrays(final_df, input_categories, colber_tokenizer)\n",
    "        final_df['colbert_score']=colber_model.predict(test_inputs)\n",
    "\n",
    "        final_df=final_df.drop(['reply_text', 'src_parent', 'com_id', 'com_text'], axis=1)\n",
    "        final_df.to_csv('../data/test1/casual_conv_final_{}.csv'.format(date_suffix), index = False)\n",
    "        \n",
    "        print(date_suffix,'done','orignial size: ',len(org_comment_df),' final size:',len(final_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c1b2d",
   "metadata": {},
   "source": [
    "# Filter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da019a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b6404db",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb4bb85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.653183"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup='i am a human'\n",
    "punchline='yes you are'\n",
    "\n",
    "value=com_svalue(setup,punchline)\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a70fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_suffices = [\n",
    "   '20210501_20211231'\n",
    "    , '20210101_20210430', '20200101_20201231',\n",
    "                  '20190101_20191231', '20180101_20181231', '20170101_20171231',\n",
    "#                  '20160101_20161231'\n",
    "                ]\n",
    "\n",
    "df_all = pd.DataFrame(columns = [\"src_id\", \"com_score\", \"combined\", \"gpt2_score\",'colbert_score'])\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "        df = pd.read_csv('../data/result/casual_conv_final_{}.csv'.format(date_suffix))\n",
    "        df=df[(df['com_score']>=2) & (df['gpt2_score']>8) &(df['colbert_score']>0.9)]\n",
    "#         df=df[(df['com_score']>=2) & (df['gpt2_score']>5) &(df['colbert_score']>0.)]\n",
    "        df_all=df_all.append(df)\n",
    "\n",
    "\n",
    "df_all.to_csv('../data/result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbd27ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3469387755102041 percentage of fun\n"
     ]
    }
   ],
   "source": [
    "print(17/49,'percentage of fun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87052b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "####not kids indicate\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "date_suffices = [\n",
    "    '20210101_20211231', '20200101_20201231', '20180101_20181231', '20170101_20171231',\n",
    "                ]\n",
    "\n",
    "df_all = pd.DataFrame(columns = [\"humor\", \"reply_text\", \"src_text\", \"com_score\",'combined'])\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "        df = pd.read_csv('../pushshift/final/askreddit_final_{}.csv'.format(date_suffix))\n",
    "#         df=df[(df['gpt2_score']>8) &(df['colbert_score']>0.9)]\n",
    "#         df=df[(df['com_score']>=2) & (df['gpt2_score']>5) &(df['colbert_score']>0.)]\n",
    "        df_all=df_all.append(df,ignore_index=True)\n",
    "\n",
    "df_all = shuffle(df_all)\n",
    "df2 = df_all.rename({'reply_text': 'source', 'src_text': 'reply'}, axis=1)\n",
    "\n",
    "df2.to_csv('../pushshift/luizcheck.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "askreddit_final_20210101_20211231"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
