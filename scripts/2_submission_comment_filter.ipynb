{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90a3716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "import neuralcoref\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0aa9b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/johnchen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a4fcc",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77a00609",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bde66b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fa687be9610>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9724b1",
   "metadata": {},
   "source": [
    "# The SMMRY Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d692b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_phrases = ['thus', 'for example', 'for instance', 'namely', 'to illustrate',\n",
    "                      'in other words', 'in particular', 'specifically', 'such as',\n",
    "                      'on the contrary', 'contrarily', 'notwithstanding', 'but', 'however',\n",
    "                      'nevertheless', 'in spite of', 'in contrast', 'yet', 'on one hand',\n",
    "                      'on the other hand', 'rather', 'or', 'nor', 'conversely', 'at the same time',\n",
    "                      'while this may be true', 'and', 'in addition to', 'furthermore',\n",
    "                      'moreover', 'besides', 'than', 'too', 'also', 'both-and', 'another',\n",
    "                      'equally important', 'second', 'etc.', 'again', 'further', 'last',\n",
    "                      'finally', 'not only-but also', 'as well as', 'in the second place',\n",
    "                      'next', 'likewise', 'similarly', 'in fact', 'as a result', 'consequently',\n",
    "                      'in the same way', 'for example', 'for instance', 'however', 'thus',\n",
    "                      'therefore', 'otherwise', 'after that', 'afterward', 'then', 'next',\n",
    "                      'last', 'at last', 'at length', 'at first', 'formerly', 'another', 'finally',\n",
    "                      'meanwhile', 'at the same time', 'afterwards', 'subsequently',\n",
    "                      'in the meantime', 'eventually', 'concurrently', 'simultaneously', 'although',\n",
    "                      'at least', 'still', 'even though', 'granted that', 'while it may be true',\n",
    "                      'in spite of', 'of course', 'similarly', 'likewise', 'in like fashion',\n",
    "                      'in like manner', 'analogous to', 'above all', 'indeed', 'of course',\n",
    "                      'certainly', 'surely', 'in fact', 'really', 'in truth', 'again', 'besides',\n",
    "                      'also', 'furthermore', 'in addition', 'specifically', 'especially',\n",
    "                      'in particular', 'to explain', 'to list', 'to enumerate', 'in detail',\n",
    "                      'namely', 'including', 'for example', 'for instance', 'to illustrate',\n",
    "                      'thus', 'in other words', 'as an illustration', 'in particular', 'so that',\n",
    "                      'with the result that', 'consequently', 'hence', 'accordingly', 'for this reason',\n",
    "                      'therefore', 'because', 'due to', 'as a result', 'in other words', 'then',\n",
    "                      'therefore', 'finally', 'consequently', 'thus', 'in conclusion', 'as a result',\n",
    "                      'accordingly', 'for this purpose', 'to this end', 'with this in mind',\n",
    "                      'with this purpose in mind', 'therefore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dff0f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_start(first_sent, dialog_turn):\n",
    "    if dialog_turn == 1:\n",
    "        for phrase in transition_phrases:\n",
    "            if first_sent.lower().startswith(phrase):\n",
    "                return True\n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5d5c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smmry(text, doc, sent_count, dialog_turn):\n",
    "\n",
    "    # some preprocessing to omit text within brackets and replace u with you. \n",
    "    \n",
    "    # text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    # text = text.replace(' u ', ' you ')\n",
    "\n",
    "    formatted_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    formatted_text = re.sub(r'\\s+', ' ', formatted_text)\n",
    "\n",
    "    # doc = nlp(text)\n",
    "\n",
    "    fdist = {}\n",
    "    word_arr = nltk.word_tokenize(formatted_text.lower())\n",
    "\n",
    "    # preparing a frequency dictionary without considering stop words\n",
    "    \n",
    "    for word in word_arr:\n",
    "        if not word in stop_words:\n",
    "            word = wnl.lemmatize(word)\n",
    "            if word not in fdist.keys():\n",
    "                    fdist[word] = 1\n",
    "            else:\n",
    "                    fdist[word] += 1\n",
    "\n",
    "    sent_arr = nltk.sent_tokenize(text)\n",
    "    sent_score_arr = []\n",
    "    summary_arr = []\n",
    "\n",
    "    sent_arr_coref_resolved = nltk.sent_tokenize(doc._.coref_resolved)\n",
    "\n",
    "    # compute scores for each sentence\n",
    "\n",
    "    for sent in sent_arr:\n",
    "        score = 0\n",
    "        token_arr = nltk.word_tokenize(sent.lower())\n",
    "        for word in token_arr:\n",
    "            word = wnl.lemmatize(word)\n",
    "            if word in fdist.keys():\n",
    "                score += fdist[word]\n",
    "\n",
    "        sent_score_arr.append(score/len(token_arr))\n",
    "\n",
    "    sent_score_arr = np.array(sent_score_arr)\n",
    "\n",
    "    all_ind_arr = sent_score_arr.argsort()[-len(sent_score_arr):][::-1]\n",
    "\n",
    "    ind_arr_unsorted = sent_score_arr.argsort()[-sent_count:][::-1]\n",
    "\n",
    "    ind_arr = np.sort(ind_arr_unsorted) \n",
    "\n",
    "    summary = ''\n",
    "    changed_first = False\n",
    "\n",
    "    if len(ind_arr) > 0:\n",
    "\n",
    "        try:\n",
    "\n",
    "            ind = ind_arr[0]\n",
    "            first_sent = sent_arr[ind]\n",
    "\n",
    "            while (first_sent != sent_arr_coref_resolved[ind] or transition_start(first_sent, dialog_turn)):\n",
    "                changed_first = True\n",
    "                for index in all_ind_arr:\n",
    "                    if index < ind:\n",
    "                        ind = index\n",
    "                        break\n",
    "                first_sent = sent_arr[ind]\n",
    "                if ind == 0:\n",
    "                    break\n",
    "            summary = summary + first_sent + ' '     \n",
    "            \n",
    "            if (changed_first):\n",
    "                first_ind = ind\n",
    "                sent_score_modified = sent_score_arr[first_ind+1:]\n",
    "                ind_arr_unsorted = sent_score_modified.argsort()[-(sent_count-1):][::-1]\n",
    "                ind_arr_next = np.sort(ind_arr_unsorted) \n",
    "                \n",
    "                for i in range(0, len(ind_arr_next)):\n",
    "                    ind = (first_ind+1) + ind_arr_next[i]\n",
    "                    if i == len(ind_arr_next) - 1:\n",
    "                        summary = summary + sent_arr[ind]\n",
    "                    else:\n",
    "                        summary = summary + sent_arr[ind] + ' '\n",
    "            \n",
    "            else:\n",
    "                for i in range(1, len(ind_arr)):\n",
    "                    ind = ind_arr[i]\n",
    "                    if i == len(ind_arr) - 1:\n",
    "                        summary = summary + sent_arr[ind]\n",
    "                    else:\n",
    "                        summary = summary + sent_arr[ind] + ' '\n",
    "\n",
    "            return summary\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(\"EXCEPTION occured\")\n",
    "            return text\n",
    "\n",
    "    else:\n",
    "        print(text)\n",
    "        print(sent_arr)\n",
    "        print(\"EXCEPTION occured: length of sentence array is not > 0\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a57f2f",
   "metadata": {},
   "source": [
    "# Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8af79b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux_verbs = ['be', 'can', 'could', 'dare', 'do', 'have', 'may', 'might', 'must',\n",
    "#              'need', 'ought', 'shall', 'should', 'will', 'would']\n",
    "# wh_words = ['what', 'when', 'where', 'which', 'who', 'whom', 'whose', 'why', 'how']\n",
    "# q_words = aux_verbs + wh_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2c1259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw(text):\n",
    "    # Check if text is a str\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    # Replace HTML escape chars\n",
    "    text = text.replace('&gt;', '>')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('#x200B;', ' ')\n",
    "    text = text.replace('nbsp;', ' ')\n",
    "\n",
    "    # Remove brackets\n",
    "    b_pattern = re.compile(r'(\\([^\\(\\)]*\\))|(\\[[^\\[\\]]*\\])')\n",
    "    while b_pattern.search(text):\n",
    "        text = re.sub(r'(\\([^\\(\\)]*\\))|(\\[[^\\[\\]]*\\])', '', text)\n",
    "\n",
    "    # Remove redundant spaces (including breaklines)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Check if text is empty\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Check if text is [deleted] or [removed]\n",
    "    if text_lower == '[deleted]' or text_lower == '[removed]':\n",
    "        return None\n",
    "\n",
    "    # Check if text contains URL\n",
    "    url_pattern = re.compile(r'[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "    if url_pattern.search(text_lower):\n",
    "        return None\n",
    "\n",
    "    # Check if text contains 'r/<subreddit>' or 'u/<username>'\n",
    "    r_pattern = re.compile(r'(^| )\\/?r\\/[^ ]*')\n",
    "    if r_pattern.search(text_lower):\n",
    "        return None\n",
    "    u_pattern = re.compile(r'(^| )\\/?u\\/[^ ]*')\n",
    "    if u_pattern.search(text_lower):\n",
    "        return None\n",
    "\n",
    "    # Check if text contains 'reddit'\n",
    "    if 'reddit' in text_lower:\n",
    "        return None\n",
    "\n",
    "    # Check the percentage of alphabetical letters\n",
    "    num_alphas = 0\n",
    "    for ch in text:\n",
    "        if ch.isalpha():\n",
    "            num_alphas += 1\n",
    "    if num_alphas / len(text) < 0.7:\n",
    "        return None\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Check the number of tokens\n",
    "    if len(doc) < 2:\n",
    "        return None\n",
    "\n",
    "    return {'text': text, 'doc': doc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d84f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_summary(text):\n",
    "    # Check if text is a str\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    # Remove redundant spaces (including breaklines)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Check if text is empty\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # Check the percentage of alphabetical letters\n",
    "    num_alphas = 0\n",
    "    for ch in text:\n",
    "        if ch.isalpha():\n",
    "            num_alphas += 1\n",
    "    if num_alphas / len(text) < 0.7:\n",
    "        return None\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Check the number of tokens\n",
    "    if len(doc) < 2:\n",
    "        return None\n",
    "\n",
    "    return {'text': text, 'doc': doc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a6bf909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_root(text, sent):\n",
    "#     # Check if the last character is a question mark\n",
    "#     if sent[-1].text == '?':\n",
    "#         return None\n",
    "\n",
    "    if sent.root.pos_ == 'VERB':\n",
    "#         # Check the first token\n",
    "#         if sent[0].lemma_.lower() in q_words:\n",
    "#             return None\n",
    "        return sent.root.lemma_\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebf556c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(preprocessed_text, dialog_turn):\n",
    "    if preprocessed_text is None:\n",
    "        return None\n",
    "\n",
    "    text = preprocessed_text['text']\n",
    "    doc = preprocessed_text['doc']\n",
    "\n",
    "    summarized = 0\n",
    "    sents = [sent for sent in doc.sents]\n",
    "    if len(sents) > 1:\n",
    "        summarized = 1\n",
    "        summary = smmry(text, doc, 1, dialog_turn)\n",
    "        preprocessed_summary = preprocess_summary(summary)\n",
    "        if preprocessed_summary is None:\n",
    "            return None\n",
    "        summarized_text = preprocessed_summary['text']\n",
    "        summarized_doc = preprocessed_summary['doc']\n",
    "        summarized_sents = [sent for sent in summarized_doc.sents]\n",
    "        if len(summarized_sents) != 1:\n",
    "            return None\n",
    "    elif len(sents) == 1:\n",
    "        summarized_text = text\n",
    "        summarized_doc = doc\n",
    "        summarized_sents = sents\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if dialog_turn > 1:\n",
    "        return {'text': summarized_text, 'summarized': summarized, 'length': len(summarized_sents[0])}\n",
    "\n",
    "    root = extract_root(summarized_text, summarized_sents[0])\n",
    "    if root is not None:\n",
    "        return {'text': summarized_text, 'summarized': summarized, 'root': root, 'length': len(summarized_sents[0])}\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c0ef6",
   "metadata": {},
   "source": [
    "# Filter Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b560f26",
   "metadata": {},
   "source": [
    "## Batch Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f82ee93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26106, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 26106/26106 [04:48<00:00, 90.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19990, 7)\n",
      "(24595, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 24595/24595 [04:36<00:00, 88.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18486, 7)\n",
      "(23326, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 23326/23326 [04:24<00:00, 88.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17392, 7)\n",
      "(28113, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 28113/28113 [05:19<00:00, 87.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21241, 7)\n",
      "(32344, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 32344/32344 [06:12<00:00, 86.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25242, 7)\n"
     ]
    }
   ],
   "source": [
    "date_suffices = ['20170101_20171231', '20180101_20181231', '20190101_20191231',\n",
    "                 '20200101_20201231', '20210101_20211231']\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    submission_df = pd.read_csv('../pushshift/submission/askReddit_submission_{}.csv'.format(date_suffix))\n",
    "    print(submission_df.shape)\n",
    "    submission_filtered_cols = ['id', 'summarized', 'from', 'text', 'root','score', 'length']\n",
    "    submission_filtered_dict = {col: [] for col in submission_filtered_cols}\n",
    "    for i in tqdm(range(submission_df.shape[0])):\n",
    "        submission_id = submission_df.iloc[i]['id']\n",
    "        submission_score = submission_df.iloc[i]['score']\n",
    "        \n",
    "        title = submission_df.iloc[i]['title']\n",
    "        preprocessed_title = preprocess_raw(title)\n",
    "        summarized_title = summarize(preprocessed_title, dialog_turn = 1)\n",
    "        if summarized_title is not None:\n",
    "            submission_filtered_dict['id'].append(submission_id)\n",
    "            submission_filtered_dict['summarized'].append(summarized_title['summarized'])\n",
    "            submission_filtered_dict['from'].append('title')\n",
    "            submission_filtered_dict['text'].append(summarized_title['text'])\n",
    "            submission_filtered_dict['root'].append(summarized_title['root'])\n",
    "            submission_filtered_dict['score'].append(submission_score)\n",
    "            submission_filtered_dict['length'].append(summarized_title['length'])\n",
    "        else:\n",
    "            selftext = submission_df.iloc[i]['selftext']\n",
    "            preprocessed_selftext = preprocess_raw(selftext)\n",
    "            summarized_selftext = summarize(preprocessed_selftext, dialog_turn = 1)\n",
    "            if summarized_selftext is not None:\n",
    "                submission_filtered_dict['id'].append(submission_id)\n",
    "                submission_filtered_dict['summarized'].append(summarized_selftext['summarized'])\n",
    "                submission_filtered_dict['from'].append('selftext')\n",
    "                submission_filtered_dict['text'].append(summarized_selftext['text'])\n",
    "                submission_filtered_dict['root'].append(summarized_selftext['root'])\n",
    "                submission_filtered_dict['score'].append(submission_score)\n",
    "                submission_filtered_dict['length'].append(summarized_selftext['length'])\n",
    "    submission_filtered_df = pd.DataFrame(submission_filtered_dict)\n",
    "    print(submission_filtered_df.shape)\n",
    "    submission_filtered_df.to_csv('../pushshift/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144ba0f",
   "metadata": {},
   "source": [
    "# Filter Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be3cb1",
   "metadata": {},
   "source": [
    "## Batch Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe0ae02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████████████████████████████████▏ | 3223/3384 [02:33<00:07, 21.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3384/3384 [02:37<00:00, 21.54it/s]\n",
      " 19%|███████▎                              | 7957/41456 [04:47<15:07, 36.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████▉                             | 9755/41456 [05:52<17:44, 29.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████▊                        | 14353/41456 [07:53<37:37, 12.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████▌                      | 16264/41456 [08:48<07:27, 56.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████▍                | 22909/41456 [12:59<06:28, 47.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████▏               | 23734/41456 [13:33<09:06, 32.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████▉  | 39165/41456 [21:29<00:58, 39.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCEPTION occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 41456/41456 [22:59<00:00, 30.06it/s]\n"
     ]
    }
   ],
   "source": [
    "date_suffices = [\n",
    "                 '20200101_20201231', '20210101_20211231'\n",
    "                ]\n",
    "for date_suffix in date_suffices:\n",
    "    comment_df = pd.read_csv('../pushshift/comment/askReddit_comment_{}.csv'.format(date_suffix))\n",
    "#     print(comment_df.shape)\n",
    "    submission_filtered_df = pd.read_csv('../pushshift/submission/askReddit_submission_{}.csv'.format(date_suffix))\n",
    "    submission_filtered_ids = submission_filtered_df['id'].tolist()\n",
    "    submission_filtered_ids = ['t3_' + x for x in submission_filtered_ids]\n",
    "    \n",
    "    coment_coment_df=comment_df[comment_df['parent_id'].str.startswith('t1',na=False)]\n",
    "    coment_coment_ids=coment_coment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    coment_coment_filtered_ids=[x for x in coment_coment_ids]\n",
    "    coment_coment_filtered_ids = list(dict.fromkeys(coment_coment_filtered_ids))\n",
    "\n",
    "    total_ids=submission_filtered_ids+coment_coment_filtered_ids\n",
    "    total_ids = list(dict.fromkeys(total_ids))   \n",
    "    comment_df=comment_df[comment_df['id'].isin(total_ids)]\n",
    "    \n",
    "    \n",
    "    comment_filtered_cols = ['id','link_id','parent_id', 'summarized', 'text', 'score','length']\n",
    "    comment_filtered_dict = {col: [] for col in comment_filtered_cols}\n",
    "    for i in tqdm(range(comment_df.shape[0])):\n",
    "        comment_id = comment_df.iloc[i]['id']\n",
    "        link_id = comment_df.iloc[i]['link_id']\n",
    "        parent_id = comment_df.iloc[i]['parent_id']\n",
    "        score=comment_df.iloc[i]['score']\n",
    "        body = comment_df.iloc[i]['body']\n",
    "        preprocessed_body = preprocess_raw(body)\n",
    "        summarized_body = summarize(preprocessed_body, dialog_turn = 2)\n",
    "        if summarized_body is not None:\n",
    "            comment_filtered_dict['id'].append(comment_id)\n",
    "            comment_filtered_dict['link_id'].append(link_id)\n",
    "            comment_filtered_dict['parent_id'].append(parent_id)\n",
    "            comment_filtered_dict['summarized'].append(summarized_body['summarized'])\n",
    "            comment_filtered_dict['text'].append(summarized_body['text'])\n",
    "            comment_filtered_dict['score'].append(score)\n",
    "            comment_filtered_dict['length'].append(summarized_body['length'])\n",
    "    comment_filtered_df = pd.DataFrame(comment_filtered_dict)\n",
    "#     print(comment_filtered_df)\n",
    "    comment_filtered_df.to_csv('../pushshift/filtered_q/comment/askReddit_comment_{}.csv'.format(date_suffix), index = False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b997972",
   "metadata": {},
   "source": [
    "# Finalize the Submissions and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8913cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_filtered_df = pd.read_csv('../data/reddit/filtered/casual_conv_submissions_{}.csv'.format(date_suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7575456",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_filtered_df = pd.read_csv('../data/reddit/filtered/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "comment_filtered_parent_ids = comment_filtered_df['parent_id'].tolist()\n",
    "comment_filtered_parent_ids = [x[3:] for x in comment_filtered_parent_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d4dc612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7941, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_filtered_df = submission_filtered_df[submission_filtered_df['id'].isin(comment_filtered_parent_ids)]\n",
    "submission_filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ac2440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = ['sub_id', 'sub_summarized', 'sub_from', 'sub_text', 'sub_root', 'sub_length',\n",
    "              'com_id', 'com_summarized', 'com_text', 'com_length']\n",
    "final_dict = {col: [] for col in final_cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a99cbb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7941/7941 [00:46<00:00, 170.12it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(submission_filtered_df.shape[0])):\n",
    "    sub_id = submission_filtered_df.iloc[i]['id']\n",
    "    sub_summarized = submission_filtered_df.iloc[i]['summarized']\n",
    "    sub_from = submission_filtered_df.iloc[i]['from']\n",
    "    sub_text = submission_filtered_df.iloc[i]['text']\n",
    "    sub_root = submission_filtered_df.iloc[i]['root']\n",
    "    sub_length = submission_filtered_df.iloc[i]['length']\n",
    "\n",
    "    comment_filtered_df_sub = comment_filtered_df[comment_filtered_df['parent_id'] == 't3_' + sub_id]\n",
    "    for j in range(comment_filtered_df_sub.shape[0]):\n",
    "        final_dict['sub_id'].append(sub_id)\n",
    "        final_dict['sub_summarized'].append(sub_summarized)\n",
    "        final_dict['sub_from'].append(sub_from)\n",
    "        final_dict['sub_text'].append(sub_text)\n",
    "        final_dict['sub_root'].append(sub_root)\n",
    "        final_dict['sub_length'].append(sub_length)\n",
    "        final_dict['com_id'].append(comment_filtered_df_sub.iloc[j]['id'])\n",
    "        final_dict['com_summarized'].append(comment_filtered_df_sub.iloc[j]['summarized'])\n",
    "        final_dict['com_text'].append(comment_filtered_df_sub.iloc[j]['text'])\n",
    "        final_dict['com_length'].append(comment_filtered_df_sub.iloc[j]['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1daf8321",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88444562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_id</th>\n",
       "      <th>sub_summarized</th>\n",
       "      <th>sub_from</th>\n",
       "      <th>sub_text</th>\n",
       "      <th>sub_root</th>\n",
       "      <th>sub_length</th>\n",
       "      <th>com_id</th>\n",
       "      <th>com_summarized</th>\n",
       "      <th>com_text</th>\n",
       "      <th>com_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6trwr</td>\n",
       "      <td>1</td>\n",
       "      <td>Take it as a compliment.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6tw3v</td>\n",
       "      <td>1</td>\n",
       "      <td>However working at a bar where sex-charged men...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6uegk</td>\n",
       "      <td>1</td>\n",
       "      <td>Why do people get bent out of shape about hair...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6xlf7</td>\n",
       "      <td>1</td>\n",
       "      <td>Will appearing more attractive to customers in...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r2v7x0</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>My boss told me that during the time my hair w...</td>\n",
       "      <td>make</td>\n",
       "      <td>21</td>\n",
       "      <td>hm6zt2c</td>\n",
       "      <td>1</td>\n",
       "      <td>are you suggesting that its a problem for peop...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53689</th>\n",
       "      <td>n8sdhl</td>\n",
       "      <td>0</td>\n",
       "      <td>selftext</td>\n",
       "      <td>I never really got to do things like this when...</td>\n",
       "      <td>get</td>\n",
       "      <td>36</td>\n",
       "      <td>gxkseja</td>\n",
       "      <td>1</td>\n",
       "      <td>Nice man</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53690</th>\n",
       "      <td>n8sdhl</td>\n",
       "      <td>0</td>\n",
       "      <td>selftext</td>\n",
       "      <td>I never really got to do things like this when...</td>\n",
       "      <td>get</td>\n",
       "      <td>36</td>\n",
       "      <td>gxocvzh</td>\n",
       "      <td>1</td>\n",
       "      <td>I've never lived anywhere with fireflies, and ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53691</th>\n",
       "      <td>n8sbcb</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>After the worst 4 years of my life during coll...</td>\n",
       "      <td>move</td>\n",
       "      <td>22</td>\n",
       "      <td>gxk4z84</td>\n",
       "      <td>0</td>\n",
       "      <td>Not a bad place to live if you can solve for COL</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53692</th>\n",
       "      <td>n8sbcb</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>After the worst 4 years of my life during coll...</td>\n",
       "      <td>move</td>\n",
       "      <td>22</td>\n",
       "      <td>gxk5bpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Good luck on your new adventure!</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53693</th>\n",
       "      <td>n8sbcb</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>After the worst 4 years of my life during coll...</td>\n",
       "      <td>move</td>\n",
       "      <td>22</td>\n",
       "      <td>gxk9wzm</td>\n",
       "      <td>1</td>\n",
       "      <td>I also only took what could fit in my car.</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53694 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sub_id  sub_summarized  sub_from  \\\n",
       "0      r2v7x0               0     title   \n",
       "1      r2v7x0               0     title   \n",
       "2      r2v7x0               0     title   \n",
       "3      r2v7x0               0     title   \n",
       "4      r2v7x0               0     title   \n",
       "...       ...             ...       ...   \n",
       "53689  n8sdhl               0  selftext   \n",
       "53690  n8sdhl               0  selftext   \n",
       "53691  n8sbcb               0     title   \n",
       "53692  n8sbcb               0     title   \n",
       "53693  n8sbcb               0     title   \n",
       "\n",
       "                                                sub_text sub_root  sub_length  \\\n",
       "0      My boss told me that during the time my hair w...     make          21   \n",
       "1      My boss told me that during the time my hair w...     make          21   \n",
       "2      My boss told me that during the time my hair w...     make          21   \n",
       "3      My boss told me that during the time my hair w...     make          21   \n",
       "4      My boss told me that during the time my hair w...     make          21   \n",
       "...                                                  ...      ...         ...   \n",
       "53689  I never really got to do things like this when...      get          36   \n",
       "53690  I never really got to do things like this when...      get          36   \n",
       "53691  After the worst 4 years of my life during coll...     move          22   \n",
       "53692  After the worst 4 years of my life during coll...     move          22   \n",
       "53693  After the worst 4 years of my life during coll...     move          22   \n",
       "\n",
       "        com_id  com_summarized  \\\n",
       "0      hm6trwr               1   \n",
       "1      hm6tw3v               1   \n",
       "2      hm6uegk               1   \n",
       "3      hm6xlf7               1   \n",
       "4      hm6zt2c               1   \n",
       "...        ...             ...   \n",
       "53689  gxkseja               1   \n",
       "53690  gxocvzh               1   \n",
       "53691  gxk4z84               0   \n",
       "53692  gxk5bpg               1   \n",
       "53693  gxk9wzm               1   \n",
       "\n",
       "                                                com_text  com_length  \n",
       "0                               Take it as a compliment.           6  \n",
       "1      However working at a bar where sex-charged men...          14  \n",
       "2      Why do people get bent out of shape about hair...          15  \n",
       "3      Will appearing more attractive to customers in...          16  \n",
       "4      are you suggesting that its a problem for peop...          28  \n",
       "...                                                  ...         ...  \n",
       "53689                                           Nice man           2  \n",
       "53690  I've never lived anywhere with fireflies, and ...          18  \n",
       "53691   Not a bad place to live if you can solve for COL          12  \n",
       "53692                   Good luck on your new adventure!           7  \n",
       "53693         I also only took what could fit in my car.          11  \n",
       "\n",
       "[53694 rows x 10 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27025c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../data/reddit/matched/casual_conv_{}.csv'.format(date_suffix), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30679e83",
   "metadata": {},
   "source": [
    "## Batch Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b971178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 7)\n",
      "20160101_20161231 Start assemble submission with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 389.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160101_20161231 Start assemble comment with comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2164/2164 [00:03<00:00, 692.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final df shape: (2186, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "date_suffices = ['20170101_20171231', '20180101_20181231', '20190101_20191231',\n",
    "                 '20200101_20201231', '20210101_20211231']\n",
    "\n",
    "\n",
    "for date_suffix in date_suffices:\n",
    "    submission_filtered_df = pd.read_csv('../pushshift/filtered_q/submission/casual_conv_submissions_{}.csv'.format(date_suffix))\n",
    "    comment_filtered_df = pd.read_csv('../pushshift/filtered_q/comment/casual_conv_comments_{}.csv'.format(date_suffix))\n",
    "    \n",
    "    final_cols = ['src_id', 'src_type', 'src_summarized', 'src_from', 'src_text', 'src_root', 'src_length',\n",
    "                  'com_id', 'com_summarized', 'com_text', 'com_length','com_score']\n",
    "    final_dict = {col: [] for col in final_cols}\n",
    "    \n",
    "    comment_filtered_parent_df=comment_filtered_df[comment_filtered_df['parent_id'].str.startswith('t3')]\n",
    "    comment_filtered_parent_ids = comment_filtered_parent_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()\n",
    "    \n",
    "    submission_filtered_df = submission_filtered_df[submission_filtered_df['id'].isin(comment_filtered_parent_ids)]\n",
    "    print(submission_filtered_df.shape)\n",
    "\n",
    "    \n",
    "    print(date_suffix,'Start assemble submission with comment')\n",
    "    for i in tqdm(range(submission_filtered_df.shape[0])):       \n",
    "        sub_id = submission_filtered_df.iloc[i]['id']\n",
    "        sub_summarized = submission_filtered_df.iloc[i]['summarized']\n",
    "        sub_from = submission_filtered_df.iloc[i]['from']\n",
    "        sub_text = submission_filtered_df.iloc[i]['text']\n",
    "        sub_root = submission_filtered_df.iloc[i]['root']\n",
    "        sub_length = submission_filtered_df.iloc[i]['length']\n",
    "        comment_filtered_df_sub = comment_filtered_df[comment_filtered_df['parent_id'] == 't3_' + sub_id]\n",
    "        for j in range(comment_filtered_df_sub.shape[0]):\n",
    "            final_dict['src_id'].append(sub_id)\n",
    "            final_dict['src_type'].append('sub')\n",
    "            final_dict['src_summarized'].append(sub_summarized)\n",
    "            final_dict['src_from'].append(sub_from)\n",
    "            final_dict['src_text'].append(sub_text)\n",
    "            final_dict['src_root'].append(sub_root)\n",
    "            final_dict['src_length'].append(sub_length)\n",
    "            final_dict['com_id'].append(comment_filtered_df_sub.iloc[j]['id'])\n",
    "            final_dict['com_summarized'].append(comment_filtered_df_sub.iloc[j]['summarized'])\n",
    "            final_dict['com_text'].append(comment_filtered_df_sub.iloc[j]['text'])\n",
    "            final_dict['com_length'].append(comment_filtered_df_sub.iloc[j]['length'])\n",
    "            final_dict['com_score'].append(comment_filtered_df_sub.iloc[j]['score'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    coment_coment_df=comment_filtered_df[comment_filtered_df['parent_id'].str.startswith('t1')]\n",
    "    coment_coment_ids=coment_coment_df['parent_id'].apply(lambda x:x.split('_')[1]).tolist()    \n",
    "    comment_have_kid_df = comment_filtered_df[comment_filtered_df['id'].isin(coment_coment_ids)]\n",
    "    \n",
    "    print(date_suffix,'Start assemble comment with comment')\n",
    "    for k in tqdm(range(comment_have_kid_df.shape[0])):       \n",
    "        src_id = comment_have_kid_df.iloc[k]['id']\n",
    "        src_summarized = comment_have_kid_df.iloc[k]['summarized']\n",
    "        src_text = comment_have_kid_df.iloc[k]['text']\n",
    "        src_length = comment_have_kid_df.iloc[k]['length']\n",
    "        \n",
    "        comment_comment_df = comment_filtered_df[comment_filtered_df['parent_id'] == 't1_' + src_id]\n",
    "        for m in range(comment_comment_df.shape[0]):\n",
    "            final_dict['src_id'].append(src_id)\n",
    "            final_dict['src_type'].append('com')\n",
    "            final_dict['src_summarized'].append(src_summarized)\n",
    "            final_dict['src_from'].append('comment')\n",
    "            final_dict['src_text'].append(src_text)\n",
    "            final_dict['src_root'].append(src_root)\n",
    "            final_dict['src_length'].append(src_length)\n",
    "            \n",
    "            final_dict['com_id'].append(comment_comment_df.iloc[m]['id'])\n",
    "            final_dict['com_summarized'].append(comment_comment_df.iloc[m]['summarized'])\n",
    "            final_dict['com_text'].append(comment_comment_df.iloc[m]['text'])\n",
    "            final_dict['com_length'].append(comment_comment_df.iloc[m]['length'])\n",
    "            final_dict['com_score'].append(comment_comment_df.iloc[m]['score'])\n",
    "\n",
    "\n",
    "    \n",
    "    final_df = pd.DataFrame(final_dict)\n",
    "    print(\"final df shape:\",final_df.shape)\n",
    "    final_df.to_csv('../pushshift/matched_q/casual_conv_{}.csv'.format(date_suffix), index = False)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5178cba1",
   "metadata": {},
   "source": [
    "# Sample Some Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04400925",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = final_df.shape[0]\n",
    "indices = np.sort(np.random.choice(N, 1000, replace = False))\n",
    "final_df_sample = final_df.iloc[indices]\n",
    "final_df_sample.to_csv('pushshift/final/casual_conv_20200101_20201231_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c42cb",
   "metadata": {},
   "source": [
    "# Some Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2464a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad703c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"A silly question\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df9d284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "AUX\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I am happy about it.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69a34e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I went to the market today\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93048868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Just worked more than I ever have in my life\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4053c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year\n",
      "PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Happy New Year!\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edfab88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "VERB\n",
      "have\n",
      "Has have AUX\n",
      "she -PRON- PRON\n",
      "done do VERB\n",
      "your -PRON- DET\n",
      "homework homework NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Has she done your homework\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.root)\n",
    "    print(sent.root.pos_)\n",
    "    print(sent[0].lemma_)\n",
    "    for token in sent:\n",
    "        print(token, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5f70370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you finish your homework\n",
      "VERB\n",
      "do\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Did you finish your homework\")\n",
    "sents = [sent for sent in doc.sents]\n",
    "sent = sents[0]\n",
    "print(sent)\n",
    "print(sent.root.pos_)\n",
    "print(sent[0].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e4caf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 gave VERB give\n",
      "-PRON- PRON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I just gave you the paper\")\n",
    "sents = [sent for sent in doc.sents]\n",
    "sent = sents[0]\n",
    "print(len(sent), sent.root, sent.root.pos_, sent.root.lemma_)\n",
    "print(sent[0].lemma_, sent[0].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04dafca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "r_pattern = re.compile(r'(^| )\\/?r\\/[^ ]*')\n",
    "print(r_pattern.search('ashjs/r/haha__ ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52e46a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how about  and and ['"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'(\\([^\\(\\)]*\\))|(\\[[^\\[\\]]*\\])', '', 'how about [ashs] and and [[ss]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef137b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
